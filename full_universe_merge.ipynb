{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35a0118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input ECB: c:\\Users\\jonat\\Downloads\\_unique_csv_master\\ECB_Data\\ECB Data\n",
      "Input ESMA: c:\\Users\\jonat\\Downloads\\_unique_csv_master\\ESMA_UE_Collat_Merged\n",
      "Output: D:\\ECB_ESMA_MERGED\n",
      "Checkpoint: D:\\ECB_ESMA_MERGED\\_checkpoint.json\n",
      "Pools requiring deduplication: 3\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Configuration and Paths\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import gzip\n",
    "import time\n",
    "import gc\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = r\"c:\\Users\\jonat\\Downloads\\_unique_csv_master\"\n",
    "ECB_PATH = os.path.join(BASE_PATH, \"ECB_Data\", \"ECB Data\")\n",
    "ESMA_PATH = os.path.join(BASE_PATH, \"ESMA_UE_Collat_Merged\")\n",
    "TEMPLATE_PATH = os.path.join(BASE_PATH, \"ESMA Template (2).xlsx\")\n",
    "POOL_MAPPING_PATH = os.path.join(BASE_PATH, \"pool_mapping.json\")\n",
    "\n",
    "# OUTPUT on D: drive\n",
    "OUTPUT_DIR = r\"D:\\ECB_ESMA_MERGED\"\n",
    "CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, \"_checkpoint.json\")\n",
    "LOG_FILE = os.path.join(OUTPUT_DIR, \"_processing_log.txt\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# POOLS WITH TEMPORAL OVERLAP (verified via analysis)\n",
    "# Only these 3 pools have ECB and ESMA data for the same months\n",
    "# Deduplication is ONLY needed for these pools\n",
    "# =============================================================================\n",
    "POOLS_WITH_OVERLAP = {\n",
    "    'RMBMBE000095100120084',  # 8 overlapping months: 2022-03 to 2023-12\n",
    "    'RMBMFR000083100220149',  # 4 overlapping months: 2021-05 to 2021-08\n",
    "    'RMBMNL000185100120109',  # 1 overlapping month: 2024-06\n",
    "}\n",
    "\n",
    "print(f\"Input ECB: {ECB_PATH}\")\n",
    "print(f\"Input ESMA: {ESMA_PATH}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_FILE}\")\n",
    "print(f\"Pools requiring deduplication: {len(POOLS_WITH_OVERLAP)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbc83935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECB->ESMA mappings: 72\n",
      "ESMA->ECB mappings: 80\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Load Template Mapping (ECB <-> ESMA columns)\n",
    "# =============================================================================\n",
    "template_df = pd.read_excel(TEMPLATE_PATH, sheet_name='Sheet1')\n",
    "template_df = template_df[['FIELD CODE', 'FIELD NAME', 'For info: existing ECB or EBA NPL template field code']].copy()\n",
    "template_df.columns = ['ESMA_Code', 'ESMA_Name', 'ECB_Code']\n",
    "template_df = template_df[template_df['ECB_Code'].notna()]\n",
    "\n",
    "# Create mappings\n",
    "esma_to_ecb = dict(zip(template_df['ESMA_Code'], template_df['ECB_Code']))\n",
    "\n",
    "ecb_to_esma = {}\n",
    "for _, row in template_df.iterrows():\n",
    "    esma_code = row['ESMA_Code']\n",
    "    ecb_code = row['ECB_Code']\n",
    "    esma_name = row['ESMA_Name']\n",
    "    if ecb_code not in ecb_to_esma:\n",
    "        ecb_to_esma[ecb_code] = esma_code\n",
    "    elif 'New' in str(esma_name):\n",
    "        ecb_to_esma[ecb_code] = esma_code\n",
    "\n",
    "print(f\"ECB->ESMA mappings: {len(ecb_to_esma)}\")\n",
    "print(f\"ESMA->ECB mappings: {len(esma_to_ecb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb24106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECB pools: 58 (1941 files)\n",
      "ESMA pools: 262 (2906 files)\n",
      "\n",
      "Matched ECB->ESMA pools: 22\n",
      "ECB-only pools: 36\n",
      "ESMA-only pools: 246\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Build File Index (All Pools)\n",
    "# =============================================================================\n",
    "# Index ECB files by pool\n",
    "all_ecb_files = glob.glob(os.path.join(ECB_PATH, \"*.gz\"))\n",
    "ecb_pool_files = defaultdict(list)\n",
    "for f in all_ecb_files:\n",
    "    fname = os.path.basename(f)\n",
    "    pool_id = fname.split('_')[0]\n",
    "    ecb_pool_files[pool_id].append(fname)\n",
    "\n",
    "# Index ESMA files by pool\n",
    "all_esma_files = glob.glob(os.path.join(ESMA_PATH, \"*.csv\"))\n",
    "esma_pool_files = defaultdict(list)\n",
    "for f in all_esma_files:\n",
    "    fname = os.path.basename(f)\n",
    "    parts = fname.split('_')\n",
    "    pool_id = parts[-3]  # Pool ID is 3rd from last\n",
    "    esma_pool_files[pool_id].append(fname)\n",
    "\n",
    "# Load pool mapping (ECB -> ESMA matches)\n",
    "with open(POOL_MAPPING_PATH) as f:\n",
    "    pool_mapping = json.load(f)\n",
    "\n",
    "matched_pools = pool_mapping['pools']\n",
    "matched_ecb_pools = set(matched_pools.keys())\n",
    "matched_esma_pools = set(p['esma_pool'] for p in matched_pools.values())\n",
    "\n",
    "# Identify ECB-only and ESMA-only pools\n",
    "ecb_only_pools = set(ecb_pool_files.keys()) - matched_ecb_pools\n",
    "esma_only_pools = set(esma_pool_files.keys()) - matched_esma_pools\n",
    "\n",
    "print(f\"ECB pools: {len(ecb_pool_files)} ({len(all_ecb_files)} files)\")\n",
    "print(f\"ESMA pools: {len(esma_pool_files)} ({len(all_esma_files)} files)\")\n",
    "print(f\"\\nMatched ECB->ESMA pools: {len(matched_pools)}\")\n",
    "print(f\"ECB-only pools: {len(ecb_only_pools)}\")\n",
    "print(f\"ESMA-only pools: {len(esma_only_pools)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "426b62eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned output folders for completed pools:\n",
      "  Matched pools: 22\n",
      "  ECB-only pools: 0\n",
      "  ESMA-only pools: 0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: Checkpoint Management\n",
    "# =============================================================================\n",
    "def scan_completed_pools():\n",
    "    \"\"\"\n",
    "    Scan output folders to determine which pools are already processed.\n",
    "    This is more reliable than checkpoint.json since it checks actual files.\n",
    "    \"\"\"\n",
    "    completed = {\n",
    "        'matched': set(),\n",
    "        'ecb_only': set(),\n",
    "        'esma_only': set()\n",
    "    }\n",
    "    \n",
    "    for pool_type in ['matched', 'ecb_only', 'esma_only']:\n",
    "        folder = os.path.join(OUTPUT_DIR, pool_type)\n",
    "        if os.path.exists(folder):\n",
    "            for fname in os.listdir(folder):\n",
    "                if fname.endswith('.csv') and not fname.endswith('.tmp'):\n",
    "                    # Pool ID is the filename without .csv\n",
    "                    pool_id = fname[:-4]\n",
    "                    completed[pool_type].add(pool_id)\n",
    "    \n",
    "    return completed\n",
    "\n",
    "def log_message(msg):\n",
    "    \"\"\"Append message to log file and print\"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_entry = f\"[{timestamp}] {msg}\"\n",
    "    print(log_entry)\n",
    "    with open(LOG_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(log_entry + '\\n')\n",
    "\n",
    "# Scan output folders to find already-completed pools\n",
    "completed_pools = scan_completed_pools()\n",
    "print(f\"Scanned output folders for completed pools:\")\n",
    "print(f\"  Matched pools: {len(completed_pools['matched'])}\")\n",
    "print(f\"  ECB-only pools: {len(completed_pools['ecb_only'])}\")\n",
    "print(f\"  ESMA-only pools: {len(completed_pools['esma_only'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b412ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined (with zlib fallback for problematic gzip files)\n",
      "  Large pool threshold: 100 MB compressed\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 5: Data Loading Functions (with retry and chunked loading)\n",
    "# =============================================================================\n",
    "import gc\n",
    "import io\n",
    "import zlib\n",
    "\n",
    "# Threshold for \"large\" pool (compressed size in bytes)\n",
    "# 100 MB compressed - more conservative to avoid memory issues\n",
    "LARGE_POOL_THRESHOLD = 100 * 1024 * 1024  # 100 MB compressed\n",
    "\n",
    "def load_ecb_file(filepath, max_retries=3):\n",
    "    \"\"\"\n",
    "    Load a single ECB .gz file with retry logic.\n",
    "    \n",
    "    Uses zlib fallback for files that fail with standard gzip reader\n",
    "    (some ECB files have gzip format issues that zlib handles correctly).\n",
    "    \"\"\"\n",
    "    fname = os.path.basename(filepath)\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Method 1: Try standard pandas read (works for most files)\n",
    "            df = pd.read_csv(filepath, compression='gzip', low_memory=False)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Check if this is a gzip-related error\n",
    "            if 'gzip' in error_msg.lower() or 'Not a gzipped file' in error_msg:\n",
    "                # Method 2: Use zlib directly (handles problematic gzip files)\n",
    "                try:\n",
    "                    with open(filepath, 'rb') as f:\n",
    "                        raw = f.read()\n",
    "                    \n",
    "                    # Decompress using zlib (skip 10-byte gzip header)\n",
    "                    decomp = zlib.decompressobj(-zlib.MAX_WBITS)\n",
    "                    result = decomp.decompress(raw[10:])\n",
    "                    result += decomp.flush()\n",
    "                    \n",
    "                    df = pd.read_csv(io.BytesIO(result), low_memory=False)\n",
    "                    log_message(f\"  Loaded {fname} using zlib fallback ({len(df):,} rows)\")\n",
    "                    return df\n",
    "                except Exception as zlib_error:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        log_message(f\"  Retry {attempt+1}/{max_retries} for {fname}: zlib also failed: {zlib_error}\")\n",
    "                        time.sleep(0.5)\n",
    "                        gc.collect()\n",
    "                    else:\n",
    "                        log_message(f\"ERROR loading ECB file {filepath}: {e} (zlib fallback: {zlib_error})\")\n",
    "                        return pd.DataFrame()\n",
    "            else:\n",
    "                if attempt < max_retries - 1:\n",
    "                    log_message(f\"  Retry {attempt+1}/{max_retries} for {fname}: {e}\")\n",
    "                    time.sleep(0.5)\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    log_message(f\"ERROR loading ECB file {filepath}: {e}\")\n",
    "                    return pd.DataFrame()\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def load_esma_file(filepath, max_retries=3):\n",
    "    \"\"\"Load a single ESMA .csv file with retry logic\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            df = pd.read_csv(filepath, low_memory=False)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                log_message(f\"  Retry {attempt+1}/{max_retries} for {os.path.basename(filepath)}: {e}\")\n",
    "                time.sleep(0.5)\n",
    "                gc.collect()\n",
    "            else:\n",
    "                log_message(f\"ERROR loading ESMA file {filepath}: {e}\")\n",
    "                return pd.DataFrame()\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def get_pool_compressed_size(pool_id, source='ecb'):\n",
    "    \"\"\"Get total compressed size of files for a pool\"\"\"\n",
    "    if source == 'ecb':\n",
    "        files = ecb_pool_files.get(pool_id, [])\n",
    "        base_path = ECB_PATH\n",
    "    else:\n",
    "        files = esma_pool_files.get(pool_id, [])\n",
    "        base_path = ESMA_PATH\n",
    "    \n",
    "    total = 0\n",
    "    for fname in files:\n",
    "        try:\n",
    "            total += os.path.getsize(os.path.join(base_path, fname))\n",
    "        except:\n",
    "            pass\n",
    "    return total\n",
    "\n",
    "def is_large_pool(pool_id, source='ecb'):\n",
    "    \"\"\"Check if pool exceeds size threshold\"\"\"\n",
    "    return get_pool_compressed_size(pool_id, source) > LARGE_POOL_THRESHOLD\n",
    "\n",
    "def load_all_ecb_for_pool(pool_id):\n",
    "    \"\"\"Load and concatenate all ECB files for a pool\"\"\"\n",
    "    files = ecb_pool_files.get(pool_id, [])\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dfs = []\n",
    "    for fname in files:\n",
    "        filepath = os.path.join(ECB_PATH, fname)\n",
    "        df = load_ecb_file(filepath)\n",
    "        if len(df) > 0:\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def load_all_esma_for_pool(pool_id):\n",
    "    \"\"\"Load and concatenate all ESMA files for a pool\"\"\"\n",
    "    files = esma_pool_files.get(pool_id, [])\n",
    "    if not files:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    dfs = []\n",
    "    for fname in files:\n",
    "        filepath = os.path.join(ESMA_PATH, fname)\n",
    "        df = load_esma_file(filepath)\n",
    "        if len(df) > 0:\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if dfs:\n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "print(\"✓ Data loading functions defined (with zlib fallback for problematic gzip files)\")\n",
    "print(f\"  Large pool threshold: {LARGE_POOL_THRESHOLD/1024/1024:.0f} MB compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9b89a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data preparation functions defined\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 6: Data Preparation Functions\n",
    "# =============================================================================\n",
    "def prepare_ecb_data(ecb_df):\n",
    "    \"\"\"\n",
    "    Prepare ECB data:\n",
    "    1. Rename columns to ESMA equivalents\n",
    "    2. Add source marker\n",
    "    3. Normalize date to YYYY-MM\n",
    "    \"\"\"\n",
    "    if len(ecb_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = ecb_df.copy()\n",
    "    \n",
    "    # Rename ECB columns to ESMA equivalents\n",
    "    rename_map = {ecb: esma for ecb, esma in ecb_to_esma.items() if ecb in df.columns}\n",
    "    df = df.rename(columns=rename_map)\n",
    "    \n",
    "    # Add source marker\n",
    "    df['source'] = 'ECB'\n",
    "    \n",
    "    # Normalize date to YYYY-MM for deduplication\n",
    "    if 'RREL6' in df.columns:\n",
    "        df['date_ym'] = df['RREL6'].astype(str).str[:7]\n",
    "    elif 'AR1' in df.columns:\n",
    "        df['date_ym'] = df['AR1'].astype(str).str[:7]\n",
    "    else:\n",
    "        df['date_ym'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def prepare_esma_data(esma_df):\n",
    "    \"\"\"\n",
    "    Prepare ESMA data:\n",
    "    1. Add source marker\n",
    "    2. Normalize date to YYYY-MM\n",
    "    \"\"\"\n",
    "    if len(esma_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = esma_df.copy()\n",
    "    df['source'] = 'ESMA'\n",
    "    \n",
    "    # Normalize date\n",
    "    if 'RREL6' in df.columns:\n",
    "        df['date_ym'] = df['RREL6'].astype(str).str[:7]\n",
    "    else:\n",
    "        df['date_ym'] = ''\n",
    "    \n",
    "    return df\n",
    "\n",
    "def remove_duplicates_prefer_esma(df):\n",
    "    \"\"\"\n",
    "    Remove duplicate loan+date rows, preferring ESMA over ECB.\n",
    "    \"\"\"\n",
    "    if len(df) == 0 or 'RREL3' not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    # Find loan+dates that have ESMA data\n",
    "    esma_rows = df[df['source'] == 'ESMA']\n",
    "    esma_loan_dates = set(zip(\n",
    "        esma_rows['RREL3'].astype(str),\n",
    "        esma_rows['date_ym'].astype(str)\n",
    "    ))\n",
    "    \n",
    "    # Keep ESMA rows, or ECB rows without ESMA equivalent\n",
    "    def should_keep(row):\n",
    "        if row['source'] == 'ESMA':\n",
    "            return True\n",
    "        loan_date = (str(row['RREL3']), str(row['date_ym']))\n",
    "        return loan_date not in esma_loan_dates\n",
    "    \n",
    "    mask = df.apply(should_keep, axis=1)\n",
    "    result = df[mask].copy()\n",
    "    \n",
    "    # Drop helper column\n",
    "    if 'date_ym' in result.columns:\n",
    "        result = result.drop(columns=['date_ym'])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_non_empty_columns(df):\n",
    "    \"\"\"Return list of columns that have at least one non-null value\"\"\"\n",
    "    non_empty = []\n",
    "    for col in df.columns:\n",
    "        if df[col].notna().any():\n",
    "            non_empty.append(col)\n",
    "    return non_empty\n",
    "\n",
    "print(\"✓ Data preparation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66492649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Pool processing functions defined (with ATOMIC writes for resume safety)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 7: Pool Processing Functions (with large pool handling)\n",
    "# =============================================================================\n",
    "def process_matched_pool(ecb_pool_id, esma_pool_id):\n",
    "    \"\"\"\n",
    "    Process a matched ECB-ESMA pool pair.\n",
    "    Returns merged DataFrame with source column.\n",
    "    \n",
    "    OPTIMIZATION: Only runs deduplication for pools with temporal overlap.\n",
    "    For pools without overlap, simply concatenates ECB and ESMA data.\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    ecb_df = load_all_ecb_for_pool(ecb_pool_id)\n",
    "    esma_df = load_all_esma_for_pool(esma_pool_id)\n",
    "    \n",
    "    log_message(f\"  ECB rows: {len(ecb_df)}, ESMA rows: {len(esma_df)}\")\n",
    "    \n",
    "    # Check if this pool needs deduplication\n",
    "    needs_dedup = ecb_pool_id in POOLS_WITH_OVERLAP\n",
    "    \n",
    "    # Prepare data\n",
    "    ecb_prepared = prepare_ecb_data(ecb_df)\n",
    "    esma_prepared = prepare_esma_data(esma_df)\n",
    "    \n",
    "    # Combine\n",
    "    if len(ecb_prepared) > 0 and len(esma_prepared) > 0:\n",
    "        combined = pd.concat([ecb_prepared, esma_prepared], ignore_index=True)\n",
    "    elif len(esma_prepared) > 0:\n",
    "        combined = esma_prepared\n",
    "    elif len(ecb_prepared) > 0:\n",
    "        combined = ecb_prepared\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Remove duplicates ONLY for pools with temporal overlap\n",
    "    if needs_dedup:\n",
    "        log_message(f\"  Pool has temporal overlap - running deduplication...\")\n",
    "        merged = remove_duplicates_prefer_esma(combined)\n",
    "        log_message(f\"  After dedup: {len(merged)} rows (removed {len(combined) - len(merged)} duplicates)\")\n",
    "    else:\n",
    "        # No overlap - skip dedup (significant speedup)\n",
    "        merged = combined\n",
    "        if 'date_ym' in merged.columns:\n",
    "            merged = merged.drop(columns=['date_ym'])\n",
    "        log_message(f\"  No temporal overlap - skipping dedup: {len(merged)} rows\")\n",
    "    \n",
    "    # Add pool identifiers\n",
    "    merged['ecb_pool_id'] = ecb_pool_id\n",
    "    merged['esma_pool_id'] = esma_pool_id\n",
    "    \n",
    "    return merged\n",
    "\n",
    "def get_all_columns_for_ecb_pool(ecb_pool_id):\n",
    "    \"\"\"Scan all ECB files to determine ALL columns for consistent output.\"\"\"\n",
    "    all_columns = set()\n",
    "    files = ecb_pool_files.get(ecb_pool_id, [])\n",
    "    \n",
    "    for fname in files:\n",
    "        try:\n",
    "            filepath = os.path.join(ECB_PATH, fname)\n",
    "            df = load_ecb_file(filepath)\n",
    "            if len(df) > 0:\n",
    "                prepared = prepare_ecb_data(df)\n",
    "                if 'date_ym' in prepared.columns:\n",
    "                    prepared = prepared.drop(columns=['date_ym'])\n",
    "                prepared['ecb_pool_id'] = ecb_pool_id\n",
    "                prepared['esma_pool_id'] = None\n",
    "                non_empty = get_non_empty_columns(prepared)\n",
    "                all_columns.update(non_empty)\n",
    "                del df, prepared\n",
    "        except Exception as e:\n",
    "            log_message(f\"    Warning: Could not scan ECB file {fname}: {e}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    return sorted(list(all_columns))\n",
    "\n",
    "def get_all_columns_for_esma_pool(esma_pool_id):\n",
    "    \"\"\"Scan all ESMA files to determine ALL columns for consistent output.\"\"\"\n",
    "    all_columns = set()\n",
    "    files = esma_pool_files.get(esma_pool_id, [])\n",
    "    \n",
    "    for fname in files:\n",
    "        try:\n",
    "            filepath = os.path.join(ESMA_PATH, fname)\n",
    "            df = load_esma_file(filepath)\n",
    "            if len(df) > 0:\n",
    "                prepared = prepare_esma_data(df)\n",
    "                if 'date_ym' in prepared.columns:\n",
    "                    prepared = prepared.drop(columns=['date_ym'])\n",
    "                prepared['ecb_pool_id'] = None\n",
    "                prepared['esma_pool_id'] = esma_pool_id\n",
    "                non_empty = get_non_empty_columns(prepared)\n",
    "                all_columns.update(non_empty)\n",
    "                del df, prepared\n",
    "        except Exception as e:\n",
    "            log_message(f\"    Warning: Could not scan ESMA file {fname}: {e}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    return sorted(list(all_columns))\n",
    "\n",
    "def process_ecb_only_pool_chunked(ecb_pool_id, output_path):\n",
    "    \"\"\"\n",
    "    Process a LARGE ECB-only pool file-by-file, appending to output.\n",
    "    This avoids loading entire pool into memory.\n",
    "    Returns total rows saved.\n",
    "    \"\"\"\n",
    "    files = ecb_pool_files.get(ecb_pool_id, [])\n",
    "    if not files:\n",
    "        return 0\n",
    "    \n",
    "    # PHASE 1: Scan all files to determine consistent column schema\n",
    "    log_message(f\"  Scanning {len(files)} files for column schema...\")\n",
    "    all_columns = get_all_columns_for_ecb_pool(ecb_pool_id)\n",
    "    log_message(f\"  Found {len(all_columns)} columns\")\n",
    "    \n",
    "    total_rows = 0\n",
    "    first_file = True\n",
    "    \n",
    "    log_message(f\"  Processing {len(files)} files in chunked mode...\")\n",
    "    \n",
    "    for i, fname in enumerate(files):\n",
    "        filepath = os.path.join(ECB_PATH, fname)\n",
    "        df = load_ecb_file(filepath)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Prepare (rename to ESMA schema)\n",
    "        prepared = prepare_ecb_data(df)\n",
    "        \n",
    "        # Drop helper column\n",
    "        if 'date_ym' in prepared.columns:\n",
    "            prepared = prepared.drop(columns=['date_ym'])\n",
    "        \n",
    "        # Add pool identifiers\n",
    "        prepared['ecb_pool_id'] = ecb_pool_id\n",
    "        prepared['esma_pool_id'] = None\n",
    "        \n",
    "        # Use consistent columns - add missing as NaN\n",
    "        for col in all_columns:\n",
    "            if col not in prepared.columns:\n",
    "                prepared[col] = np.nan\n",
    "        \n",
    "        df_to_save = prepared[all_columns]\n",
    "        \n",
    "        # Append to file (write header only for first chunk)\n",
    "        df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "        first_file = False\n",
    "        total_rows += len(df_to_save)\n",
    "        \n",
    "        # Free memory\n",
    "        del df, prepared, df_to_save\n",
    "        gc.collect()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            log_message(f\"    Processed {i+1}/{len(files)} files, {total_rows:,} rows so far\")\n",
    "    \n",
    "    return total_rows\n",
    "\n",
    "def process_ecb_only_pool(ecb_pool_id):\n",
    "    \"\"\"\n",
    "    Process an ECB-only pool (no ESMA match).\n",
    "    Rename columns to ESMA schema.\n",
    "    \"\"\"\n",
    "    ecb_df = load_all_ecb_for_pool(ecb_pool_id)\n",
    "    \n",
    "    if len(ecb_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    log_message(f\"  ECB rows: {len(ecb_df)}\")\n",
    "    \n",
    "    # Prepare (rename to ESMA schema)\n",
    "    prepared = prepare_ecb_data(ecb_df)\n",
    "    \n",
    "    # Drop helper column\n",
    "    if 'date_ym' in prepared.columns:\n",
    "        prepared = prepared.drop(columns=['date_ym'])\n",
    "    \n",
    "    # Add pool identifiers\n",
    "    prepared['ecb_pool_id'] = ecb_pool_id\n",
    "    prepared['esma_pool_id'] = None\n",
    "    \n",
    "    return prepared\n",
    "\n",
    "def process_esma_only_pool_chunked(esma_pool_id, output_path):\n",
    "    \"\"\"\n",
    "    Process a LARGE ESMA-only pool file-by-file, appending to output.\n",
    "    Returns total rows saved.\n",
    "    \"\"\"\n",
    "    files = esma_pool_files.get(esma_pool_id, [])\n",
    "    if not files:\n",
    "        return 0\n",
    "    \n",
    "    # PHASE 1: Scan all files to determine consistent column schema\n",
    "    log_message(f\"  Scanning {len(files)} files for column schema...\")\n",
    "    all_columns = get_all_columns_for_esma_pool(esma_pool_id)\n",
    "    log_message(f\"  Found {len(all_columns)} columns\")\n",
    "    \n",
    "    total_rows = 0\n",
    "    first_file = True\n",
    "    \n",
    "    log_message(f\"  Processing {len(files)} files in chunked mode...\")\n",
    "    \n",
    "    for i, fname in enumerate(files):\n",
    "        filepath = os.path.join(ESMA_PATH, fname)\n",
    "        df = load_esma_file(filepath)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Prepare\n",
    "        prepared = prepare_esma_data(df)\n",
    "        \n",
    "        # Drop helper column\n",
    "        if 'date_ym' in prepared.columns:\n",
    "            prepared = prepared.drop(columns=['date_ym'])\n",
    "        \n",
    "        # Add pool identifiers\n",
    "        prepared['ecb_pool_id'] = None\n",
    "        prepared['esma_pool_id'] = esma_pool_id\n",
    "        \n",
    "        # Use consistent columns - add missing as NaN\n",
    "        for col in all_columns:\n",
    "            if col not in prepared.columns:\n",
    "                prepared[col] = np.nan\n",
    "        \n",
    "        df_to_save = prepared[all_columns]\n",
    "        \n",
    "        # Append to file\n",
    "        df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "        first_file = False\n",
    "        total_rows += len(df_to_save)\n",
    "        \n",
    "        # Free memory\n",
    "        del df, prepared, df_to_save\n",
    "        gc.collect()\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            log_message(f\"    Processed {i+1}/{len(files)} files, {total_rows:,} rows so far\")\n",
    "    \n",
    "    return total_rows\n",
    "\n",
    "def process_esma_only_pool(esma_pool_id):\n",
    "    \"\"\"\n",
    "    Process an ESMA-only pool (no ECB match).\n",
    "    \"\"\"\n",
    "    esma_df = load_all_esma_for_pool(esma_pool_id)\n",
    "    \n",
    "    if len(esma_df) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    log_message(f\"  ESMA rows: {len(esma_df)}\")\n",
    "    \n",
    "    # Prepare\n",
    "    prepared = prepare_esma_data(esma_df)\n",
    "    \n",
    "    # Drop helper column\n",
    "    if 'date_ym' in prepared.columns:\n",
    "        prepared = prepared.drop(columns=['date_ym'])\n",
    "    \n",
    "    # Add pool identifiers\n",
    "    prepared['ecb_pool_id'] = None\n",
    "    prepared['esma_pool_id'] = esma_pool_id\n",
    "    \n",
    "    return prepared\n",
    "\n",
    "def get_all_columns_for_matched_pool(ecb_pool_id, esma_pool_id):\n",
    "    \"\"\"\n",
    "    PHASE 1: Scan sample files from both ECB and ESMA to determine ALL columns.\n",
    "    This ensures consistent column alignment when writing chunks.\n",
    "    Returns sorted list of all non-empty columns across both sources.\n",
    "    \"\"\"\n",
    "    all_columns = set()\n",
    "    \n",
    "    ecb_files = ecb_pool_files.get(ecb_pool_id, [])\n",
    "    esma_files = esma_pool_files.get(esma_pool_id, [])\n",
    "    \n",
    "    # Scan ALL files to ensure we capture every possible column\n",
    "    # This prevents column misalignment if later files have different columns\n",
    "    \n",
    "    # Scan ECB files\n",
    "    for fname in ecb_files:\n",
    "        try:\n",
    "            filepath = os.path.join(ECB_PATH, fname)\n",
    "            df = load_ecb_file(filepath)\n",
    "            if len(df) > 0:\n",
    "                prepared = prepare_ecb_data(df)\n",
    "                if 'date_ym' in prepared.columns:\n",
    "                    prepared = prepared.drop(columns=['date_ym'])\n",
    "                prepared['ecb_pool_id'] = ecb_pool_id\n",
    "                prepared['esma_pool_id'] = esma_pool_id\n",
    "                non_empty = get_non_empty_columns(prepared)\n",
    "                all_columns.update(non_empty)\n",
    "                del df, prepared\n",
    "        except Exception as e:\n",
    "            log_message(f\"    Warning: Could not scan ECB file {fname}: {e}\")\n",
    "    \n",
    "    # Scan ESMA files\n",
    "    for fname in esma_files:\n",
    "        try:\n",
    "            filepath = os.path.join(ESMA_PATH, fname)\n",
    "            df = load_esma_file(filepath)\n",
    "            if len(df) > 0:\n",
    "                prepared = prepare_esma_data(df)\n",
    "                if 'date_ym' in prepared.columns:\n",
    "                    prepared = prepared.drop(columns=['date_ym'])\n",
    "                prepared['ecb_pool_id'] = ecb_pool_id\n",
    "                prepared['esma_pool_id'] = esma_pool_id\n",
    "                non_empty = get_non_empty_columns(prepared)\n",
    "                all_columns.update(non_empty)\n",
    "                del df, prepared\n",
    "        except Exception as e:\n",
    "            log_message(f\"    Warning: Could not scan ESMA file {fname}: {e}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # Return sorted list for consistent ordering\n",
    "    return sorted(list(all_columns))\n",
    "\n",
    "def process_matched_pool_chunked(ecb_pool_id, esma_pool_id, output_path):\n",
    "    \"\"\"\n",
    "    Process a LARGE matched ECB-ESMA pool file-by-file.\n",
    "    \n",
    "    FIXED: Now determines ALL columns upfront to ensure consistent alignment.\n",
    "    \n",
    "    INTEGRITY LOGIC:\n",
    "    - We prefer ESMA data over ECB when both have the same loan+date\n",
    "    - Strategy depends on sizes:\n",
    "      - If smaller side < 500MB: load smaller, chunk larger\n",
    "      - If BOTH sides > 500MB: chunk both, skip dedup (rare overlap anyway)\n",
    "    \n",
    "    Returns total rows saved.\n",
    "    \"\"\"\n",
    "    ecb_files = ecb_pool_files.get(ecb_pool_id, [])\n",
    "    esma_files = esma_pool_files.get(esma_pool_id, [])\n",
    "    \n",
    "    if not ecb_files and not esma_files:\n",
    "        return 0\n",
    "    \n",
    "    # Determine sizes\n",
    "    ecb_size = get_pool_compressed_size(ecb_pool_id, 'ecb')\n",
    "    esma_size = get_pool_compressed_size(esma_pool_id, 'esma')\n",
    "    smaller_size = min(ecb_size, esma_size)\n",
    "    \n",
    "    # Threshold for \"can fit in memory\" - 500 MB compressed max\n",
    "    MEMORY_SAFE_THRESHOLD = 500 * 1024 * 1024\n",
    "    \n",
    "    log_message(f\"  ECB size: {ecb_size/1024/1024:.0f} MB, ESMA size: {esma_size/1024/1024:.0f} MB\")\n",
    "    \n",
    "    # PHASE 1: Determine ALL columns upfront for consistent alignment\n",
    "    log_message(f\"  Phase 1: Scanning files to determine column schema...\")\n",
    "    all_columns = get_all_columns_for_matched_pool(ecb_pool_id, esma_pool_id)\n",
    "    log_message(f\"  Found {len(all_columns)} total columns across both sources\")\n",
    "    \n",
    "    total_rows = 0\n",
    "    first_file = True\n",
    "    \n",
    "    if smaller_size > MEMORY_SAFE_THRESHOLD:\n",
    "        # CASE 0: BOTH sides are huge - chunk both, write ECB first, then ESMA (preferred at end)\n",
    "        log_message(f\"  Strategy: MEGA POOL - chunk both sides with consistent columns\")\n",
    "        \n",
    "        # Process ALL ECB files first\n",
    "        log_message(f\"  Processing {len(ecb_files)} ECB files...\")\n",
    "        for i, fname in enumerate(ecb_files):\n",
    "            filepath = os.path.join(ECB_PATH, fname)\n",
    "            df = load_ecb_file(filepath)\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            \n",
    "            prepared = prepare_ecb_data(df)\n",
    "            if 'date_ym' in prepared.columns:\n",
    "                prepared = prepared.drop(columns=['date_ym'])\n",
    "            \n",
    "            prepared['ecb_pool_id'] = ecb_pool_id\n",
    "            prepared['esma_pool_id'] = esma_pool_id\n",
    "            \n",
    "            # Use consistent columns - add missing columns as NaN\n",
    "            for col in all_columns:\n",
    "                if col not in prepared.columns:\n",
    "                    prepared[col] = np.nan\n",
    "            \n",
    "            df_to_save = prepared[all_columns]\n",
    "            df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "            first_file = False\n",
    "            total_rows += len(df_to_save)\n",
    "            \n",
    "            del df, prepared, df_to_save\n",
    "            gc.collect()\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                log_message(f\"    ECB: {i+1}/{len(ecb_files)} files, {total_rows:,} rows\")\n",
    "        \n",
    "        ecb_rows = total_rows\n",
    "        log_message(f\"  ECB complete: {ecb_rows:,} rows\")\n",
    "        \n",
    "        # Process ALL ESMA files (preferred source - at end)\n",
    "        log_message(f\"  Processing {len(esma_files)} ESMA files...\")\n",
    "        for i, fname in enumerate(esma_files):\n",
    "            filepath = os.path.join(ESMA_PATH, fname)\n",
    "            df = load_esma_file(filepath)\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            \n",
    "            prepared = prepare_esma_data(df)\n",
    "            if 'date_ym' in prepared.columns:\n",
    "                prepared = prepared.drop(columns=['date_ym'])\n",
    "            \n",
    "            prepared['ecb_pool_id'] = ecb_pool_id\n",
    "            prepared['esma_pool_id'] = esma_pool_id\n",
    "            \n",
    "            # Use consistent columns - add missing columns as NaN\n",
    "            for col in all_columns:\n",
    "                if col not in prepared.columns:\n",
    "                    prepared[col] = np.nan\n",
    "            \n",
    "            df_to_save = prepared[all_columns]\n",
    "            df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "            first_file = False\n",
    "            total_rows += len(df_to_save)\n",
    "            \n",
    "            del df, prepared, df_to_save\n",
    "            gc.collect()\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                log_message(f\"    ESMA: {i+1}/{len(esma_files)} files, {total_rows - ecb_rows:,} ESMA rows\")\n",
    "        \n",
    "        log_message(f\"  ESMA complete: {total_rows - ecb_rows:,} rows\")\n",
    "        \n",
    "        # POST-PROCESSING: Deduplicate the output file\n",
    "        # Logic: Keep ALL ESMA rows. Remove ECB rows where same (loan, date) exists in ESMA.\n",
    "        needs_dedup = ecb_pool_id in POOLS_WITH_OVERLAP\n",
    "        \n",
    "        if needs_dedup:\n",
    "            log_message(f\"  Post-processing: removing ECB duplicates where ESMA exists...\")\n",
    "            try:\n",
    "                # PASS 1: Collect all ESMA (loan, date) keys\n",
    "                esma_keys = set()\n",
    "                for chunk in pd.read_csv(output_path, chunksize=100000, dtype=str, usecols=['RREL3', 'RREL6', 'source']):\n",
    "                    esma_rows = chunk[chunk['source'] == 'ESMA']\n",
    "                    if len(esma_rows) > 0 and 'RREL3' in esma_rows.columns:\n",
    "                        keys = zip(esma_rows['RREL3'].fillna(''), esma_rows['RREL6'].str[:7].fillna(''))\n",
    "                        esma_keys.update(keys)\n",
    "                log_message(f\"    Found {len(esma_keys):,} unique ESMA (loan, date) keys\")\n",
    "                \n",
    "                # PASS 2: Write all ESMA rows + ECB rows not in esma_keys\n",
    "                temp_path = output_path + \".dedup_temp\"\n",
    "                rows_before = total_rows\n",
    "                total_rows = 0\n",
    "                first_chunk = True\n",
    "                \n",
    "                for chunk in pd.read_csv(output_path, chunksize=100000, dtype=str):\n",
    "                    if 'source' in chunk.columns and 'RREL3' in chunk.columns:\n",
    "                        # Keep row if: ESMA, or ECB with no ESMA equivalent\n",
    "                        is_esma = chunk['source'] == 'ESMA'\n",
    "                        keys = list(zip(chunk['RREL3'].fillna(''), chunk['RREL6'].str[:7].fillna('')))\n",
    "                        is_dup_ecb = [(not esma) and (k in esma_keys) for esma, k in zip(is_esma, keys)]\n",
    "                        chunk_filtered = chunk.loc[~pd.Series(is_dup_ecb, index=chunk.index)]\n",
    "                    else:\n",
    "                        chunk_filtered = chunk\n",
    "                    \n",
    "                    chunk_filtered.to_csv(temp_path, mode='a', index=False, header=first_chunk)\n",
    "                    first_chunk = False\n",
    "                    total_rows += len(chunk_filtered)\n",
    "                    \n",
    "                    del chunk, chunk_filtered\n",
    "                    gc.collect()\n",
    "                \n",
    "                os.replace(temp_path, output_path)\n",
    "                log_message(f\"  Deduplication complete: {rows_before - total_rows:,} ECB duplicates removed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_message(f\"  WARNING: Deduplication failed ({e}), keeping original file\")\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "        else:\n",
    "            log_message(f\"  Skipping deduplication - no temporal overlap\")\n",
    "        \n",
    "    elif esma_size <= ecb_size:\n",
    "        # CASE 1: ECB is larger - load ESMA first, chunk ECB\n",
    "        log_message(f\"  Strategy: Load ESMA (smaller), chunk ECB files\")\n",
    "        \n",
    "        needs_dedup = ecb_pool_id in POOLS_WITH_OVERLAP\n",
    "        if needs_dedup:\n",
    "            log_message(f\"  Pool has temporal overlap - will filter ECB duplicates\")\n",
    "        else:\n",
    "            log_message(f\"  No temporal overlap - skipping deduplication\")\n",
    "        \n",
    "        # Load all ESMA data\n",
    "        log_message(f\"  Loading ESMA data ({len(esma_files)} files)...\")\n",
    "        esma_dfs = []\n",
    "        for fname in esma_files:\n",
    "            filepath = os.path.join(ESMA_PATH, fname)\n",
    "            df = load_esma_file(filepath)\n",
    "            if len(df) > 0:\n",
    "                esma_dfs.append(df)\n",
    "        \n",
    "        if esma_dfs:\n",
    "            esma_all = pd.concat(esma_dfs, ignore_index=True)\n",
    "            esma_prepared = prepare_esma_data(esma_all)\n",
    "            if needs_dedup:\n",
    "                esma_loan_dates = set(zip(\n",
    "                    esma_prepared['RREL3'].astype(str),\n",
    "                    esma_prepared['date_ym'].astype(str)\n",
    "                ))\n",
    "                log_message(f\"  ESMA: {len(esma_prepared)} rows, {len(esma_loan_dates)} unique loan-dates\")\n",
    "            else:\n",
    "                esma_loan_dates = set()\n",
    "                log_message(f\"  ESMA: {len(esma_prepared)} rows\")\n",
    "            del esma_all\n",
    "        else:\n",
    "            esma_prepared = pd.DataFrame()\n",
    "            esma_loan_dates = set()\n",
    "        \n",
    "        del esma_dfs\n",
    "        gc.collect()\n",
    "        \n",
    "        # Process ECB files one by one\n",
    "        log_message(f\"  Processing {len(ecb_files)} ECB files...\")\n",
    "        for i, fname in enumerate(ecb_files):\n",
    "            filepath = os.path.join(ECB_PATH, fname)\n",
    "            df = load_ecb_file(filepath)\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            \n",
    "            ecb_prepared = prepare_ecb_data(df)\n",
    "            \n",
    "            # Filter: keep ECB rows only if NOT in ESMA\n",
    "            if needs_dedup and len(esma_loan_dates) > 0 and 'RREL3' in ecb_prepared.columns:\n",
    "                ecb_prepared['_key'] = ecb_prepared['RREL3'].astype(str) + '|' + ecb_prepared['date_ym'].astype(str)\n",
    "                mask = ~ecb_prepared['_key'].apply(lambda x: (x.split('|')[0], x.split('|')[1]) in esma_loan_dates)\n",
    "                ecb_filtered = ecb_prepared[mask].drop(columns=['_key']).copy()\n",
    "            else:\n",
    "                ecb_filtered = ecb_prepared\n",
    "            \n",
    "            if 'date_ym' in ecb_filtered.columns:\n",
    "                ecb_filtered = ecb_filtered.drop(columns=['date_ym'])\n",
    "            \n",
    "            ecb_filtered['ecb_pool_id'] = ecb_pool_id\n",
    "            ecb_filtered['esma_pool_id'] = esma_pool_id\n",
    "            \n",
    "            if len(ecb_filtered) > 0:\n",
    "                # Use consistent columns\n",
    "                for col in all_columns:\n",
    "                    if col not in ecb_filtered.columns:\n",
    "                        ecb_filtered[col] = np.nan\n",
    "                \n",
    "                df_to_save = ecb_filtered[all_columns]\n",
    "                df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "                first_file = False\n",
    "                total_rows += len(df_to_save)\n",
    "            \n",
    "            del df, ecb_prepared, ecb_filtered\n",
    "            gc.collect()\n",
    "            \n",
    "            if (i + 1) % 20 == 0:\n",
    "                log_message(f\"    Processed {i+1}/{len(ecb_files)} ECB files, {total_rows:,} ECB rows kept\")\n",
    "        \n",
    "        # Append ALL ESMA rows (preferred source)\n",
    "        if len(esma_prepared) > 0:\n",
    "            if 'date_ym' in esma_prepared.columns:\n",
    "                esma_prepared = esma_prepared.drop(columns=['date_ym'])\n",
    "            esma_prepared['ecb_pool_id'] = ecb_pool_id\n",
    "            esma_prepared['esma_pool_id'] = esma_pool_id\n",
    "            \n",
    "            # Use consistent columns\n",
    "            for col in all_columns:\n",
    "                if col not in esma_prepared.columns:\n",
    "                    esma_prepared[col] = np.nan\n",
    "            \n",
    "            df_to_save = esma_prepared[all_columns]\n",
    "            df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "            total_rows += len(df_to_save)\n",
    "            log_message(f\"  Added {len(df_to_save):,} ESMA rows (preferred)\")\n",
    "        \n",
    "    else:\n",
    "        # CASE 2: ESMA is larger - load ECB first, chunk ESMA\n",
    "        log_message(f\"  Strategy: Load ECB (smaller), chunk ESMA files\")\n",
    "        \n",
    "        needs_dedup = ecb_pool_id in POOLS_WITH_OVERLAP\n",
    "        if needs_dedup:\n",
    "            log_message(f\"  Pool has temporal overlap - will filter ECB duplicates\")\n",
    "        else:\n",
    "            log_message(f\"  No temporal overlap - skipping deduplication\")\n",
    "        \n",
    "        # Load all ECB data\n",
    "        log_message(f\"  Loading ECB data ({len(ecb_files)} files)...\")\n",
    "        ecb_dfs = []\n",
    "        for fname in ecb_files:\n",
    "            filepath = os.path.join(ECB_PATH, fname)\n",
    "            df = load_ecb_file(filepath)\n",
    "            if len(df) > 0:\n",
    "                ecb_dfs.append(df)\n",
    "        \n",
    "        if ecb_dfs:\n",
    "            ecb_all = pd.concat(ecb_dfs, ignore_index=True)\n",
    "            ecb_prepared = prepare_ecb_data(ecb_all)\n",
    "            if needs_dedup:\n",
    "                ecb_loan_dates = set(zip(\n",
    "                    ecb_prepared['RREL3'].astype(str),\n",
    "                    ecb_prepared['date_ym'].astype(str)\n",
    "                ))\n",
    "                log_message(f\"  ECB: {len(ecb_prepared)} rows, {len(ecb_loan_dates)} unique loan-dates\")\n",
    "            else:\n",
    "                ecb_loan_dates = set()\n",
    "                log_message(f\"  ECB: {len(ecb_prepared)} rows\")\n",
    "            del ecb_all\n",
    "        else:\n",
    "            ecb_prepared = pd.DataFrame()\n",
    "            ecb_loan_dates = set()\n",
    "        \n",
    "        del ecb_dfs\n",
    "        gc.collect()\n",
    "        \n",
    "        ecb_loan_dates_covered_by_esma = set()\n",
    "        \n",
    "        # Write ECB rows first (will be filtered at end if needed)\n",
    "        if len(ecb_prepared) > 0:\n",
    "            if 'date_ym' in ecb_prepared.columns:\n",
    "                ecb_for_write = ecb_prepared.drop(columns=['date_ym']).copy()\n",
    "            else:\n",
    "                ecb_for_write = ecb_prepared.copy()\n",
    "            \n",
    "            ecb_for_write['ecb_pool_id'] = ecb_pool_id\n",
    "            ecb_for_write['esma_pool_id'] = esma_pool_id\n",
    "            \n",
    "            # Use consistent columns\n",
    "            for col in all_columns:\n",
    "                if col not in ecb_for_write.columns:\n",
    "                    ecb_for_write[col] = np.nan\n",
    "            \n",
    "            df_to_save = ecb_for_write[all_columns]\n",
    "            df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "            first_file = False\n",
    "            total_rows += len(df_to_save)\n",
    "            log_message(f\"  Wrote {len(df_to_save):,} ECB rows\")\n",
    "            del ecb_for_write\n",
    "        \n",
    "        # Process ESMA files one by one\n",
    "        log_message(f\"  Processing {len(esma_files)} ESMA files...\")\n",
    "        for i, fname in enumerate(esma_files):\n",
    "            filepath = os.path.join(ESMA_PATH, fname)\n",
    "            df = load_esma_file(filepath)\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "            \n",
    "            esma_chunk = prepare_esma_data(df)\n",
    "            \n",
    "            # Track covered loan-dates for dedup\n",
    "            if needs_dedup and 'RREL3' in esma_chunk.columns:\n",
    "                chunk_loan_dates = set(zip(\n",
    "                    esma_chunk['RREL3'].astype(str),\n",
    "                    esma_chunk['date_ym'].astype(str)\n",
    "                ))\n",
    "                ecb_loan_dates_covered_by_esma.update(chunk_loan_dates & ecb_loan_dates)\n",
    "            \n",
    "            if 'date_ym' in esma_chunk.columns:\n",
    "                esma_chunk = esma_chunk.drop(columns=['date_ym'])\n",
    "            \n",
    "            esma_chunk['ecb_pool_id'] = ecb_pool_id\n",
    "            esma_chunk['esma_pool_id'] = esma_pool_id\n",
    "            \n",
    "            # Use consistent columns\n",
    "            for col in all_columns:\n",
    "                if col not in esma_chunk.columns:\n",
    "                    esma_chunk[col] = np.nan\n",
    "            \n",
    "            df_to_save = esma_chunk[all_columns]\n",
    "            df_to_save.to_csv(output_path, mode='a', index=False, header=first_file)\n",
    "            first_file = False\n",
    "            total_rows += len(df_to_save)\n",
    "            \n",
    "            del df, esma_chunk\n",
    "            gc.collect()\n",
    "            \n",
    "            if (i + 1) % 5 == 0:\n",
    "                log_message(f\"    Processed {i+1}/{len(esma_files)} ESMA files, {total_rows:,} total rows\")\n",
    "        \n",
    "        # Post-processing dedup if needed\n",
    "        if needs_dedup and len(ecb_loan_dates_covered_by_esma) > 0:\n",
    "            log_message(f\"  Post-processing: removing {len(ecb_loan_dates_covered_by_esma)} ECB duplicates...\")\n",
    "            try:\n",
    "                temp_path = output_path + \".dedup_temp\"\n",
    "                rows_before = total_rows\n",
    "                total_rows = 0\n",
    "                first_chunk = True\n",
    "                \n",
    "                for chunk in pd.read_csv(output_path, chunksize=100000, dtype=str):\n",
    "                    if 'source' in chunk.columns and 'RREL3' in chunk.columns and 'RREL6' in chunk.columns:\n",
    "                        # Keep all ESMA rows, filter ECB rows\n",
    "                        is_esma = chunk['source'] == 'ESMA'\n",
    "                        keys = list(zip(chunk['RREL3'].fillna(''), chunk['RREL6'].str[:7].fillna('')))\n",
    "                        is_dup_ecb = [(not esma) and (k in ecb_loan_dates_covered_by_esma) for esma, k in zip(is_esma, keys)]\n",
    "                        chunk_filtered = chunk.loc[~pd.Series(is_dup_ecb, index=chunk.index)]\n",
    "                    else:\n",
    "                        chunk_filtered = chunk\n",
    "                    \n",
    "                    chunk_filtered.to_csv(temp_path, mode='a', index=False, header=first_chunk)\n",
    "                    first_chunk = False\n",
    "                    total_rows += len(chunk_filtered)\n",
    "                    \n",
    "                    del chunk, chunk_filtered\n",
    "                    gc.collect()\n",
    "                \n",
    "                os.replace(temp_path, output_path)\n",
    "                log_message(f\"  Deduplication complete: {rows_before - total_rows:,} duplicates removed\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                log_message(f\"  WARNING: Deduplication failed ({e})\")\n",
    "                if os.path.exists(temp_path):\n",
    "                    os.remove(temp_path)\n",
    "    \n",
    "    return total_rows\n",
    "\n",
    "def save_pool_result(df, pool_type, pool_id):\n",
    "    \"\"\"\n",
    "    Save pool result to disk using ATOMIC write (temp file + rename).\n",
    "    This ensures partial files are never left behind if interrupted.\n",
    "    Uses dynamic column selection - only non-empty columns.\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Dynamic column selection: only keep non-empty columns\n",
    "    non_empty_cols = get_non_empty_columns(df)\n",
    "    df_to_save = df[non_empty_cols]\n",
    "    \n",
    "    # Create subdirectory by pool type\n",
    "    subdir = os.path.join(OUTPUT_DIR, pool_type)\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    \n",
    "    # ATOMIC WRITE: Save to temp file first, then rename\n",
    "    safe_pool_id = pool_id.replace('/', '_').replace('\\\\', '_')\n",
    "    output_path = os.path.join(subdir, f\"{safe_pool_id}.csv\")\n",
    "    temp_path = output_path + \".tmp\"\n",
    "    \n",
    "    df_to_save.to_csv(temp_path, index=False)\n",
    "    os.replace(temp_path, output_path)  # Atomic rename\n",
    "    \n",
    "    return len(df_to_save)\n",
    "\n",
    "print(\"✓ Pool processing functions defined (with ATOMIC writes for resume safety)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d1e3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 13:04:48] ======================================================================\n",
      "[2025-12-10 13:04:48] STARTING MATCHED POOLS PROCESSING\n",
      "[2025-12-10 13:04:48] ======================================================================\n",
      "[2025-12-10 13:04:48] Matched pools: 22 total, 22 remaining\n",
      "[2025-12-10 13:04:48]   Normal pools: 5, Large pools (chunked): 17\n",
      "[2025-12-10 13:04:48] \n",
      "[1/5] Processing matched pool: RMBMES000140100120090\n",
      "[2025-12-10 13:04:50]   Loaded RMBMES000140100120090_2023-09-01_Pool.csv.gz using zlib fallback (1,543 rows)\n",
      "[2025-12-10 13:04:50]   Loaded RMBMES000140100120090_2023-09-01_Pool.csv.gz using zlib fallback (1,543 rows)\n",
      "[2025-12-10 13:04:50]   ECB rows: 70976, ESMA rows: 4715\n",
      "[2025-12-10 13:04:50]   ECB rows: 70976, ESMA rows: 4715\n",
      "[2025-12-10 13:04:50]   No temporal overlap - skipping dedup: 75691 rows\n",
      "[2025-12-10 13:04:50]   No temporal overlap - skipping dedup: 75691 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['ecb_pool_id'] = ecb_pool_id\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['esma_pool_id'] = esma_pool_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 13:04:53]   Saved 75,691 rows in 4.2s\n",
      "[2025-12-10 13:04:53] \n",
      "[2/5] Processing matched pool: RMBMES000140100220122\n",
      "[2025-12-10 13:04:55]   ECB rows: 122314, ESMA rows: 9587\n",
      "[2025-12-10 13:04:55]   ECB rows: 122314, ESMA rows: 9587\n",
      "[2025-12-10 13:04:56]   No temporal overlap - skipping dedup: 131901 rows\n",
      "[2025-12-10 13:04:56]   No temporal overlap - skipping dedup: 131901 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['ecb_pool_id'] = ecb_pool_id\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['esma_pool_id'] = esma_pool_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 13:05:00]   Saved 131,901 rows in 7.2s\n",
      "[2025-12-10 13:05:00] \n",
      "[3/5] Processing matched pool: RMBMNL001345100320187\n",
      "[2025-12-10 13:05:00]   ECB rows: 2551, ESMA rows: 25631\n",
      "[2025-12-10 13:05:00]   No temporal overlap - skipping dedup: 28182 rows\n",
      "[2025-12-10 13:05:00]   ECB rows: 2551, ESMA rows: 25631\n",
      "[2025-12-10 13:05:00]   No temporal overlap - skipping dedup: 28182 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['ecb_pool_id'] = ecb_pool_id\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['esma_pool_id'] = esma_pool_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 13:05:02]   Saved 28,182 rows in 1.9s\n",
      "[2025-12-10 13:05:02] \n",
      "[4/5] Processing matched pool: RMBSBE000087100320118\n",
      "[2025-12-10 13:05:09]   ECB rows: 507938, ESMA rows: 7540\n",
      "[2025-12-10 13:05:09]   ECB rows: 507938, ESMA rows: 7540\n",
      "[2025-12-10 13:05:12]   No temporal overlap - skipping dedup: 515478 rows\n",
      "[2025-12-10 13:05:12]   No temporal overlap - skipping dedup: 515478 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['ecb_pool_id'] = ecb_pool_id\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['esma_pool_id'] = esma_pool_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 13:05:24]   Saved 515,478 rows in 22.3s\n",
      "[2025-12-10 13:05:24] \n",
      "[5/5] Processing matched pool: RMBMNL001345100220171\n",
      "[2025-12-10 13:05:25]   ECB rows: 40021, ESMA rows: 25631\n",
      "[2025-12-10 13:05:25]   ECB rows: 40021, ESMA rows: 25631\n",
      "[2025-12-10 13:05:26]   No temporal overlap - skipping dedup: 65652 rows\n",
      "[2025-12-10 13:05:26]   No temporal overlap - skipping dedup: 65652 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:48: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['ecb_pool_id'] = ecb_pool_id\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  merged['esma_pool_id'] = esma_pool_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 13:05:27]   Saved 65,652 rows in 3.4s\n",
      "[2025-12-10 13:05:27] \n",
      "[LARGE 1/17] Processing matched pool: RMBMNL000185100120109\n",
      "[2025-12-10 13:05:27]   ECB: 356 MB, ESMA: 127 MB\n",
      "[2025-12-10 13:05:29]   ECB size: 356 MB, ESMA size: 127 MB\n",
      "[2025-12-10 13:05:29]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:05:29]   ECB size: 356 MB, ESMA size: 127 MB\n",
      "[2025-12-10 13:05:29]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:06:39]   Found 216 total columns across both sources\n",
      "[2025-12-10 13:06:39]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:06:39]   Pool has temporal overlap - will filter ECB duplicates\n",
      "[2025-12-10 13:06:39]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 13:06:39]   Found 216 total columns across both sources\n",
      "[2025-12-10 13:06:39]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:06:39]   Pool has temporal overlap - will filter ECB duplicates\n",
      "[2025-12-10 13:06:39]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 13:06:41]   ESMA: 142548 rows, 142548 unique loan-dates\n",
      "[2025-12-10 13:06:41]   ESMA: 142548 rows, 142548 unique loan-dates\n",
      "[2025-12-10 13:06:41]   Processing 47 ECB files...\n",
      "[2025-12-10 13:06:41]   Processing 47 ECB files...\n",
      "[2025-12-10 13:08:04]     Processed 20/47 ECB files, 1,667,002 ECB rows kept\n",
      "[2025-12-10 13:08:04]     Processed 20/47 ECB files, 1,667,002 ECB rows kept\n",
      "[2025-12-10 13:09:05]     Processed 40/47 ECB files, 2,814,141 ECB rows kept\n",
      "[2025-12-10 13:09:05]     Processed 40/47 ECB files, 2,814,141 ECB rows kept\n",
      "[2025-12-10 13:09:22]   Added 142,548 ESMA rows (preferred)\n",
      "[2025-12-10 13:09:22]   Saved 3,148,198 rows in 234.4s (chunked)\n",
      "[2025-12-10 13:09:22] \n",
      "[LARGE 2/17] Processing matched pool: RMBMUK000172100220060\n",
      "[2025-12-10 13:09:22]   ECB: 895 MB, ESMA: 12 MB\n",
      "[2025-12-10 13:09:22]   Added 142,548 ESMA rows (preferred)\n",
      "[2025-12-10 13:09:22]   Saved 3,148,198 rows in 234.4s (chunked)\n",
      "[2025-12-10 13:09:22] \n",
      "[LARGE 2/17] Processing matched pool: RMBMUK000172100220060\n",
      "[2025-12-10 13:09:22]   ECB: 895 MB, ESMA: 12 MB\n",
      "[2025-12-10 13:09:24]   ECB size: 895 MB, ESMA size: 12 MB\n",
      "[2025-12-10 13:09:24]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:09:24]   ECB size: 895 MB, ESMA size: 12 MB\n",
      "[2025-12-10 13:09:24]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:12:36]   Found 197 total columns across both sources\n",
      "[2025-12-10 13:12:36]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:12:36]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:12:36]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 13:12:36]   ESMA: 18523 rows\n",
      "[2025-12-10 13:12:36]   Found 197 total columns across both sources\n",
      "[2025-12-10 13:12:36]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:12:36]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:12:36]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 13:12:36]   ESMA: 18523 rows\n",
      "[2025-12-10 13:12:36]   Processing 117 ECB files...\n",
      "[2025-12-10 13:12:36]   Processing 117 ECB files...\n",
      "[2025-12-10 13:16:44]     Processed 20/117 ECB files, 5,363,756 ECB rows kept\n",
      "[2025-12-10 13:16:44]     Processed 20/117 ECB files, 5,363,756 ECB rows kept\n",
      "[2025-12-10 13:19:37]     Processed 40/117 ECB files, 9,119,044 ECB rows kept\n",
      "[2025-12-10 13:19:37]     Processed 40/117 ECB files, 9,119,044 ECB rows kept\n",
      "[2025-12-10 13:21:52]     Processed 60/117 ECB files, 11,823,780 ECB rows kept\n",
      "[2025-12-10 13:21:52]     Processed 60/117 ECB files, 11,823,780 ECB rows kept\n",
      "[2025-12-10 13:22:39]     Processed 80/117 ECB files, 12,652,547 ECB rows kept\n",
      "[2025-12-10 13:22:39]     Processed 80/117 ECB files, 12,652,547 ECB rows kept\n",
      "[2025-12-10 13:23:03]     Processed 100/117 ECB files, 12,981,434 ECB rows kept\n",
      "[2025-12-10 13:23:03]     Processed 100/117 ECB files, 12,981,434 ECB rows kept\n",
      "[2025-12-10 13:23:20]   Added 18,523 ESMA rows (preferred)\n",
      "[2025-12-10 13:23:20]   Saved 13,210,315 rows in 838.4s (chunked)\n",
      "[2025-12-10 13:23:20] \n",
      "[LARGE 3/17] Processing matched pool: RMBMUK000209100520063\n",
      "[2025-12-10 13:23:20]   ECB: 523 MB, ESMA: 12 MB\n",
      "[2025-12-10 13:23:20]   Added 18,523 ESMA rows (preferred)\n",
      "[2025-12-10 13:23:20]   Saved 13,210,315 rows in 838.4s (chunked)\n",
      "[2025-12-10 13:23:20] \n",
      "[LARGE 3/17] Processing matched pool: RMBMUK000209100520063\n",
      "[2025-12-10 13:23:20]   ECB: 523 MB, ESMA: 12 MB\n",
      "[2025-12-10 13:23:22]   ECB size: 523 MB, ESMA size: 12 MB\n",
      "[2025-12-10 13:23:22]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:23:22]   ECB size: 523 MB, ESMA size: 12 MB\n",
      "[2025-12-10 13:23:22]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:25:43]   Loaded RMBMUK000209100520063_2017-12-31_Pool.csv.gz using zlib fallback (233,051 rows)\n",
      "[2025-12-10 13:25:43]   Loaded RMBMUK000209100520063_2017-12-31_Pool.csv.gz using zlib fallback (233,051 rows)\n",
      "[2025-12-10 13:26:21]   Found 208 total columns across both sources\n",
      "[2025-12-10 13:26:21]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:26:21]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:26:21]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 13:26:21]   ESMA: 18523 rows\n",
      "[2025-12-10 13:26:21]   Found 208 total columns across both sources\n",
      "[2025-12-10 13:26:21]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:26:21]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:26:21]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 13:26:21]   ESMA: 18523 rows\n",
      "[2025-12-10 13:26:21]   Processing 32 ECB files...\n",
      "[2025-12-10 13:26:21]   Processing 32 ECB files...\n",
      "[2025-12-10 13:30:57]     Processed 20/32 ECB files, 5,796,367 ECB rows kept\n",
      "[2025-12-10 13:30:57]     Processed 20/32 ECB files, 5,796,367 ECB rows kept\n",
      "[2025-12-10 13:31:02]   Loaded RMBMUK000209100520063_2017-12-31_Pool.csv.gz using zlib fallback (233,051 rows)\n",
      "[2025-12-10 13:31:02]   Loaded RMBMUK000209100520063_2017-12-31_Pool.csv.gz using zlib fallback (233,051 rows)\n",
      "[2025-12-10 13:32:24]   Added 18,523 ESMA rows (preferred)\n",
      "[2025-12-10 13:32:25]   Saved 7,617,414 rows in 544.2s (chunked)\n",
      "[2025-12-10 13:32:25] \n",
      "[LARGE 4/17] Processing matched pool: RMBSBE000044100820191\n",
      "[2025-12-10 13:32:25]   ECB: 81 MB, ESMA: 298 MB\n",
      "[2025-12-10 13:32:24]   Added 18,523 ESMA rows (preferred)\n",
      "[2025-12-10 13:32:25]   Saved 7,617,414 rows in 544.2s (chunked)\n",
      "[2025-12-10 13:32:25] \n",
      "[LARGE 4/17] Processing matched pool: RMBSBE000044100820191\n",
      "[2025-12-10 13:32:25]   ECB: 81 MB, ESMA: 298 MB\n",
      "[2025-12-10 13:32:26]   ECB size: 81 MB, ESMA size: 298 MB\n",
      "[2025-12-10 13:32:26]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:32:26]   ECB size: 81 MB, ESMA size: 298 MB\n",
      "[2025-12-10 13:32:26]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:32:50]   Found 132 total columns across both sources\n",
      "[2025-12-10 13:32:50]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 13:32:50]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:32:50]   Loading ECB data (32 files)...\n",
      "[2025-12-10 13:32:50]   Found 132 total columns across both sources\n",
      "[2025-12-10 13:32:50]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 13:32:50]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:32:50]   Loading ECB data (32 files)...\n",
      "[2025-12-10 13:33:07]   ECB: 1270797 rows\n",
      "[2025-12-10 13:33:07]   ECB: 1270797 rows\n",
      "[2025-12-10 13:33:33]   Wrote 1,270,797 ECB rows\n",
      "[2025-12-10 13:33:33]   Wrote 1,270,797 ECB rows\n",
      "[2025-12-10 13:33:34]   Processing 20 ESMA files...\n",
      "[2025-12-10 13:33:34]   Processing 20 ESMA files...\n",
      "[2025-12-10 13:33:39]     Processed 5/20 ESMA files, 1,387,024 total rows\n",
      "[2025-12-10 13:33:39]     Processed 5/20 ESMA files, 1,387,024 total rows\n",
      "[2025-12-10 13:33:44]     Processed 10/20 ESMA files, 1,498,895 total rows\n",
      "[2025-12-10 13:33:44]     Processed 10/20 ESMA files, 1,498,895 total rows\n",
      "[2025-12-10 13:33:49]     Processed 15/20 ESMA files, 1,605,779 total rows\n",
      "[2025-12-10 13:33:49]     Processed 15/20 ESMA files, 1,605,779 total rows\n",
      "[2025-12-10 13:33:53]     Processed 20/20 ESMA files, 1,707,236 total rows\n",
      "[2025-12-10 13:33:53]     Processed 20/20 ESMA files, 1,707,236 total rows\n",
      "[2025-12-10 13:33:54]   Saved 1,707,236 rows in 89.3s (chunked)\n",
      "[2025-12-10 13:33:54] \n",
      "[LARGE 5/17] Processing matched pool: RMBSBE000087100420090\n",
      "[2025-12-10 13:33:54]   ECB: 141 MB, ESMA: 5 MB\n",
      "[2025-12-10 13:33:54]   Saved 1,707,236 rows in 89.3s (chunked)\n",
      "[2025-12-10 13:33:54] \n",
      "[LARGE 5/17] Processing matched pool: RMBSBE000087100420090\n",
      "[2025-12-10 13:33:54]   ECB: 141 MB, ESMA: 5 MB\n",
      "[2025-12-10 13:33:56]   ECB size: 141 MB, ESMA size: 5 MB\n",
      "[2025-12-10 13:33:56]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:33:56]   ECB size: 141 MB, ESMA size: 5 MB\n",
      "[2025-12-10 13:33:56]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:34:31]   Found 209 total columns across both sources\n",
      "[2025-12-10 13:34:31]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:34:31]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:34:31]   Loading ESMA data (4 files)...\n",
      "[2025-12-10 13:34:31]   ESMA: 7540 rows\n",
      "[2025-12-10 13:34:31]   Found 209 total columns across both sources\n",
      "[2025-12-10 13:34:31]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:34:31]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:34:31]   Loading ESMA data (4 files)...\n",
      "[2025-12-10 13:34:31]   ESMA: 7540 rows\n",
      "[2025-12-10 13:34:32]   Processing 33 ECB files...\n",
      "[2025-12-10 13:34:32]   Processing 33 ECB files...\n",
      "[2025-12-10 13:35:25]     Processed 20/33 ECB files, 1,048,710 ECB rows kept\n",
      "[2025-12-10 13:35:25]     Processed 20/33 ECB files, 1,048,710 ECB rows kept\n",
      "[2025-12-10 13:36:00]   Added 7,540 ESMA rows (preferred)\n",
      "[2025-12-10 13:36:00]   Saved 1,737,905 rows in 126.0s (chunked)\n",
      "[2025-12-10 13:36:00] \n",
      "[LARGE 6/17] Processing matched pool: RMBSBE000087100520121\n",
      "[2025-12-10 13:36:00]   ECB: 123 MB, ESMA: 9 MB\n",
      "[2025-12-10 13:36:00]   Added 7,540 ESMA rows (preferred)\n",
      "[2025-12-10 13:36:00]   Saved 1,737,905 rows in 126.0s (chunked)\n",
      "[2025-12-10 13:36:00] \n",
      "[LARGE 6/17] Processing matched pool: RMBSBE000087100520121\n",
      "[2025-12-10 13:36:00]   ECB: 123 MB, ESMA: 9 MB\n",
      "[2025-12-10 13:36:02]   ECB size: 123 MB, ESMA size: 9 MB\n",
      "[2025-12-10 13:36:02]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:36:02]   ECB size: 123 MB, ESMA size: 9 MB\n",
      "[2025-12-10 13:36:02]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:36:38]   Found 207 total columns across both sources\n",
      "[2025-12-10 13:36:38]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:36:38]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:36:38]   Loading ESMA data (4 files)...\n",
      "[2025-12-10 13:36:39]   ESMA: 14456 rows\n",
      "[2025-12-10 13:36:38]   Found 207 total columns across both sources\n",
      "[2025-12-10 13:36:38]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 13:36:38]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 13:36:38]   Loading ESMA data (4 files)...\n",
      "[2025-12-10 13:36:39]   ESMA: 14456 rows\n",
      "[2025-12-10 13:36:39]   Processing 33 ECB files...\n",
      "[2025-12-10 13:36:39]   Processing 33 ECB files...\n",
      "[2025-12-10 13:37:29]     Processed 20/33 ECB files, 967,145 ECB rows kept\n",
      "[2025-12-10 13:37:29]     Processed 20/33 ECB files, 967,145 ECB rows kept\n",
      "[2025-12-10 13:38:05]   Added 14,456 ESMA rows (preferred)\n",
      "[2025-12-10 13:38:05]   Saved 1,682,444 rows in 125.0s (chunked)\n",
      "[2025-12-10 13:38:05] \n",
      "[LARGE 7/17] Processing matched pool: RMBMBE000095100120084\n",
      "[2025-12-10 13:38:05]   ECB: 1837 MB, ESMA: 4429 MB\n",
      "[2025-12-10 13:38:05]   Added 14,456 ESMA rows (preferred)\n",
      "[2025-12-10 13:38:05]   Saved 1,682,444 rows in 125.0s (chunked)\n",
      "[2025-12-10 13:38:05] \n",
      "[LARGE 7/17] Processing matched pool: RMBMBE000095100120084\n",
      "[2025-12-10 13:38:05]   ECB: 1837 MB, ESMA: 4429 MB\n",
      "[2025-12-10 13:38:07]   ECB size: 1837 MB, ESMA size: 4429 MB\n",
      "[2025-12-10 13:38:07]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:38:07]   ECB size: 1837 MB, ESMA size: 4429 MB\n",
      "[2025-12-10 13:38:07]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 13:49:33]   Found 217 total columns across both sources\n",
      "[2025-12-10 13:49:33]   Strategy: MEGA POOL - chunk both sides with consistent columns\n",
      "[2025-12-10 13:49:33]   Processing 47 ECB files...\n",
      "[2025-12-10 13:49:33]   Found 217 total columns across both sources\n",
      "[2025-12-10 13:49:33]   Strategy: MEGA POOL - chunk both sides with consistent columns\n",
      "[2025-12-10 13:49:33]   Processing 47 ECB files...\n",
      "[2025-12-10 13:58:56]     ECB: 20/47 files, 11,266,897 rows\n",
      "[2025-12-10 13:58:56]     ECB: 20/47 files, 11,266,897 rows\n",
      "[2025-12-10 14:07:01]     ECB: 40/47 files, 20,645,863 rows\n",
      "[2025-12-10 14:07:01]     ECB: 40/47 files, 20,645,863 rows\n",
      "[2025-12-10 14:09:19]   ECB complete: 23,304,387 rows\n",
      "[2025-12-10 14:09:19]   Processing 14 ESMA files...\n",
      "[2025-12-10 14:09:19]   ECB complete: 23,304,387 rows\n",
      "[2025-12-10 14:09:19]   Processing 14 ESMA files...\n",
      "[2025-12-10 14:11:14]     ESMA: 5/14 files, 2,264,754 ESMA rows\n",
      "[2025-12-10 14:11:14]     ESMA: 5/14 files, 2,264,754 ESMA rows\n",
      "[2025-12-10 14:13:05]     ESMA: 10/14 files, 4,464,245 ESMA rows\n",
      "[2025-12-10 14:13:05]     ESMA: 10/14 files, 4,464,245 ESMA rows\n",
      "[2025-12-10 14:14:33]   ESMA complete: 6,185,106 rows\n",
      "[2025-12-10 14:14:33]   Post-processing: removing ECB duplicates where ESMA exists...\n",
      "[2025-12-10 14:14:33]   ESMA complete: 6,185,106 rows\n",
      "[2025-12-10 14:14:33]   Post-processing: removing ECB duplicates where ESMA exists...\n",
      "[2025-12-10 14:16:17]     Found 4,922,898 unique ESMA (loan, date) keys\n",
      "[2025-12-10 14:16:17]     Found 4,922,898 unique ESMA (loan, date) keys\n",
      "[2025-12-10 14:35:00]   Deduplication complete: 2,876,618 ECB duplicates removed\n",
      "[2025-12-10 14:35:00]   Deduplication complete: 2,876,618 ECB duplicates removed\n",
      "[2025-12-10 14:35:01]   Saved 26,612,875 rows in 3416.3s (chunked)\n",
      "[2025-12-10 14:35:01] \n",
      "[LARGE 8/17] Processing matched pool: RMBMFR000083100220149\n",
      "[2025-12-10 14:35:01]   ECB: 5575 MB, ESMA: 28291 MB\n",
      "[2025-12-10 14:35:01]   Saved 26,612,875 rows in 3416.3s (chunked)\n",
      "[2025-12-10 14:35:01] \n",
      "[LARGE 8/17] Processing matched pool: RMBMFR000083100220149\n",
      "[2025-12-10 14:35:01]   ECB: 5575 MB, ESMA: 28291 MB\n",
      "[2025-12-10 14:35:03]   ECB size: 5575 MB, ESMA size: 28291 MB\n",
      "[2025-12-10 14:35:03]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 14:35:03]   ECB size: 5575 MB, ESMA size: 28291 MB\n",
      "[2025-12-10 14:35:03]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 15:05:26]   Found 215 total columns across both sources\n",
      "[2025-12-10 15:05:26]   Strategy: MEGA POOL - chunk both sides with consistent columns\n",
      "[2025-12-10 15:05:26]   Processing 87 ECB files...\n",
      "[2025-12-10 15:05:26]   Found 215 total columns across both sources\n",
      "[2025-12-10 15:05:26]   Strategy: MEGA POOL - chunk both sides with consistent columns\n",
      "[2025-12-10 15:05:26]   Processing 87 ECB files...\n",
      "[2025-12-10 15:14:50]     ECB: 20/87 files, 11,426,296 rows\n",
      "[2025-12-10 15:14:50]     ECB: 20/87 files, 11,426,296 rows\n",
      "[2025-12-10 15:24:27]     ECB: 40/87 files, 22,918,386 rows\n",
      "[2025-12-10 15:24:27]     ECB: 40/87 files, 22,918,386 rows\n",
      "[2025-12-10 15:33:53]     ECB: 60/87 files, 34,348,580 rows\n",
      "[2025-12-10 15:33:53]     ECB: 60/87 files, 34,348,580 rows\n",
      "[2025-12-10 15:43:27]     ECB: 80/87 files, 45,591,584 rows\n",
      "[2025-12-10 15:43:27]     ECB: 80/87 files, 45,591,584 rows\n",
      "[2025-12-10 15:46:45]   ECB complete: 49,384,355 rows\n",
      "[2025-12-10 15:46:45]   Processing 51 ESMA files...\n",
      "[2025-12-10 15:46:45]   ECB complete: 49,384,355 rows\n",
      "[2025-12-10 15:46:45]   Processing 51 ESMA files...\n",
      "[2025-12-10 15:49:04]     ESMA: 5/51 files, 2,695,175 ESMA rows\n",
      "[2025-12-10 15:49:04]     ESMA: 5/51 files, 2,695,175 ESMA rows\n",
      "[2025-12-10 15:51:25]     ESMA: 10/51 files, 5,350,970 ESMA rows\n",
      "[2025-12-10 15:51:25]     ESMA: 10/51 files, 5,350,970 ESMA rows\n",
      "[2025-12-10 15:53:49]     ESMA: 15/51 files, 7,990,587 ESMA rows\n",
      "[2025-12-10 15:53:49]     ESMA: 15/51 files, 7,990,587 ESMA rows\n",
      "[2025-12-10 15:56:02]     ESMA: 20/51 files, 10,604,247 ESMA rows\n",
      "[2025-12-10 15:56:02]     ESMA: 20/51 files, 10,604,247 ESMA rows\n",
      "[2025-12-10 15:58:31]     ESMA: 25/51 files, 13,594,865 ESMA rows\n",
      "[2025-12-10 15:58:31]     ESMA: 25/51 files, 13,594,865 ESMA rows\n",
      "[2025-12-10 16:01:36]     ESMA: 30/51 files, 17,217,333 ESMA rows\n",
      "[2025-12-10 16:01:36]     ESMA: 30/51 files, 17,217,333 ESMA rows\n",
      "[2025-12-10 16:05:03]     ESMA: 35/51 files, 21,402,395 ESMA rows\n",
      "[2025-12-10 16:05:03]     ESMA: 35/51 files, 21,402,395 ESMA rows\n",
      "[2025-12-10 16:08:38]     ESMA: 40/51 files, 25,753,289 ESMA rows\n",
      "[2025-12-10 16:08:38]     ESMA: 40/51 files, 25,753,289 ESMA rows\n",
      "[2025-12-10 16:12:21]     ESMA: 45/51 files, 30,073,826 ESMA rows\n",
      "[2025-12-10 16:12:21]     ESMA: 45/51 files, 30,073,826 ESMA rows\n",
      "[2025-12-10 16:16:04]     ESMA: 50/51 files, 34,392,736 ESMA rows\n",
      "[2025-12-10 16:16:04]     ESMA: 50/51 files, 34,392,736 ESMA rows\n",
      "[2025-12-10 16:16:49]   ESMA complete: 35,254,933 rows\n",
      "[2025-12-10 16:16:49]   Post-processing: removing ECB duplicates where ESMA exists...\n",
      "[2025-12-10 16:16:49]   ESMA complete: 35,254,933 rows\n",
      "[2025-12-10 16:16:49]   Post-processing: removing ECB duplicates where ESMA exists...\n",
      "[2025-12-10 16:22:10]     Found 35,254,933 unique ESMA (loan, date) keys\n",
      "[2025-12-10 16:22:10]     Found 35,254,933 unique ESMA (loan, date) keys\n",
      "[2025-12-10 17:37:56]   Deduplication complete: 2,159,522 ECB duplicates removed\n",
      "[2025-12-10 17:37:56]   Deduplication complete: 2,159,522 ECB duplicates removed\n",
      "[2025-12-10 17:38:08]   Saved 82,479,766 rows in 10987.3s (chunked)\n",
      "[2025-12-10 17:38:08] \n",
      "[LARGE 9/17] Processing matched pool: RMBMIT000103100520171\n",
      "[2025-12-10 17:38:08]   ECB: 47 MB, ESMA: 4429 MB\n",
      "[2025-12-10 17:38:08]   Saved 82,479,766 rows in 10987.3s (chunked)\n",
      "[2025-12-10 17:38:08] \n",
      "[LARGE 9/17] Processing matched pool: RMBMIT000103100520171\n",
      "[2025-12-10 17:38:08]   ECB: 47 MB, ESMA: 4429 MB\n",
      "[2025-12-10 17:38:10]   ECB size: 47 MB, ESMA size: 4429 MB\n",
      "[2025-12-10 17:38:10]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:38:10]   ECB size: 47 MB, ESMA size: 4429 MB\n",
      "[2025-12-10 17:38:10]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:40:19]   Found 216 total columns across both sources\n",
      "[2025-12-10 17:40:19]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:40:19]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:40:19]   Loading ECB data (28 files)...\n",
      "[2025-12-10 17:40:19]   Found 216 total columns across both sources\n",
      "[2025-12-10 17:40:19]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:40:19]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:40:19]   Loading ECB data (28 files)...\n",
      "[2025-12-10 17:40:30]   ECB: 733393 rows\n",
      "[2025-12-10 17:40:30]   ECB: 733393 rows\n",
      "[2025-12-10 17:40:49]   Wrote 733,393 ECB rows\n",
      "[2025-12-10 17:40:49]   Wrote 733,393 ECB rows\n",
      "[2025-12-10 17:40:49]   Processing 14 ESMA files...\n",
      "[2025-12-10 17:40:49]   Processing 14 ESMA files...\n",
      "[2025-12-10 17:42:33]     Processed 5/14 ESMA files, 2,998,147 total rows\n",
      "[2025-12-10 17:42:33]     Processed 5/14 ESMA files, 2,998,147 total rows\n",
      "[2025-12-10 17:44:15]     Processed 10/14 ESMA files, 5,197,638 total rows\n",
      "[2025-12-10 17:44:15]     Processed 10/14 ESMA files, 5,197,638 total rows\n",
      "[2025-12-10 17:45:36]   Saved 6,918,499 rows in 447.6s (chunked)\n",
      "[2025-12-10 17:45:36] \n",
      "[LARGE 10/17] Processing matched pool: RMBMUK000203100120065\n",
      "[2025-12-10 17:45:36]   ECB: 248 MB, ESMA: 457 MB\n",
      "[2025-12-10 17:45:36]   Saved 6,918,499 rows in 447.6s (chunked)\n",
      "[2025-12-10 17:45:36] \n",
      "[LARGE 10/17] Processing matched pool: RMBMUK000203100120065\n",
      "[2025-12-10 17:45:36]   ECB: 248 MB, ESMA: 457 MB\n",
      "[2025-12-10 17:45:38]   ECB size: 248 MB, ESMA size: 457 MB\n",
      "[2025-12-10 17:45:38]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:45:38]   ECB size: 248 MB, ESMA size: 457 MB\n",
      "[2025-12-10 17:45:38]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:47:32]   Found 212 total columns across both sources\n",
      "[2025-12-10 17:47:32]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:47:32]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:47:32]   Loading ECB data (11 files)...\n",
      "[2025-12-10 17:47:32]   Found 212 total columns across both sources\n",
      "[2025-12-10 17:47:32]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:47:32]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:47:32]   Loading ECB data (11 files)...\n",
      "[2025-12-10 17:49:27]   ECB: 4509701 rows\n",
      "[2025-12-10 17:49:27]   ECB: 4509701 rows\n",
      "[2025-12-10 17:51:59]   Wrote 4,509,701 ECB rows\n",
      "[2025-12-10 17:51:59]   Wrote 4,509,701 ECB rows\n",
      "[2025-12-10 17:52:01]   Processing 11 ESMA files...\n",
      "[2025-12-10 17:52:01]   Processing 11 ESMA files...\n",
      "[2025-12-10 17:52:18]     Processed 5/11 ESMA files, 4,812,172 total rows\n",
      "[2025-12-10 17:52:18]     Processed 5/11 ESMA files, 4,812,172 total rows\n",
      "[2025-12-10 17:52:33]     Processed 10/11 ESMA files, 5,123,707 total rows\n",
      "[2025-12-10 17:52:33]     Processed 10/11 ESMA files, 5,123,707 total rows\n",
      "[2025-12-10 17:52:41]   Saved 5,189,825 rows in 425.4s (chunked)\n",
      "[2025-12-10 17:52:41] \n",
      "[LARGE 11/17] Processing matched pool: RMBMUK000551100120075\n",
      "[2025-12-10 17:52:41]   ECB: 120 MB, ESMA: 688 MB\n",
      "[2025-12-10 17:52:41]   Saved 5,189,825 rows in 425.4s (chunked)\n",
      "[2025-12-10 17:52:41] \n",
      "[LARGE 11/17] Processing matched pool: RMBMUK000551100120075\n",
      "[2025-12-10 17:52:41]   ECB: 120 MB, ESMA: 688 MB\n",
      "[2025-12-10 17:52:43]   ECB size: 120 MB, ESMA size: 688 MB\n",
      "[2025-12-10 17:52:43]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:52:43]   ECB size: 120 MB, ESMA size: 688 MB\n",
      "[2025-12-10 17:52:43]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:53:15]   Found 198 total columns across both sources\n",
      "[2025-12-10 17:53:15]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:53:15]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:53:15]   Loading ECB data (26 files)...\n",
      "[2025-12-10 17:53:15]   Found 198 total columns across both sources\n",
      "[2025-12-10 17:53:15]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:53:15]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:53:15]   Loading ECB data (26 files)...\n",
      "[2025-12-10 17:53:32]   ECB: 1098337 rows\n",
      "[2025-12-10 17:53:32]   ECB: 1098337 rows\n",
      "[2025-12-10 17:54:05]   Wrote 1,098,337 ECB rows\n",
      "[2025-12-10 17:54:05]   Wrote 1,098,337 ECB rows\n",
      "[2025-12-10 17:54:05]   Processing 16 ESMA files...\n",
      "[2025-12-10 17:54:05]   Processing 16 ESMA files...\n",
      "[2025-12-10 17:54:19]     Processed 5/16 ESMA files, 1,406,852 total rows\n",
      "[2025-12-10 17:54:19]     Processed 5/16 ESMA files, 1,406,852 total rows\n",
      "[2025-12-10 17:54:33]     Processed 10/16 ESMA files, 1,709,624 total rows\n",
      "[2025-12-10 17:54:33]     Processed 10/16 ESMA files, 1,709,624 total rows\n",
      "[2025-12-10 17:54:47]     Processed 15/16 ESMA files, 2,006,777 total rows\n",
      "[2025-12-10 17:54:47]     Processed 15/16 ESMA files, 2,006,777 total rows\n",
      "[2025-12-10 17:54:50]   Saved 2,065,342 rows in 128.7s (chunked)\n",
      "[2025-12-10 17:54:50] \n",
      "[LARGE 12/17] Processing matched pool: RMBSBE000043100120081\n",
      "[2025-12-10 17:54:50]   ECB: 165 MB, ESMA: 901 MB\n",
      "[2025-12-10 17:54:50]   Saved 2,065,342 rows in 128.7s (chunked)\n",
      "[2025-12-10 17:54:50] \n",
      "[LARGE 12/17] Processing matched pool: RMBSBE000043100120081\n",
      "[2025-12-10 17:54:50]   ECB: 165 MB, ESMA: 901 MB\n",
      "[2025-12-10 17:54:52]   ECB size: 165 MB, ESMA size: 901 MB\n",
      "[2025-12-10 17:54:52]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:54:52]   ECB size: 165 MB, ESMA size: 901 MB\n",
      "[2025-12-10 17:54:52]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:55:57]   Found 215 total columns across both sources\n",
      "[2025-12-10 17:55:57]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:55:57]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:55:57]   Loading ECB data (16 files)...\n",
      "[2025-12-10 17:55:57]   Found 215 total columns across both sources\n",
      "[2025-12-10 17:55:57]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 17:55:57]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 17:55:57]   Loading ECB data (16 files)...\n",
      "[2025-12-10 17:56:44]   ECB: 2209328 rows\n",
      "[2025-12-10 17:56:44]   ECB: 2209328 rows\n",
      "[2025-12-10 17:57:52]   Wrote 2,209,328 ECB rows\n",
      "[2025-12-10 17:57:52]   Wrote 2,209,328 ECB rows\n",
      "[2025-12-10 17:57:52]   Processing 20 ESMA files...\n",
      "[2025-12-10 17:57:52]   Processing 20 ESMA files...\n",
      "[2025-12-10 17:58:08]     Processed 5/20 ESMA files, 2,523,456 total rows\n",
      "[2025-12-10 17:58:08]     Processed 5/20 ESMA files, 2,523,456 total rows\n",
      "[2025-12-10 17:58:22]     Processed 10/20 ESMA files, 2,809,150 total rows\n",
      "[2025-12-10 17:58:22]     Processed 10/20 ESMA files, 2,809,150 total rows\n",
      "[2025-12-10 17:58:37]     Processed 15/20 ESMA files, 3,099,238 total rows\n",
      "[2025-12-10 17:58:37]     Processed 15/20 ESMA files, 3,099,238 total rows\n",
      "[2025-12-10 17:58:53]     Processed 20/20 ESMA files, 3,427,542 total rows\n",
      "[2025-12-10 17:58:53]     Processed 20/20 ESMA files, 3,427,542 total rows\n",
      "[2025-12-10 17:58:54]   Saved 3,427,542 rows in 244.0s (chunked)\n",
      "[2025-12-10 17:58:54] \n",
      "[LARGE 13/17] Processing matched pool: RMBSBE000043100220113\n",
      "[2025-12-10 17:58:54]   ECB: 207 MB, ESMA: 457 MB\n",
      "[2025-12-10 17:58:54]   Saved 3,427,542 rows in 244.0s (chunked)\n",
      "[2025-12-10 17:58:54] \n",
      "[LARGE 13/17] Processing matched pool: RMBSBE000043100220113\n",
      "[2025-12-10 17:58:54]   ECB: 207 MB, ESMA: 457 MB\n",
      "[2025-12-10 17:58:56]   ECB size: 207 MB, ESMA size: 457 MB\n",
      "[2025-12-10 17:58:56]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 17:58:56]   ECB size: 207 MB, ESMA size: 457 MB\n",
      "[2025-12-10 17:58:56]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:00:10]   Found 212 total columns across both sources\n",
      "[2025-12-10 18:00:10]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:00:10]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:00:10]   Loading ECB data (19 files)...\n",
      "[2025-12-10 18:00:10]   Found 212 total columns across both sources\n",
      "[2025-12-10 18:00:10]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:00:10]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:00:10]   Loading ECB data (19 files)...\n",
      "[2025-12-10 18:01:18]   ECB: 2831893 rows\n",
      "[2025-12-10 18:01:18]   ECB: 2831893 rows\n",
      "[2025-12-10 18:02:46]   Wrote 2,831,893 ECB rows\n",
      "[2025-12-10 18:02:46]   Wrote 2,831,893 ECB rows\n",
      "[2025-12-10 18:02:47]   Processing 11 ESMA files...\n",
      "[2025-12-10 18:02:47]   Processing 11 ESMA files...\n",
      "[2025-12-10 18:03:03]     Processed 5/11 ESMA files, 3,134,364 total rows\n",
      "[2025-12-10 18:03:03]     Processed 5/11 ESMA files, 3,134,364 total rows\n",
      "[2025-12-10 18:03:18]     Processed 10/11 ESMA files, 3,445,899 total rows\n",
      "[2025-12-10 18:03:18]     Processed 10/11 ESMA files, 3,445,899 total rows\n",
      "[2025-12-10 18:03:22]   Saved 3,512,017 rows in 267.8s (chunked)\n",
      "[2025-12-10 18:03:22] \n",
      "[LARGE 14/17] Processing matched pool: RMBSBE000043100420150\n",
      "[2025-12-10 18:03:22]   ECB: 16 MB, ESMA: 454 MB\n",
      "[2025-12-10 18:03:22]   Saved 3,512,017 rows in 267.8s (chunked)\n",
      "[2025-12-10 18:03:22] \n",
      "[LARGE 14/17] Processing matched pool: RMBSBE000043100420150\n",
      "[2025-12-10 18:03:22]   ECB: 16 MB, ESMA: 454 MB\n",
      "[2025-12-10 18:03:24]   ECB size: 16 MB, ESMA size: 454 MB\n",
      "[2025-12-10 18:03:24]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:03:24]   ECB size: 16 MB, ESMA size: 454 MB\n",
      "[2025-12-10 18:03:24]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:03:36]   Found 214 total columns across both sources\n",
      "[2025-12-10 18:03:36]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:03:36]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:03:36]   Loading ECB data (21 files)...\n",
      "[2025-12-10 18:03:36]   Found 214 total columns across both sources\n",
      "[2025-12-10 18:03:36]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:03:36]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:03:36]   Loading ECB data (21 files)...\n",
      "[2025-12-10 18:03:40]   ECB: 206527 rows\n",
      "[2025-12-10 18:03:40]   ECB: 206527 rows\n",
      "[2025-12-10 18:03:46]   Wrote 206,527 ECB rows\n",
      "[2025-12-10 18:03:46]   Processing 15 ESMA files...\n",
      "[2025-12-10 18:03:46]   Wrote 206,527 ECB rows\n",
      "[2025-12-10 18:03:46]   Processing 15 ESMA files...\n",
      "[2025-12-10 18:03:55]     Processed 5/15 ESMA files, 405,689 total rows\n",
      "[2025-12-10 18:03:55]     Processed 5/15 ESMA files, 405,689 total rows\n",
      "[2025-12-10 18:04:06]     Processed 10/15 ESMA files, 623,690 total rows\n",
      "[2025-12-10 18:04:06]     Processed 10/15 ESMA files, 623,690 total rows\n",
      "[2025-12-10 18:04:18]     Processed 15/15 ESMA files, 862,745 total rows\n",
      "[2025-12-10 18:04:18]   Saved 862,745 rows in 55.8s (chunked)\n",
      "[2025-12-10 18:04:18] \n",
      "[LARGE 15/17] Processing matched pool: RMBSBE000043100520173\n",
      "[2025-12-10 18:04:18]   ECB: 370 MB, ESMA: 457 MB\n",
      "[2025-12-10 18:04:18]     Processed 15/15 ESMA files, 862,745 total rows\n",
      "[2025-12-10 18:04:18]   Saved 862,745 rows in 55.8s (chunked)\n",
      "[2025-12-10 18:04:18] \n",
      "[LARGE 15/17] Processing matched pool: RMBSBE000043100520173\n",
      "[2025-12-10 18:04:18]   ECB: 370 MB, ESMA: 457 MB\n",
      "[2025-12-10 18:04:19]   ECB size: 370 MB, ESMA size: 457 MB\n",
      "[2025-12-10 18:04:19]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:04:19]   ECB size: 370 MB, ESMA size: 457 MB\n",
      "[2025-12-10 18:04:19]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:05:54]   Found 212 total columns across both sources\n",
      "[2025-12-10 18:05:54]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:05:54]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:05:54]   Loading ECB data (60 files)...\n",
      "[2025-12-10 18:05:54]   Found 212 total columns across both sources\n",
      "[2025-12-10 18:05:54]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:05:54]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:05:54]   Loading ECB data (60 files)...\n",
      "[2025-12-10 18:07:25]   ECB: 4498964 rows\n",
      "[2025-12-10 18:07:25]   ECB: 4498964 rows\n",
      "[2025-12-10 18:09:55]   Wrote 4,498,964 ECB rows\n",
      "[2025-12-10 18:09:55]   Wrote 4,498,964 ECB rows\n",
      "[2025-12-10 18:09:57]   Processing 11 ESMA files...\n",
      "[2025-12-10 18:09:57]   Processing 11 ESMA files...\n",
      "[2025-12-10 18:10:12]     Processed 5/11 ESMA files, 4,801,435 total rows\n",
      "[2025-12-10 18:10:12]     Processed 5/11 ESMA files, 4,801,435 total rows\n",
      "[2025-12-10 18:10:27]     Processed 10/11 ESMA files, 5,112,970 total rows\n",
      "[2025-12-10 18:10:27]     Processed 10/11 ESMA files, 5,112,970 total rows\n",
      "[2025-12-10 18:10:34]   Saved 5,179,088 rows in 376.4s (chunked)\n",
      "[2025-12-10 18:10:34] \n",
      "[LARGE 16/17] Processing matched pool: RMBSDE000097100120083\n",
      "[2025-12-10 18:10:34]   ECB: 101 MB, ESMA: 431 MB\n",
      "[2025-12-10 18:10:34]   Saved 5,179,088 rows in 376.4s (chunked)\n",
      "[2025-12-10 18:10:34] \n",
      "[LARGE 16/17] Processing matched pool: RMBSDE000097100120083\n",
      "[2025-12-10 18:10:34]   ECB: 101 MB, ESMA: 431 MB\n",
      "[2025-12-10 18:10:36]   ECB size: 101 MB, ESMA size: 431 MB\n",
      "[2025-12-10 18:10:36]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:10:36]   ECB size: 101 MB, ESMA size: 431 MB\n",
      "[2025-12-10 18:10:36]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:11:17]   Found 210 total columns across both sources\n",
      "[2025-12-10 18:11:17]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:11:17]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:11:17]   Loading ECB data (22 files)...\n",
      "[2025-12-10 18:11:17]   Found 210 total columns across both sources\n",
      "[2025-12-10 18:11:17]   Strategy: Load ECB (smaller), chunk ESMA files\n",
      "[2025-12-10 18:11:17]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:11:17]   Loading ECB data (22 files)...\n",
      "[2025-12-10 18:11:48]   ECB: 1579072 rows\n",
      "[2025-12-10 18:11:48]   ECB: 1579072 rows\n",
      "[2025-12-10 18:12:32]   Wrote 1,579,072 ECB rows\n",
      "[2025-12-10 18:12:32]   Wrote 1,579,072 ECB rows\n",
      "[2025-12-10 18:12:32]   Processing 19 ESMA files...\n",
      "[2025-12-10 18:12:32]   Processing 19 ESMA files...\n",
      "[2025-12-10 18:12:41]     Processed 5/19 ESMA files, 1,739,156 total rows\n",
      "[2025-12-10 18:12:41]     Processed 5/19 ESMA files, 1,739,156 total rows\n",
      "[2025-12-10 18:12:49]     Processed 10/19 ESMA files, 1,894,642 total rows\n",
      "[2025-12-10 18:12:49]     Processed 10/19 ESMA files, 1,894,642 total rows\n",
      "[2025-12-10 18:12:57]     Processed 15/19 ESMA files, 2,045,226 total rows\n",
      "[2025-12-10 18:12:57]     Processed 15/19 ESMA files, 2,045,226 total rows\n",
      "[2025-12-10 18:13:04]   Saved 2,159,244 rows in 149.6s (chunked)\n",
      "[2025-12-10 18:13:04] \n",
      "[LARGE 17/17] Processing matched pool: RMBSDE000556100120088\n",
      "[2025-12-10 18:13:04]   ECB: 157 MB, ESMA: 18 MB\n",
      "[2025-12-10 18:13:04]   Saved 2,159,244 rows in 149.6s (chunked)\n",
      "[2025-12-10 18:13:04] \n",
      "[LARGE 17/17] Processing matched pool: RMBSDE000556100120088\n",
      "[2025-12-10 18:13:04]   ECB: 157 MB, ESMA: 18 MB\n",
      "[2025-12-10 18:13:05]   ECB size: 157 MB, ESMA size: 18 MB\n",
      "[2025-12-10 18:13:05]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:13:05]   ECB size: 157 MB, ESMA size: 18 MB\n",
      "[2025-12-10 18:13:05]   Phase 1: Scanning files to determine column schema...\n",
      "[2025-12-10 18:13:31]   Found 137 total columns across both sources\n",
      "[2025-12-10 18:13:31]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 18:13:31]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:13:31]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 18:13:31]   Found 137 total columns across both sources\n",
      "[2025-12-10 18:13:31]   Strategy: Load ESMA (smaller), chunk ECB files\n",
      "[2025-12-10 18:13:31]   No temporal overlap - skipping deduplication\n",
      "[2025-12-10 18:13:31]   Loading ESMA data (5 files)...\n",
      "[2025-12-10 18:13:31]   ESMA: 26576 rows\n",
      "[2025-12-10 18:13:31]   ESMA: 26576 rows\n",
      "[2025-12-10 18:13:32]   Processing 30 ECB files...\n",
      "[2025-12-10 18:13:32]   Processing 30 ECB files...\n",
      "[2025-12-10 18:14:28]     Processed 20/30 ECB files, 1,477,335 ECB rows kept\n",
      "[2025-12-10 18:14:28]     Processed 20/30 ECB files, 1,477,335 ECB rows kept\n",
      "[2025-12-10 18:14:56]   Added 26,576 ESMA rows (preferred)\n",
      "[2025-12-10 18:14:56]   Saved 2,247,162 rows in 112.0s (chunked)\n",
      "[2025-12-10 18:14:56] \\nMatched pools complete: 18607.1s\n",
      "[2025-12-10 18:14:56]   Added 26,576 ESMA rows (preferred)\n",
      "[2025-12-10 18:14:56]   Saved 2,247,162 rows in 112.0s (chunked)\n",
      "[2025-12-10 18:14:56] \\nMatched pools complete: 18607.1s\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 8: Main Processing Loop - MATCHED POOLS (with large pool handling)\n",
    "# =============================================================================\n",
    "log_message(\"=\"*70)\n",
    "log_message(\"STARTING MATCHED POOLS PROCESSING\")\n",
    "log_message(\"=\"*70)\n",
    "\n",
    "# Scan output folders to find already-completed pools (file-based tracking)\n",
    "completed_pools = scan_completed_pools()\n",
    "completed_matched = completed_pools['matched']\n",
    "\n",
    "# Track total rows for this session\n",
    "session_rows_processed = 0\n",
    "\n",
    "# Get pools to process (skip already completed)\n",
    "pools_to_process = [(ecb_id, info['esma_pool']) \n",
    "                    for ecb_id, info in matched_pools.items() \n",
    "                    if ecb_id not in completed_matched]\n",
    "\n",
    "log_message(f\"Matched pools: {len(matched_pools)} total, {len(pools_to_process)} remaining\")\n",
    "\n",
    "# Identify large pools - check BOTH ECB and ESMA sides (either can cause memory issues)\n",
    "def is_large_matched_pool(ecb_id, esma_id):\n",
    "    return is_large_pool(ecb_id, 'ecb') or is_large_pool(esma_id, 'esma')\n",
    "\n",
    "large_matched = [(e, s) for e, s in pools_to_process if is_large_matched_pool(e, s)]\n",
    "normal_matched = [(e, s) for e, s in pools_to_process if not is_large_matched_pool(e, s)]\n",
    "log_message(f\"  Normal pools: {len(normal_matched)}, Large pools (chunked): {len(large_matched)}\")\n",
    "\n",
    "matched_start = time.time()\n",
    "\n",
    "# Process normal pools first\n",
    "for i, (ecb_pool_id, esma_pool_id) in enumerate(normal_matched):\n",
    "    pool_start = time.time()\n",
    "    log_message(f\"\\n[{i+1}/{len(normal_matched)}] Processing matched pool: {ecb_pool_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Process pool\n",
    "        result_df = process_matched_pool(ecb_pool_id, esma_pool_id)\n",
    "        \n",
    "        # Save result\n",
    "        rows_saved = save_pool_result(result_df, \"matched\", ecb_pool_id)\n",
    "        \n",
    "        # Free memory\n",
    "        del result_df\n",
    "        gc.collect()\n",
    "        \n",
    "        # Track session progress (file-based tracking means completion is automatic)\n",
    "        session_rows_processed += rows_saved\n",
    "        \n",
    "        pool_elapsed = time.time() - pool_start\n",
    "        log_message(f\"  Saved {rows_saved:,} rows in {pool_elapsed:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR processing {ecb_pool_id}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        gc.collect()\n",
    "\n",
    "# Process large pools with chunked mode\n",
    "for i, (ecb_pool_id, esma_pool_id) in enumerate(large_matched):\n",
    "    pool_start = time.time()\n",
    "    ecb_size_mb = get_pool_compressed_size(ecb_pool_id, 'ecb') / 1024 / 1024\n",
    "    esma_size_mb = get_pool_compressed_size(esma_pool_id, 'esma') / 1024 / 1024\n",
    "    log_message(f\"\\n[LARGE {i+1}/{len(large_matched)}] Processing matched pool: {ecb_pool_id}\")\n",
    "    log_message(f\"  ECB: {ecb_size_mb:.0f} MB, ESMA: {esma_size_mb:.0f} MB\")\n",
    "    \n",
    "    # Aggressive memory cleanup before large pool\n",
    "    gc.collect()\n",
    "    gc.collect()  # Run twice to catch cyclic references\n",
    "    time.sleep(1)  # Brief pause to let OS reclaim memory\n",
    "    \n",
    "    try:\n",
    "        # Setup output path with ATOMIC write pattern\n",
    "        subdir = os.path.join(OUTPUT_DIR, \"matched\")\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "        safe_pool_id = ecb_pool_id.replace('/', '_').replace('\\\\', '_')\n",
    "        output_path = os.path.join(subdir, f\"{safe_pool_id}.csv\")\n",
    "        temp_path = output_path + \".tmp\"  # Write to temp file first\n",
    "        \n",
    "        # Remove existing files if any (fresh start)\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        \n",
    "        # Process with chunked mode - writes to TEMP file\n",
    "        rows_saved = process_matched_pool_chunked(ecb_pool_id, esma_pool_id, temp_path)\n",
    "        \n",
    "        # ATOMIC: Rename temp to final ONLY after complete success\n",
    "        os.replace(temp_path, output_path)\n",
    "        \n",
    "        # Track session progress (file-based tracking means completion is automatic)\n",
    "        session_rows_processed += rows_saved\n",
    "        \n",
    "        pool_elapsed = time.time() - pool_start\n",
    "        log_message(f\"  Saved {rows_saved:,} rows in {pool_elapsed:.1f}s (chunked)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR processing {ecb_pool_id}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        # Clean up temp file if it exists\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        gc.collect()\n",
    "\n",
    "matched_elapsed = time.time() - matched_start\n",
    "log_message(f\"\\\\nMatched pools complete: {matched_elapsed:.1f}s\")\n",
    "log_message(f\"Session rows processed: {session_rows_processed:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c89e110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 20:19:28] ======================================================================\n",
      "[2025-12-10 20:19:28] STARTING ECB-ONLY POOLS PROCESSING\n",
      "[2025-12-10 20:19:28] ======================================================================\n",
      "[2025-12-10 20:19:28] ECB-only pools: 36 total, 36 remaining\n",
      "[2025-12-10 20:19:28]   Normal pools: 24, Large pools (chunked): 12\n",
      "[2025-12-10 20:19:28] \n",
      "[1/24] Processing ECB-only pool: RMBMNL000125100120063\n",
      "[2025-12-10 20:19:32]   ECB rows: 300969\n",
      "[2025-12-10 20:19:32]   ECB rows: 300969\n",
      "[2025-12-10 20:19:43]   Saved 300,969 rows in 15.0s\n",
      "[2025-12-10 20:19:43] \n",
      "[2/24] Processing ECB-only pool: RMBMDE000950100120151\n",
      "[2025-12-10 20:19:43]   Saved 300,969 rows in 15.0s\n",
      "[2025-12-10 20:19:43] \n",
      "[2/24] Processing ECB-only pool: RMBMDE000950100120151\n",
      "[2025-12-10 20:19:45]   ECB rows: 106140\n",
      "[2025-12-10 20:19:45]   ECB rows: 106140\n",
      "[2025-12-10 20:19:47]   Saved 106,140 rows in 4.6s\n",
      "[2025-12-10 20:19:47] \n",
      "[3/24] Processing ECB-only pool: RMBSES000045100120098\n",
      "[2025-12-10 20:19:48]   ECB rows: 13182\n",
      "[2025-12-10 20:19:47]   Saved 106,140 rows in 4.6s\n",
      "[2025-12-10 20:19:47] \n",
      "[3/24] Processing ECB-only pool: RMBSES000045100120098\n",
      "[2025-12-10 20:19:48]   ECB rows: 13182\n",
      "[2025-12-10 20:19:48]   Saved 13,182 rows in 0.6s\n",
      "[2025-12-10 20:19:48] \n",
      "[4/24] Processing ECB-only pool: RMBSBE000044100320093\n",
      "[2025-12-10 20:19:48]   Saved 13,182 rows in 0.6s\n",
      "[2025-12-10 20:19:48] \n",
      "[4/24] Processing ECB-only pool: RMBSBE000044100320093\n",
      "[2025-12-10 20:19:51]   ECB rows: 255036\n",
      "[2025-12-10 20:19:51]   ECB rows: 255036\n",
      "[2025-12-10 20:19:54]   Saved 255,036 rows in 5.8s\n",
      "[2025-12-10 20:19:54] \n",
      "[5/24] Processing ECB-only pool: RMBSDE000055100220089\n",
      "[2025-12-10 20:19:54]   Saved 255,036 rows in 5.8s\n",
      "[2025-12-10 20:19:54] \n",
      "[5/24] Processing ECB-only pool: RMBSDE000055100220089\n",
      "[2025-12-10 20:20:01]   ECB rows: 509035\n",
      "[2025-12-10 20:20:01]   ECB rows: 509035\n",
      "[2025-12-10 20:20:11]   Saved 509,035 rows in 17.1s\n",
      "[2025-12-10 20:20:11] \n",
      "[6/24] Processing ECB-only pool: RMBMNL000185100320139\n",
      "[2025-12-10 20:20:11]   Saved 509,035 rows in 17.1s\n",
      "[2025-12-10 20:20:11] \n",
      "[6/24] Processing ECB-only pool: RMBMNL000185100320139\n",
      "[2025-12-10 20:20:14]   ECB rows: 194844\n",
      "[2025-12-10 20:20:14]   ECB rows: 194844\n",
      "[2025-12-10 20:20:18]   Saved 194,844 rows in 7.3s\n",
      "[2025-12-10 20:20:18] \n",
      "[7/24] Processing ECB-only pool: RMBSBE000044100420117\n",
      "[2025-12-10 20:20:18]   Saved 194,844 rows in 7.3s\n",
      "[2025-12-10 20:20:18] \n",
      "[7/24] Processing ECB-only pool: RMBSBE000044100420117\n",
      "[2025-12-10 20:20:24]   ECB rows: 631624\n",
      "[2025-12-10 20:20:24]   ECB rows: 631624\n",
      "[2025-12-10 20:20:31]   Saved 631,624 rows in 12.7s\n",
      "[2025-12-10 20:20:31] \n",
      "[8/24] Processing ECB-only pool: RMBMES000138100220050\n",
      "[2025-12-10 20:20:31]   Saved 631,624 rows in 12.7s\n",
      "[2025-12-10 20:20:31] \n",
      "[8/24] Processing ECB-only pool: RMBMES000138100220050\n",
      "[2025-12-10 20:20:33]   ECB rows: 112998\n",
      "[2025-12-10 20:20:33]   ECB rows: 112998\n",
      "[2025-12-10 20:20:35]   Saved 112,998 rows in 4.4s\n",
      "[2025-12-10 20:20:35] \n",
      "[9/24] Processing ECB-only pool: RMBMES000138100120037\n",
      "[2025-12-10 20:20:35]   Saved 112,998 rows in 4.4s\n",
      "[2025-12-10 20:20:35] \n",
      "[9/24] Processing ECB-only pool: RMBMES000138100120037\n",
      "[2025-12-10 20:20:37]   ECB rows: 127860\n",
      "[2025-12-10 20:20:37]   ECB rows: 127860\n",
      "[2025-12-10 20:20:40]   Saved 127,860 rows in 5.0s\n",
      "[2025-12-10 20:20:40] \n",
      "[10/24] Processing ECB-only pool: RMBSBE000108100320112\n",
      "[2025-12-10 20:20:40]   Saved 127,860 rows in 5.0s\n",
      "[2025-12-10 20:20:40] \n",
      "[10/24] Processing ECB-only pool: RMBSBE000108100320112\n",
      "[2025-12-10 20:20:46]   ECB rows: 422254\n",
      "[2025-12-10 20:20:46]   ECB rows: 422254\n",
      "[2025-12-10 20:20:53]   Saved 422,254 rows in 12.4s\n",
      "[2025-12-10 20:20:53] \n",
      "[11/24] Processing ECB-only pool: RMBSBE000170100120090\n",
      "[2025-12-10 20:20:53]   Saved 422,254 rows in 12.4s\n",
      "[2025-12-10 20:20:53] \n",
      "[11/24] Processing ECB-only pool: RMBSBE000170100120090\n",
      "[2025-12-10 20:20:53]   ECB rows: 46158\n",
      "[2025-12-10 20:20:53]   ECB rows: 46158\n",
      "[2025-12-10 20:20:55]   Saved 46,158 rows in 1.8s\n",
      "[2025-12-10 20:20:55] \n",
      "[12/24] Processing ECB-only pool: RMBSES000045100220096\n",
      "[2025-12-10 20:20:55]   Saved 46,158 rows in 1.8s\n",
      "[2025-12-10 20:20:55] \n",
      "[12/24] Processing ECB-only pool: RMBSES000045100220096\n",
      "[2025-12-10 20:20:55]   ECB rows: 18234\n",
      "[2025-12-10 20:20:55]   ECB rows: 18234\n",
      "[2025-12-10 20:20:55]   Saved 18,234 rows in 0.7s\n",
      "[2025-12-10 20:20:55] \n",
      "[13/24] Processing ECB-only pool: RMBSES000045100720046\n",
      "[2025-12-10 20:20:55]   ECB rows: 1766\n",
      "[2025-12-10 20:20:56]   Saved 1,766 rows in 0.1s\n",
      "[2025-12-10 20:20:56] \n",
      "[14/24] Processing ECB-only pool: RMBSBE000108100120082\n",
      "[2025-12-10 20:20:55]   Saved 18,234 rows in 0.7s\n",
      "[2025-12-10 20:20:55] \n",
      "[13/24] Processing ECB-only pool: RMBSES000045100720046\n",
      "[2025-12-10 20:20:55]   ECB rows: 1766\n",
      "[2025-12-10 20:20:56]   Saved 1,766 rows in 0.1s\n",
      "[2025-12-10 20:20:56] \n",
      "[14/24] Processing ECB-only pool: RMBSBE000108100120082\n",
      "[2025-12-10 20:21:08]   ECB rows: 986326\n",
      "[2025-12-10 20:21:08]   ECB rows: 986326\n",
      "[2025-12-10 20:21:23]   Saved 986,326 rows in 27.5s\n",
      "[2025-12-10 20:21:23] \n",
      "[15/24] Processing ECB-only pool: RMBSBE000161100320178\n",
      "[2025-12-10 20:21:23]   Saved 986,326 rows in 27.5s\n",
      "[2025-12-10 20:21:23] \n",
      "[15/24] Processing ECB-only pool: RMBSBE000161100320178\n",
      "[2025-12-10 20:21:24]   Loaded RMBSBE000161100320178_2018-12-31_Pool.csv.gz using zlib fallback (6,697 rows)\n",
      "[2025-12-10 20:21:24]   Loaded RMBSBE000161100320178_2018-12-31_Pool.csv.gz using zlib fallback (6,697 rows)\n",
      "[2025-12-10 20:21:25]   ECB rows: 147764\n",
      "[2025-12-10 20:21:25]   ECB rows: 147764\n",
      "[2025-12-10 20:21:28]   Saved 147,764 rows in 5.3s\n",
      "[2025-12-10 20:21:28] \n",
      "[16/24] Processing ECB-only pool: RMBSBE000044100120071\n",
      "[2025-12-10 20:21:28]   Saved 147,764 rows in 5.3s\n",
      "[2025-12-10 20:21:28] \n",
      "[16/24] Processing ECB-only pool: RMBSBE000044100120071\n",
      "[2025-12-10 20:21:47]   ECB rows: 1722715\n",
      "[2025-12-10 20:21:47]   ECB rows: 1722715\n",
      "[2025-12-10 20:22:09]   Saved 1,722,715 rows in 40.5s\n",
      "[2025-12-10 20:22:09] \n",
      "[17/24] Processing ECB-only pool: RMBMNL000185100220115\n",
      "[2025-12-10 20:22:09]   Saved 1,722,715 rows in 40.5s\n",
      "[2025-12-10 20:22:09] \n",
      "[17/24] Processing ECB-only pool: RMBMNL000185100220115\n",
      "[2025-12-10 20:22:11]   ECB rows: 173160\n",
      "[2025-12-10 20:22:11]   ECB rows: 173160\n",
      "[2025-12-10 20:22:15]   Saved 173,160 rows in 5.9s\n",
      "[2025-12-10 20:22:15] \n",
      "[18/24] Processing ECB-only pool: RMBSDE000190100120086\n",
      "[2025-12-10 20:22:15]   Saved 173,160 rows in 5.9s\n",
      "[2025-12-10 20:22:15] \n",
      "[18/24] Processing ECB-only pool: RMBSDE000190100120086\n",
      "[2025-12-10 20:22:35]   ECB rows: 1313593\n",
      "[2025-12-10 20:22:35]   ECB rows: 1313593\n",
      "[2025-12-10 20:23:03]   Saved 1,313,593 rows in 48.4s\n",
      "[2025-12-10 20:23:03] \n",
      "[19/24] Processing ECB-only pool: RMBSBE000108100220106\n",
      "[2025-12-10 20:23:03]   Saved 1,313,593 rows in 48.4s\n",
      "[2025-12-10 20:23:03] \n",
      "[19/24] Processing ECB-only pool: RMBSBE000108100220106\n",
      "[2025-12-10 20:23:07]   ECB rows: 331098\n",
      "[2025-12-10 20:23:07]   ECB rows: 331098\n",
      "[2025-12-10 20:23:13]   Saved 331,098 rows in 9.5s\n",
      "[2025-12-10 20:23:13] \n",
      "[20/24] Processing ECB-only pool: RMBSBE000161100120115\n",
      "[2025-12-10 20:23:13]   Saved 331,098 rows in 9.5s\n",
      "[2025-12-10 20:23:13] \n",
      "[20/24] Processing ECB-only pool: RMBSBE000161100120115\n",
      "[2025-12-10 20:23:15]   ECB rows: 123329\n",
      "[2025-12-10 20:23:15]   ECB rows: 123329\n",
      "[2025-12-10 20:23:17]   Saved 123,329 rows in 4.6s\n",
      "[2025-12-10 20:23:17] \n",
      "[21/24] Processing ECB-only pool: RMBMES000138100320066\n",
      "[2025-12-10 20:23:17]   Saved 123,329 rows in 4.6s\n",
      "[2025-12-10 20:23:17] \n",
      "[21/24] Processing ECB-only pool: RMBMES000138100320066\n",
      "[2025-12-10 20:23:23]   ECB rows: 374424\n",
      "[2025-12-10 20:23:23]   ECB rows: 374424\n",
      "[2025-12-10 20:23:31]   Saved 374,424 rows in 14.0s\n",
      "[2025-12-10 20:23:31] \n",
      "[22/24] Processing ECB-only pool: RMBSBE000044100220087\n",
      "[2025-12-10 20:23:31]   Saved 374,424 rows in 14.0s\n",
      "[2025-12-10 20:23:31] \n",
      "[22/24] Processing ECB-only pool: RMBSBE000044100220087\n",
      "[2025-12-10 20:23:33]   ECB rows: 124766\n",
      "[2025-12-10 20:23:33]   ECB rows: 124766\n",
      "[2025-12-10 20:23:34]   Saved 124,766 rows in 2.6s\n",
      "[2025-12-10 20:23:34] \n",
      "[23/24] Processing ECB-only pool: RMBMIT000103100220095\n",
      "[2025-12-10 20:23:34]   Saved 124,766 rows in 2.6s\n",
      "[2025-12-10 20:23:34] \n",
      "[23/24] Processing ECB-only pool: RMBMIT000103100220095\n",
      "[2025-12-10 20:23:37]   ECB rows: 323878\n",
      "[2025-12-10 20:23:37]   ECB rows: 323878\n",
      "[2025-12-10 20:23:41]   Saved 323,878 rows in 7.3s\n",
      "[2025-12-10 20:23:41] \n",
      "[24/24] Processing ECB-only pool: RMBSBE000161100220121\n",
      "[2025-12-10 20:23:41]   Saved 323,878 rows in 7.3s\n",
      "[2025-12-10 20:23:41] \n",
      "[24/24] Processing ECB-only pool: RMBSBE000161100220121\n",
      "[2025-12-10 20:23:44]   ECB rows: 189227\n",
      "[2025-12-10 20:23:44]   ECB rows: 189227\n",
      "[2025-12-10 20:23:49]   Saved 189,227 rows in 7.2s\n",
      "[2025-12-10 20:23:49] \n",
      "[LARGE 1/12] Processing ECB-only pool: RMBMNL000125100220111 (191 MB compressed)\n",
      "[2025-12-10 20:23:49]   Scanning 15 files for column schema...\n",
      "[2025-12-10 20:23:49]   Saved 189,227 rows in 7.2s\n",
      "[2025-12-10 20:23:49] \n",
      "[LARGE 1/12] Processing ECB-only pool: RMBMNL000125100220111 (191 MB compressed)\n",
      "[2025-12-10 20:23:49]   Scanning 15 files for column schema...\n",
      "[2025-12-10 20:24:33]   Found 162 columns\n",
      "[2025-12-10 20:24:33]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 20:24:33]   Found 162 columns\n",
      "[2025-12-10 20:24:33]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 20:25:44]   Saved 1,903,914 rows in 115.3s (chunked)\n",
      "[2025-12-10 20:25:44] \n",
      "[LARGE 2/12] Processing ECB-only pool: RMBSDE000055100320152 (903 MB compressed)\n",
      "[2025-12-10 20:25:44]   Scanning 31 files for column schema...\n",
      "[2025-12-10 20:25:44]   Saved 1,903,914 rows in 115.3s (chunked)\n",
      "[2025-12-10 20:25:44] \n",
      "[LARGE 2/12] Processing ECB-only pool: RMBSDE000055100320152 (903 MB compressed)\n",
      "[2025-12-10 20:25:44]   Scanning 31 files for column schema...\n",
      "[2025-12-10 20:28:27]   Found 161 columns\n",
      "[2025-12-10 20:28:27]   Processing 31 files in chunked mode...\n",
      "[2025-12-10 20:28:27]   Found 161 columns\n",
      "[2025-12-10 20:28:27]   Processing 31 files in chunked mode...\n",
      "[2025-12-10 20:31:36]     Processed 20/31 files, 4,745,169 rows so far\n",
      "[2025-12-10 20:31:36]     Processed 20/31 files, 4,745,169 rows so far\n",
      "[2025-12-10 20:33:18]   Saved 7,325,954 rows in 454.4s (chunked)\n",
      "[2025-12-10 20:33:18] \n",
      "[LARGE 3/12] Processing ECB-only pool: RMBMUK000172100420082 (507 MB compressed)\n",
      "[2025-12-10 20:33:18]   Scanning 96 files for column schema...\n",
      "[2025-12-10 20:33:18]   Saved 7,325,954 rows in 454.4s (chunked)\n",
      "[2025-12-10 20:33:18] \n",
      "[LARGE 3/12] Processing ECB-only pool: RMBMUK000172100420082 (507 MB compressed)\n",
      "[2025-12-10 20:33:18]   Scanning 96 files for column schema...\n",
      "[2025-12-10 20:35:03]   Found 146 columns\n",
      "[2025-12-10 20:35:03]   Processing 96 files in chunked mode...\n",
      "[2025-12-10 20:35:03]   Found 146 columns\n",
      "[2025-12-10 20:35:03]   Processing 96 files in chunked mode...\n",
      "[2025-12-10 20:36:22]     Processed 20/96 files, 2,145,794 rows so far\n",
      "[2025-12-10 20:36:22]     Processed 20/96 files, 2,145,794 rows so far\n",
      "[2025-12-10 20:37:21]     Processed 40/96 files, 3,743,173 rows so far\n",
      "[2025-12-10 20:37:21]     Processed 40/96 files, 3,743,173 rows so far\n",
      "[2025-12-10 20:38:07]     Processed 60/96 files, 5,012,462 rows so far\n",
      "[2025-12-10 20:38:07]     Processed 60/96 files, 5,012,462 rows so far\n",
      "[2025-12-10 20:38:59]     Processed 80/96 files, 6,456,341 rows so far\n",
      "[2025-12-10 20:38:59]     Processed 80/96 files, 6,456,341 rows so far\n",
      "[2025-12-10 20:39:35]   Saved 7,462,635 rows in 376.6s (chunked)\n",
      "[2025-12-10 20:39:35] \n",
      "[LARGE 4/12] Processing ECB-only pool: RMBSDE000055100120081 (179 MB compressed)\n",
      "[2025-12-10 20:39:35]   Scanning 27 files for column schema...\n",
      "[2025-12-10 20:39:35]   Saved 7,462,635 rows in 376.6s (chunked)\n",
      "[2025-12-10 20:39:35] \n",
      "[LARGE 4/12] Processing ECB-only pool: RMBSDE000055100120081 (179 MB compressed)\n",
      "[2025-12-10 20:39:35]   Scanning 27 files for column schema...\n",
      "[2025-12-10 20:40:14]   Found 161 columns\n",
      "[2025-12-10 20:40:14]   Processing 27 files in chunked mode...\n",
      "[2025-12-10 20:40:14]   Found 161 columns\n",
      "[2025-12-10 20:40:14]   Processing 27 files in chunked mode...\n",
      "[2025-12-10 20:40:44]     Processed 20/27 files, 793,899 rows so far\n",
      "[2025-12-10 20:40:44]     Processed 20/27 files, 793,899 rows so far\n",
      "[2025-12-10 20:41:22]   Saved 1,881,272 rows in 107.2s (chunked)\n",
      "[2025-12-10 20:41:22] \n",
      "[LARGE 5/12] Processing ECB-only pool: RMBSDE000055100520173 (1728 MB compressed)\n",
      "[2025-12-10 20:41:22]   Scanning 59 files for column schema...\n",
      "[2025-12-10 20:41:22]   Saved 1,881,272 rows in 107.2s (chunked)\n",
      "[2025-12-10 20:41:22] \n",
      "[LARGE 5/12] Processing ECB-only pool: RMBSDE000055100520173 (1728 MB compressed)\n",
      "[2025-12-10 20:41:22]   Scanning 59 files for column schema...\n",
      "[2025-12-10 20:47:18]   Found 161 columns\n",
      "[2025-12-10 20:47:18]   Processing 59 files in chunked mode...\n",
      "[2025-12-10 20:47:18]   Found 161 columns\n",
      "[2025-12-10 20:47:18]   Processing 59 files in chunked mode...\n",
      "[2025-12-10 20:50:08]     Processed 20/59 files, 4,483,815 rows so far\n",
      "[2025-12-10 20:50:08]     Processed 20/59 files, 4,483,815 rows so far\n",
      "[2025-12-10 20:53:40]     Processed 40/59 files, 9,956,928 rows so far\n",
      "[2025-12-10 20:53:40]     Processed 40/59 files, 9,956,928 rows so far\n",
      "[2025-12-10 20:57:04]   Saved 15,154,221 rows in 942.4s (chunked)\n",
      "[2025-12-10 20:57:04] \n",
      "[LARGE 6/12] Processing ECB-only pool: RMBMUK000172100320068 (695 MB compressed)\n",
      "[2025-12-10 20:57:04]   Scanning 133 files for column schema...\n",
      "[2025-12-10 20:57:04]   Saved 15,154,221 rows in 942.4s (chunked)\n",
      "[2025-12-10 20:57:04] \n",
      "[LARGE 6/12] Processing ECB-only pool: RMBMUK000172100320068 (695 MB compressed)\n",
      "[2025-12-10 20:57:04]   Scanning 133 files for column schema...\n",
      "[2025-12-10 20:59:31]   Found 145 columns\n",
      "[2025-12-10 20:59:31]   Processing 133 files in chunked mode...\n",
      "[2025-12-10 20:59:31]   Found 145 columns\n",
      "[2025-12-10 20:59:31]   Processing 133 files in chunked mode...\n",
      "[2025-12-10 21:01:09]     Processed 20/133 files, 2,625,025 rows so far\n",
      "[2025-12-10 21:01:09]     Processed 20/133 files, 2,625,025 rows so far\n",
      "[2025-12-10 21:02:15]     Processed 40/133 files, 4,479,299 rows so far\n",
      "[2025-12-10 21:02:15]     Processed 40/133 files, 4,479,299 rows so far\n",
      "[2025-12-10 21:03:02]     Processed 60/133 files, 5,801,461 rows so far\n",
      "[2025-12-10 21:03:02]     Processed 60/133 files, 5,801,461 rows so far\n",
      "[2025-12-10 21:04:05]     Processed 80/133 files, 7,532,541 rows so far\n",
      "[2025-12-10 21:04:05]     Processed 80/133 files, 7,532,541 rows so far\n",
      "[2025-12-10 21:04:58]     Processed 100/133 files, 9,033,406 rows so far\n",
      "[2025-12-10 21:04:58]     Processed 100/133 files, 9,033,406 rows so far\n",
      "[2025-12-10 21:05:26]     Processed 120/133 files, 9,857,213 rows so far\n",
      "[2025-12-10 21:05:26]     Processed 120/133 files, 9,857,213 rows so far\n",
      "[2025-12-10 21:05:42]   Saved 10,340,447 rows in 517.7s (chunked)\n",
      "[2025-12-10 21:05:42] \n",
      "[LARGE 7/12] Processing ECB-only pool: RMBMUK000064100120061 (464 MB compressed)\n",
      "[2025-12-10 21:05:42]   Scanning 62 files for column schema...\n",
      "[2025-12-10 21:05:42]   Saved 10,340,447 rows in 517.7s (chunked)\n",
      "[2025-12-10 21:05:42] \n",
      "[LARGE 7/12] Processing ECB-only pool: RMBMUK000064100120061 (464 MB compressed)\n",
      "[2025-12-10 21:05:42]   Scanning 62 files for column schema...\n",
      "[2025-12-10 21:07:15]   Found 162 columns\n",
      "[2025-12-10 21:07:15]   Processing 62 files in chunked mode...\n",
      "[2025-12-10 21:07:15]   Found 162 columns\n",
      "[2025-12-10 21:07:15]   Processing 62 files in chunked mode...\n",
      "[2025-12-10 21:08:22]     Processed 20/62 files, 1,769,971 rows so far\n",
      "[2025-12-10 21:08:22]     Processed 20/62 files, 1,769,971 rows so far\n",
      "[2025-12-10 21:09:10]     Processed 40/62 files, 3,166,222 rows so far\n",
      "[2025-12-10 21:09:10]     Processed 40/62 files, 3,166,222 rows so far\n",
      "[2025-12-10 21:09:44]     Processed 60/62 files, 4,181,402 rows so far\n",
      "[2025-12-10 21:09:44]     Processed 60/62 files, 4,181,402 rows so far\n",
      "[2025-12-10 21:09:47]   Saved 4,266,948 rows in 245.1s (chunked)\n",
      "[2025-12-10 21:09:47] \n",
      "[LARGE 8/12] Processing ECB-only pool: RMBMNL000125100420117 (208 MB compressed)\n",
      "[2025-12-10 21:09:47]   Scanning 11 files for column schema...\n",
      "[2025-12-10 21:09:47]   Saved 4,266,948 rows in 245.1s (chunked)\n",
      "[2025-12-10 21:09:47] \n",
      "[LARGE 8/12] Processing ECB-only pool: RMBMNL000125100420117 (208 MB compressed)\n",
      "[2025-12-10 21:09:47]   Scanning 11 files for column schema...\n",
      "[2025-12-10 21:10:40]   Found 162 columns\n",
      "[2025-12-10 21:10:40]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 21:10:40]   Found 162 columns\n",
      "[2025-12-10 21:10:40]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 21:12:03]   Saved 2,065,361 rows in 135.7s (chunked)\n",
      "[2025-12-10 21:12:03] \n",
      "[LARGE 9/12] Processing ECB-only pool: RMBMUK000113100120082 (1708 MB compressed)\n",
      "[2025-12-10 21:12:03]   Scanning 101 files for column schema...\n",
      "[2025-12-10 21:12:03]   Saved 2,065,361 rows in 135.7s (chunked)\n",
      "[2025-12-10 21:12:03] \n",
      "[LARGE 9/12] Processing ECB-only pool: RMBMUK000113100120082 (1708 MB compressed)\n",
      "[2025-12-10 21:12:03]   Scanning 101 files for column schema...\n",
      "[2025-12-10 21:18:44]   Found 162 columns\n",
      "[2025-12-10 21:18:44]   Processing 101 files in chunked mode...\n",
      "[2025-12-10 21:18:44]   Found 162 columns\n",
      "[2025-12-10 21:18:44]   Processing 101 files in chunked mode...\n",
      "[2025-12-10 21:22:13]     Processed 20/101 files, 5,455,083 rows so far\n",
      "[2025-12-10 21:22:13]     Processed 20/101 files, 5,455,083 rows so far\n",
      "[2025-12-10 21:24:46]     Processed 40/101 files, 9,424,636 rows so far\n",
      "[2025-12-10 21:24:46]     Processed 40/101 files, 9,424,636 rows so far\n",
      "[2025-12-10 21:26:31]     Processed 60/101 files, 12,240,484 rows so far\n",
      "[2025-12-10 21:26:31]     Processed 60/101 files, 12,240,484 rows so far\n",
      "[2025-12-10 21:27:54]     Processed 80/101 files, 14,462,746 rows so far\n",
      "[2025-12-10 21:27:54]     Processed 80/101 files, 14,462,746 rows so far\n",
      "[2025-12-10 21:29:38]     Processed 100/101 files, 17,241,312 rows so far\n",
      "[2025-12-10 21:29:38]     Processed 100/101 files, 17,241,312 rows so far\n",
      "[2025-12-10 21:29:43]   Saved 17,361,228 rows in 1059.9s (chunked)\n",
      "[2025-12-10 21:29:43] \n",
      "[LARGE 10/12] Processing ECB-only pool: RMBMNL000125100320077 (421 MB compressed)\n",
      "[2025-12-10 21:29:43]   Scanning 23 files for column schema...\n",
      "[2025-12-10 21:29:43]   Saved 17,361,228 rows in 1059.9s (chunked)\n",
      "[2025-12-10 21:29:43] \n",
      "[LARGE 10/12] Processing ECB-only pool: RMBMNL000125100320077 (421 MB compressed)\n",
      "[2025-12-10 21:29:43]   Scanning 23 files for column schema...\n",
      "[2025-12-10 21:31:30]   Found 162 columns\n",
      "[2025-12-10 21:31:30]   Processing 23 files in chunked mode...\n",
      "[2025-12-10 21:31:30]   Found 162 columns\n",
      "[2025-12-10 21:31:30]   Processing 23 files in chunked mode...\n",
      "[2025-12-10 21:34:03]     Processed 20/23 files, 4,004,439 rows so far\n",
      "[2025-12-10 21:34:03]     Processed 20/23 files, 4,004,439 rows so far\n",
      "[2025-12-10 21:34:23]   Saved 4,595,898 rows in 279.5s (chunked)\n",
      "[2025-12-10 21:34:23] \n",
      "[LARGE 11/12] Processing ECB-only pool: RMBSBE000044100520163 (214 MB compressed)\n",
      "[2025-12-10 21:34:23]   Scanning 94 files for column schema...\n",
      "[2025-12-10 21:34:23]   Saved 4,595,898 rows in 279.5s (chunked)\n",
      "[2025-12-10 21:34:23] \n",
      "[LARGE 11/12] Processing ECB-only pool: RMBSBE000044100520163 (214 MB compressed)\n",
      "[2025-12-10 21:34:23]   Scanning 94 files for column schema...\n",
      "[2025-12-10 21:34:52]   Loaded RMBSBE000044100520163_2021-02-28_Pool.csv.gz using zlib fallback (41,913 rows)\n",
      "[2025-12-10 21:34:52]   Loaded RMBSBE000044100520163_2021-02-28_Pool.csv.gz using zlib fallback (41,913 rows)\n",
      "[2025-12-10 21:35:09]   Found 60 columns\n",
      "[2025-12-10 21:35:09]   Processing 94 files in chunked mode...\n",
      "[2025-12-10 21:35:09]   Found 60 columns\n",
      "[2025-12-10 21:35:09]   Processing 94 files in chunked mode...\n",
      "[2025-12-10 21:35:25]     Processed 20/94 files, 838,431 rows so far\n",
      "[2025-12-10 21:35:25]     Processed 20/94 files, 838,431 rows so far\n",
      "[2025-12-10 21:35:41]     Processed 40/94 files, 1,676,178 rows so far\n",
      "[2025-12-10 21:35:41]     Processed 40/94 files, 1,676,178 rows so far\n",
      "[2025-12-10 21:35:57]   Loaded RMBSBE000044100520163_2021-02-28_Pool.csv.gz using zlib fallback (41,913 rows)\n",
      "[2025-12-10 21:35:57]   Loaded RMBSBE000044100520163_2021-02-28_Pool.csv.gz using zlib fallback (41,913 rows)\n",
      "[2025-12-10 21:35:57]     Processed 60/94 files, 2,514,437 rows so far\n",
      "[2025-12-10 21:35:57]     Processed 60/94 files, 2,514,437 rows so far\n",
      "[2025-12-10 21:36:13]     Processed 80/94 files, 3,352,696 rows so far\n",
      "[2025-12-10 21:36:13]     Processed 80/94 files, 3,352,696 rows so far\n",
      "[2025-12-10 21:36:24]   Saved 3,915,083 rows in 121.1s (chunked)\n",
      "[2025-12-10 21:36:24] \n",
      "[LARGE 12/12] Processing ECB-only pool: RMBMNL000125100520080 (1505 MB compressed)\n",
      "[2025-12-10 21:36:24]   Scanning 39 files for column schema...\n",
      "[2025-12-10 21:36:24]   Saved 3,915,083 rows in 121.1s (chunked)\n",
      "[2025-12-10 21:36:24] \n",
      "[LARGE 12/12] Processing ECB-only pool: RMBMNL000125100520080 (1505 MB compressed)\n",
      "[2025-12-10 21:36:24]   Scanning 39 files for column schema...\n",
      "[2025-12-10 21:43:09]   Found 163 columns\n",
      "[2025-12-10 21:43:09]   Processing 39 files in chunked mode...\n",
      "[2025-12-10 21:43:09]   Found 163 columns\n",
      "[2025-12-10 21:43:09]   Processing 39 files in chunked mode...\n",
      "[2025-12-10 21:48:32]     Processed 20/39 files, 8,072,532 rows so far\n",
      "[2025-12-10 21:48:32]     Processed 20/39 files, 8,072,532 rows so far\n",
      "[2025-12-10 21:54:19]   Saved 17,194,000 rows in 1075.4s (chunked)\n",
      "[2025-12-10 21:54:19] \\nECB-only pools complete: 5691.2s\n",
      "[2025-12-10 21:54:19] Session rows processed: 102,017,341\n",
      "[2025-12-10 21:54:19]   Saved 17,194,000 rows in 1075.4s (chunked)\n",
      "[2025-12-10 21:54:19] \\nECB-only pools complete: 5691.2s\n",
      "[2025-12-10 21:54:19] Session rows processed: 102,017,341\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 9: Main Processing Loop - ECB-ONLY POOLS (with large pool handling)\n",
    "# =============================================================================\n",
    "log_message(\"=\"*70)\n",
    "log_message(\"STARTING ECB-ONLY POOLS PROCESSING\")\n",
    "log_message(\"=\"*70)\n",
    "\n",
    "# Scan output folders to find already-completed pools (file-based tracking)\n",
    "completed_pools = scan_completed_pools()\n",
    "completed_ecb_only = completed_pools['ecb_only']\n",
    "\n",
    "# Track total rows for this session\n",
    "session_rows_processed = 0\n",
    "\n",
    "# Get pools to process (skip already completed)\n",
    "ecb_only_to_process = [p for p in ecb_only_pools if p not in completed_ecb_only]\n",
    "\n",
    "log_message(f\"ECB-only pools: {len(ecb_only_pools)} total, {len(ecb_only_to_process)} remaining\")\n",
    "\n",
    "# Identify large pools\n",
    "large_pools = [p for p in ecb_only_to_process if is_large_pool(p, 'ecb')]\n",
    "normal_pools = [p for p in ecb_only_to_process if not is_large_pool(p, 'ecb')]\n",
    "log_message(f\"  Normal pools: {len(normal_pools)}, Large pools (chunked): {len(large_pools)}\")\n",
    "\n",
    "ecb_only_start = time.time()\n",
    "\n",
    "# Process normal pools first\n",
    "for i, ecb_pool_id in enumerate(normal_pools):\n",
    "    pool_start = time.time()\n",
    "    log_message(f\"\\n[{i+1}/{len(normal_pools)}] Processing ECB-only pool: {ecb_pool_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Process pool\n",
    "        result_df = process_ecb_only_pool(ecb_pool_id)\n",
    "        \n",
    "        # Save result\n",
    "        rows_saved = save_pool_result(result_df, \"ecb_only\", ecb_pool_id)\n",
    "        \n",
    "        # Free memory\n",
    "        del result_df\n",
    "        gc.collect()\n",
    "        \n",
    "        # Track session progress (file-based tracking means completion is automatic)\n",
    "        session_rows_processed += rows_saved\n",
    "        \n",
    "        pool_elapsed = time.time() - pool_start\n",
    "        log_message(f\"  Saved {rows_saved:,} rows in {pool_elapsed:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR processing {ecb_pool_id}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        gc.collect()\n",
    "\n",
    "# Process large pools with chunked mode\n",
    "for i, ecb_pool_id in enumerate(large_pools):\n",
    "    pool_start = time.time()\n",
    "    pool_size_mb = get_pool_compressed_size(ecb_pool_id, 'ecb') / 1024 / 1024\n",
    "    log_message(f\"\\n[LARGE {i+1}/{len(large_pools)}] Processing ECB-only pool: {ecb_pool_id} ({pool_size_mb:.0f} MB compressed)\")\n",
    "    \n",
    "    try:\n",
    "        # Setup output path with ATOMIC write pattern\n",
    "        subdir = os.path.join(OUTPUT_DIR, \"ecb_only\")\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "        safe_pool_id = ecb_pool_id.replace('/', '_').replace('\\\\', '_')\n",
    "        output_path = os.path.join(subdir, f\"{safe_pool_id}.csv\")\n",
    "        temp_path = output_path + \".tmp\"\n",
    "        \n",
    "        # Remove existing files if any (fresh start)\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        \n",
    "        # Process with chunked mode - writes to TEMP file\n",
    "        rows_saved = process_ecb_only_pool_chunked(ecb_pool_id, temp_path)\n",
    "        \n",
    "        # ATOMIC: Rename temp to final ONLY after complete success\n",
    "        os.replace(temp_path, output_path)\n",
    "        \n",
    "        # Track session progress (file-based tracking means completion is automatic)\n",
    "        session_rows_processed += rows_saved\n",
    "        \n",
    "        pool_elapsed = time.time() - pool_start\n",
    "        log_message(f\"  Saved {rows_saved:,} rows in {pool_elapsed:.1f}s (chunked)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR processing {ecb_pool_id}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        # Clean up temp file if it exists\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        gc.collect()\n",
    "\n",
    "ecb_only_elapsed = time.time() - ecb_only_start\n",
    "log_message(f\"\\\\nECB-only pools complete: {ecb_only_elapsed:.1f}s\")\n",
    "log_message(f\"Session rows processed: {session_rows_processed:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "747b66d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 21:54:19] ======================================================================\n",
      "[2025-12-10 21:54:19] STARTING ESMA-ONLY POOLS PROCESSING\n",
      "[2025-12-10 21:54:19] ======================================================================\n",
      "[2025-12-10 21:54:19] ESMA-only pools: 246 total, 246 remaining\n",
      "[2025-12-10 21:54:19]   Normal pools: 180, Large pools (chunked): 66\n",
      "[2025-12-10 21:54:19] \n",
      "[1/180] Processing ESMA-only pool: 549300ELBHWFMLA2R125N200601\n",
      "[2025-12-10 21:54:19]   Normal pools: 180, Large pools (chunked): 66\n",
      "[2025-12-10 21:54:19] \n",
      "[1/180] Processing ESMA-only pool: 549300ELBHWFMLA2R125N200601\n",
      "[2025-12-10 21:54:21]   ESMA rows: 112245\n",
      "[2025-12-10 21:54:21]   ESMA rows: 112245\n",
      "[2025-12-10 21:54:23]   Saved 112,245 rows in 3.4s\n",
      "[2025-12-10 21:54:23] \n",
      "[2/180] Processing ESMA-only pool: 95980020140005209659N201001\n",
      "[2025-12-10 21:54:23]   Saved 112,245 rows in 3.4s\n",
      "[2025-12-10 21:54:23] \n",
      "[2/180] Processing ESMA-only pool: 95980020140005209659N201001\n",
      "[2025-12-10 21:54:23]   ESMA rows: 67517\n",
      "[2025-12-10 21:54:23]   ESMA rows: 67517\n",
      "[2025-12-10 21:54:25]   Saved 67,517 rows in 1.8s\n",
      "[2025-12-10 21:54:25] \n",
      "[3/180] Processing ESMA-only pool: 635400AJEHTFGD5HYS44N202101\n",
      "[2025-12-10 21:54:25]   Saved 67,517 rows in 1.8s\n",
      "[2025-12-10 21:54:25] \n",
      "[3/180] Processing ESMA-only pool: 635400AJEHTFGD5HYS44N202101\n",
      "[2025-12-10 21:54:26]   ESMA rows: 78934\n",
      "[2025-12-10 21:54:26]   ESMA rows: 78934\n",
      "[2025-12-10 21:54:28]   Saved 78,934 rows in 3.1s\n",
      "[2025-12-10 21:54:28] \n",
      "[4/180] Processing ESMA-only pool: 549300F2EUS6QS7H4D81N201902\n",
      "[2025-12-10 21:54:28]   Saved 78,934 rows in 3.1s\n",
      "[2025-12-10 21:54:28] \n",
      "[4/180] Processing ESMA-only pool: 549300F2EUS6QS7H4D81N201902\n",
      "[2025-12-10 21:54:28]   ESMA rows: 32805\n",
      "[2025-12-10 21:54:28]   ESMA rows: 32805\n",
      "[2025-12-10 21:54:29]   Saved 32,805 rows in 1.4s\n",
      "[2025-12-10 21:54:29] \n",
      "[5/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200702\n",
      "[2025-12-10 21:54:29]   Saved 32,805 rows in 1.4s\n",
      "[2025-12-10 21:54:29] \n",
      "[5/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200702\n",
      "[2025-12-10 21:54:30]   ESMA rows: 42240\n",
      "[2025-12-10 21:54:30]   ESMA rows: 42240\n",
      "[2025-12-10 21:54:30]   Saved 42,240 rows in 1.3s\n",
      "[2025-12-10 21:54:30] \n",
      "[6/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200504\n",
      "[2025-12-10 21:54:30]   Saved 42,240 rows in 1.3s\n",
      "[2025-12-10 21:54:30] \n",
      "[6/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200504\n",
      "[2025-12-10 21:54:31]   ESMA rows: 19365\n",
      "[2025-12-10 21:54:31]   ESMA rows: 19365\n",
      "[2025-12-10 21:54:31]   Saved 19,365 rows in 0.7s\n",
      "[2025-12-10 21:54:31] \n",
      "[7/180] Processing ESMA-only pool: 959800521QYXV2UA6L30N202001\n",
      "[2025-12-10 21:54:31]   Saved 19,365 rows in 0.7s\n",
      "[2025-12-10 21:54:31] \n",
      "[7/180] Processing ESMA-only pool: 959800521QYXV2UA6L30N202001\n",
      "[2025-12-10 21:54:32]   ESMA rows: 88994\n",
      "[2025-12-10 21:54:32]   ESMA rows: 88994\n",
      "[2025-12-10 21:54:34]   Saved 88,994 rows in 3.2s\n",
      "[2025-12-10 21:54:34] \n",
      "[8/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202102\n",
      "[2025-12-10 21:54:34]   Saved 88,994 rows in 3.2s\n",
      "[2025-12-10 21:54:34] \n",
      "[8/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202102\n",
      "[2025-12-10 21:54:35]   ESMA rows: 41820\n",
      "[2025-12-10 21:54:35]   ESMA rows: 41820\n",
      "[2025-12-10 21:54:36]   Saved 41,820 rows in 1.5s\n",
      "[2025-12-10 21:54:36] \n",
      "[9/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200801\n",
      "[2025-12-10 21:54:36]   Saved 41,820 rows in 1.5s\n",
      "[2025-12-10 21:54:36] \n",
      "[9/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200801\n",
      "[2025-12-10 21:54:36]   ESMA rows: 36630\n",
      "[2025-12-10 21:54:36]   ESMA rows: 36630\n",
      "[2025-12-10 21:54:37]   Saved 36,630 rows in 1.2s\n",
      "[2025-12-10 21:54:37] \n",
      "[10/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200602\n",
      "[2025-12-10 21:54:37]   Saved 36,630 rows in 1.2s\n",
      "[2025-12-10 21:54:37] \n",
      "[10/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200602\n",
      "[2025-12-10 21:54:37]   ESMA rows: 26954\n",
      "[2025-12-10 21:54:37]   ESMA rows: 26954\n",
      "[2025-12-10 21:54:38]   Saved 26,954 rows in 1.0s\n",
      "[2025-12-10 21:54:38] \n",
      "[11/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202301\n",
      "[2025-12-10 21:54:38]   Saved 26,954 rows in 1.0s\n",
      "[2025-12-10 21:54:38] \n",
      "[11/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202301\n",
      "[2025-12-10 21:54:38]   ESMA rows: 27470\n",
      "[2025-12-10 21:54:38]   ESMA rows: 27470\n",
      "[2025-12-10 21:54:39]   Saved 27,470 rows in 1.1s\n",
      "[2025-12-10 21:54:39] \n",
      "[12/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200701\n",
      "[2025-12-10 21:54:39]   Saved 27,470 rows in 1.1s\n",
      "[2025-12-10 21:54:39] \n",
      "[12/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200701\n",
      "[2025-12-10 21:54:40]   ESMA rows: 41800\n",
      "[2025-12-10 21:54:40]   ESMA rows: 41800\n",
      "[2025-12-10 21:54:41]   Saved 41,800 rows in 1.4s\n",
      "[2025-12-10 21:54:41] \n",
      "[13/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200602\n",
      "[2025-12-10 21:54:41]   Saved 41,800 rows in 1.4s\n",
      "[2025-12-10 21:54:41] \n",
      "[13/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200602\n",
      "[2025-12-10 21:54:41]   ESMA rows: 28425\n",
      "[2025-12-10 21:54:41]   ESMA rows: 28425\n",
      "[2025-12-10 21:54:41]   Saved 28,425 rows in 0.9s\n",
      "[2025-12-10 21:54:41] \n",
      "[14/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N202301\n",
      "[2025-12-10 21:54:41]   Saved 28,425 rows in 0.9s\n",
      "[2025-12-10 21:54:41] \n",
      "[14/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N202301\n",
      "[2025-12-10 21:54:42]   ESMA rows: 66083\n",
      "[2025-12-10 21:54:42]   ESMA rows: 66083\n",
      "[2025-12-10 21:54:44]   Saved 66,083 rows in 2.6s\n",
      "[2025-12-10 21:54:44] \n",
      "[15/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N200602\n",
      "[2025-12-10 21:54:44]   Saved 66,083 rows in 2.6s\n",
      "[2025-12-10 21:54:44] \n",
      "[15/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N200602\n",
      "[2025-12-10 21:54:44]   ESMA rows: 17097\n",
      "[2025-12-10 21:54:44]   ESMA rows: 17097\n",
      "[2025-12-10 21:54:45]   Saved 17,097 rows in 0.7s\n",
      "[2025-12-10 21:54:45] \n",
      "[16/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202302\n",
      "[2025-12-10 21:54:45]   Saved 17,097 rows in 0.7s\n",
      "[2025-12-10 21:54:45] \n",
      "[16/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202302\n",
      "[2025-12-10 21:54:45]   ESMA rows: 15513\n",
      "[2025-12-10 21:54:45]   ESMA rows: 15513\n",
      "[2025-12-10 21:54:45]   Saved 15,513 rows in 0.7s\n",
      "[2025-12-10 21:54:45] \n",
      "[17/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N201701\n",
      "[2025-12-10 21:54:45]   Saved 15,513 rows in 0.7s\n",
      "[2025-12-10 21:54:45] \n",
      "[17/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N201701\n",
      "[2025-12-10 21:54:46]   ESMA rows: 100535\n",
      "[2025-12-10 21:54:46]   ESMA rows: 100535\n",
      "[2025-12-10 21:54:49]   Saved 100,535 rows in 3.0s\n",
      "[2025-12-10 21:54:49] \n",
      "[18/180] Processing ESMA-only pool: A6NZLYKYN1UV7VVGFX65N202101\n",
      "[2025-12-10 21:54:49]   Saved 100,535 rows in 3.0s\n",
      "[2025-12-10 21:54:49] \n",
      "[18/180] Processing ESMA-only pool: A6NZLYKYN1UV7VVGFX65N202101\n",
      "[2025-12-10 21:54:50]   ESMA rows: 98252\n",
      "[2025-12-10 21:54:50]   ESMA rows: 98252\n",
      "[2025-12-10 21:54:52]   Saved 98,252 rows in 3.9s\n",
      "[2025-12-10 21:54:52] \n",
      "[19/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N201801\n",
      "[2025-12-10 21:54:52]   Saved 98,252 rows in 3.9s\n",
      "[2025-12-10 21:54:52] \n",
      "[19/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N201801\n",
      "[2025-12-10 21:54:53]   ESMA rows: 74348\n",
      "[2025-12-10 21:54:53]   ESMA rows: 74348\n",
      "[2025-12-10 21:54:55]   Saved 74,348 rows in 2.8s\n",
      "[2025-12-10 21:54:55] \n",
      "[20/180] Processing ESMA-only pool: 6354001ECZ5C5ROCKB97N201901\n",
      "[2025-12-10 21:54:55]   Saved 74,348 rows in 2.8s\n",
      "[2025-12-10 21:54:55] \n",
      "[20/180] Processing ESMA-only pool: 6354001ECZ5C5ROCKB97N201901\n",
      "[2025-12-10 21:54:55]   ESMA rows: 13816\n",
      "[2025-12-10 21:54:55]   ESMA rows: 13816\n",
      "[2025-12-10 21:54:56]   Saved 13,816 rows in 0.7s\n",
      "[2025-12-10 21:54:56] \n",
      "[21/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200702\n",
      "[2025-12-10 21:54:56]   Saved 13,816 rows in 0.7s\n",
      "[2025-12-10 21:54:56] \n",
      "[21/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200702\n",
      "[2025-12-10 21:54:56]   ESMA rows: 27462\n",
      "[2025-12-10 21:54:56]   ESMA rows: 27462\n",
      "[2025-12-10 21:54:57]   Saved 27,462 rows in 1.1s\n",
      "[2025-12-10 21:54:57] \n",
      "[22/180] Processing ESMA-only pool: 2138004FIUXU3B2MR537N200801\n",
      "[2025-12-10 21:54:57]   ESMA rows: 9804\n",
      "[2025-12-10 21:54:57]   Saved 27,462 rows in 1.1s\n",
      "[2025-12-10 21:54:57] \n",
      "[22/180] Processing ESMA-only pool: 2138004FIUXU3B2MR537N200801\n",
      "[2025-12-10 21:54:57]   ESMA rows: 9804\n",
      "[2025-12-10 21:54:57]   Saved 9,804 rows in 0.4s\n",
      "[2025-12-10 21:54:57] \n",
      "[23/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200506\n",
      "[2025-12-10 21:54:57]   Saved 9,804 rows in 0.4s\n",
      "[2025-12-10 21:54:57] \n",
      "[23/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200506\n",
      "[2025-12-10 21:54:58]   ESMA rows: 16193\n",
      "[2025-12-10 21:54:58]   ESMA rows: 16193\n",
      "[2025-12-10 21:54:58]   Saved 16,193 rows in 0.7s\n",
      "[2025-12-10 21:54:58] \n",
      "[24/180] Processing ESMA-only pool: 549300F0CVELHBU9A156N202301\n",
      "[2025-12-10 21:54:58]   Saved 16,193 rows in 0.7s\n",
      "[2025-12-10 21:54:58] \n",
      "[24/180] Processing ESMA-only pool: 549300F0CVELHBU9A156N202301\n",
      "[2025-12-10 21:54:59]   ESMA rows: 106879\n",
      "[2025-12-10 21:54:59]   ESMA rows: 106879\n",
      "[2025-12-10 21:55:02]   Saved 106,879 rows in 3.6s\n",
      "[2025-12-10 21:55:02] \n",
      "[25/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N200801\n",
      "[2025-12-10 21:55:02]   Saved 106,879 rows in 3.6s\n",
      "[2025-12-10 21:55:02] \n",
      "[25/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N200801\n",
      "[2025-12-10 21:55:02]   ESMA rows: 27692\n",
      "[2025-12-10 21:55:02]   ESMA rows: 27692\n",
      "[2025-12-10 21:55:03]   Saved 27,692 rows in 1.0s\n",
      "[2025-12-10 21:55:03] \n",
      "[26/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N201901\n",
      "[2025-12-10 21:55:03]   Saved 27,692 rows in 1.0s\n",
      "[2025-12-10 21:55:03] \n",
      "[26/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N201901\n",
      "[2025-12-10 21:55:03]   ESMA rows: 69043\n",
      "[2025-12-10 21:55:03]   ESMA rows: 69043\n",
      "[2025-12-10 21:55:05]   Saved 69,043 rows in 2.3s\n",
      "[2025-12-10 21:55:05] \n",
      "[27/180] Processing ESMA-only pool: 815600722FAEE18D6013N201901\n",
      "[2025-12-10 21:55:05]   Saved 69,043 rows in 2.3s\n",
      "[2025-12-10 21:55:05] \n",
      "[27/180] Processing ESMA-only pool: 815600722FAEE18D6013N201901\n",
      "[2025-12-10 21:55:05]   ESMA rows: 23066\n",
      "[2025-12-10 21:55:05]   ESMA rows: 23066\n",
      "[2025-12-10 21:55:06]   Saved 23,066 rows in 1.0s\n",
      "[2025-12-10 21:55:06] \n",
      "[28/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202402\n",
      "[2025-12-10 21:55:06]   ESMA rows: 6124\n",
      "[2025-12-10 21:55:06]   Saved 23,066 rows in 1.0s\n",
      "[2025-12-10 21:55:06] \n",
      "[28/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202402\n",
      "[2025-12-10 21:55:06]   ESMA rows: 6124\n",
      "[2025-12-10 21:55:06]   Saved 6,124 rows in 0.3s\n",
      "[2025-12-10 21:55:06] \n",
      "[29/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200801\n",
      "[2025-12-10 21:55:06]   Saved 6,124 rows in 0.3s\n",
      "[2025-12-10 21:55:06] \n",
      "[29/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200801\n",
      "[2025-12-10 21:55:07]   ESMA rows: 56480\n",
      "[2025-12-10 21:55:07]   ESMA rows: 56480\n",
      "[2025-12-10 21:55:08]   Saved 56,480 rows in 1.8s\n",
      "[2025-12-10 21:55:08] \n",
      "[30/180] Processing ESMA-only pool: 724500ISK5V0VFQS1654N202501\n",
      "[2025-12-10 21:55:08]   ESMA rows: 4760\n",
      "[2025-12-10 21:55:08]   Saved 56,480 rows in 1.8s\n",
      "[2025-12-10 21:55:08] \n",
      "[30/180] Processing ESMA-only pool: 724500ISK5V0VFQS1654N202501\n",
      "[2025-12-10 21:55:08]   ESMA rows: 4760\n",
      "[2025-12-10 21:55:08]   Saved 4,760 rows in 0.2s\n",
      "[2025-12-10 21:55:08] \n",
      "[31/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202002\n",
      "[2025-12-10 21:55:08]   Saved 4,760 rows in 0.2s\n",
      "[2025-12-10 21:55:08] \n",
      "[31/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202002\n",
      "[2025-12-10 21:55:09]   ESMA rows: 36160\n",
      "[2025-12-10 21:55:09]   ESMA rows: 36160\n",
      "[2025-12-10 21:55:10]   Saved 36,160 rows in 1.4s\n",
      "[2025-12-10 21:55:10] \n",
      "[32/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200701\n",
      "[2025-12-10 21:55:10]   Saved 36,160 rows in 1.4s\n",
      "[2025-12-10 21:55:10] \n",
      "[32/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200701\n",
      "[2025-12-10 21:55:10]   ESMA rows: 39240\n",
      "[2025-12-10 21:55:10]   ESMA rows: 39240\n",
      "[2025-12-10 21:55:11]   Saved 39,240 rows in 1.3s\n",
      "[2025-12-10 21:55:11] \n",
      "[33/180] Processing ESMA-only pool: 95980020140005209368N202101\n",
      "[2025-12-10 21:55:11]   Saved 39,240 rows in 1.3s\n",
      "[2025-12-10 21:55:11] \n",
      "[33/180] Processing ESMA-only pool: 95980020140005209368N202101\n",
      "[2025-12-10 21:55:12]   ESMA rows: 84044\n",
      "[2025-12-10 21:55:12]   ESMA rows: 84044\n",
      "[2025-12-10 21:55:14]   Saved 84,044 rows in 3.5s\n",
      "[2025-12-10 21:55:14] \n",
      "[34/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200709\n",
      "[2025-12-10 21:55:14]   Saved 84,044 rows in 3.5s\n",
      "[2025-12-10 21:55:14] \n",
      "[34/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200709\n",
      "[2025-12-10 21:55:15]   ESMA rows: 23909\n",
      "[2025-12-10 21:55:15]   ESMA rows: 23909\n",
      "[2025-12-10 21:55:15]   Saved 23,909 rows in 0.9s\n",
      "[2025-12-10 21:55:15] \n",
      "[35/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N201901\n",
      "[2025-12-10 21:55:15]   ESMA rows: 3457\n",
      "[2025-12-10 21:55:15]   Saved 3,457 rows in 0.2s\n",
      "[2025-12-10 21:55:15] \n",
      "[36/180] Processing ESMA-only pool: 724500XOGMV0BMF6EK71N202001\n",
      "[2025-12-10 21:55:15]   Saved 23,909 rows in 0.9s\n",
      "[2025-12-10 21:55:15] \n",
      "[35/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N201901\n",
      "[2025-12-10 21:55:15]   ESMA rows: 3457\n",
      "[2025-12-10 21:55:15]   Saved 3,457 rows in 0.2s\n",
      "[2025-12-10 21:55:15] \n",
      "[36/180] Processing ESMA-only pool: 724500XOGMV0BMF6EK71N202001\n",
      "[2025-12-10 21:55:16]   ESMA rows: 9755\n",
      "[2025-12-10 21:55:16]   ESMA rows: 9755\n",
      "[2025-12-10 21:55:16]   Saved 9,755 rows in 0.6s\n",
      "[2025-12-10 21:55:16] \n",
      "[37/180] Processing ESMA-only pool: 959800LQ598A5RQASA61N200814\n",
      "[2025-12-10 21:55:16]   ESMA rows: 9213\n",
      "[2025-12-10 21:55:16]   Saved 9,755 rows in 0.6s\n",
      "[2025-12-10 21:55:16] \n",
      "[37/180] Processing ESMA-only pool: 959800LQ598A5RQASA61N200814\n",
      "[2025-12-10 21:55:16]   ESMA rows: 9213\n",
      "[2025-12-10 21:55:16]   Saved 9,213 rows in 0.4s\n",
      "[2025-12-10 21:55:16] \n",
      "[38/180] Processing ESMA-only pool: 724500T5HC7NI06PEW24N202401\n",
      "[2025-12-10 21:55:17]   ESMA rows: 12176\n",
      "[2025-12-10 21:55:16]   Saved 9,213 rows in 0.4s\n",
      "[2025-12-10 21:55:16] \n",
      "[38/180] Processing ESMA-only pool: 724500T5HC7NI06PEW24N202401\n",
      "[2025-12-10 21:55:17]   ESMA rows: 12176\n",
      "[2025-12-10 21:55:17]   Saved 12,176 rows in 0.6s\n",
      "[2025-12-10 21:55:17] \n",
      "[39/180] Processing ESMA-only pool: 95980094CL01DWRL6C36N201301\n",
      "[2025-12-10 21:55:17]   Saved 12,176 rows in 0.6s\n",
      "[2025-12-10 21:55:17] \n",
      "[39/180] Processing ESMA-only pool: 95980094CL01DWRL6C36N201301\n",
      "[2025-12-10 21:55:17]   ESMA rows: 20416\n",
      "[2025-12-10 21:55:17]   ESMA rows: 20416\n",
      "[2025-12-10 21:55:18]   Saved 20,416 rows in 0.8s\n",
      "[2025-12-10 21:55:18] \n",
      "[40/180] Processing ESMA-only pool: 54930032M1SQPJNRZ420N201901\n",
      "[2025-12-10 21:55:18]   Saved 20,416 rows in 0.8s\n",
      "[2025-12-10 21:55:18] \n",
      "[40/180] Processing ESMA-only pool: 54930032M1SQPJNRZ420N201901\n",
      "[2025-12-10 21:55:19]   ESMA rows: 117013\n",
      "[2025-12-10 21:55:19]   ESMA rows: 117013\n",
      "[2025-12-10 21:55:22]   Saved 117,013 rows in 4.4s\n",
      "[2025-12-10 21:55:22] \n",
      "[41/180] Processing ESMA-only pool: 95980020140005209853N201101\n",
      "[2025-12-10 21:55:22]   Saved 117,013 rows in 4.4s\n",
      "[2025-12-10 21:55:22] \n",
      "[41/180] Processing ESMA-only pool: 95980020140005209853N201101\n",
      "[2025-12-10 21:55:22]   ESMA rows: 20586\n",
      "[2025-12-10 21:55:22]   ESMA rows: 20586\n",
      "[2025-12-10 21:55:23]   Saved 20,586 rows in 0.6s\n",
      "[2025-12-10 21:55:23] \n",
      "[42/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N202001\n",
      "[2025-12-10 21:55:23]   Saved 20,586 rows in 0.6s\n",
      "[2025-12-10 21:55:23] \n",
      "[42/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N202001\n",
      "[2025-12-10 21:55:23]   ESMA rows: 11304\n",
      "[2025-12-10 21:55:23]   ESMA rows: 11304\n",
      "[2025-12-10 21:55:23]   Saved 11,304 rows in 0.5s\n",
      "[2025-12-10 21:55:23] \n",
      "[43/180] Processing ESMA-only pool: 95980020140005212763N200801\n",
      "[2025-12-10 21:55:24]   ESMA rows: 10027\n",
      "[2025-12-10 21:55:23]   Saved 11,304 rows in 0.5s\n",
      "[2025-12-10 21:55:23] \n",
      "[43/180] Processing ESMA-only pool: 95980020140005212763N200801\n",
      "[2025-12-10 21:55:24]   ESMA rows: 10027\n",
      "[2025-12-10 21:55:24]   Saved 10,027 rows in 0.4s\n",
      "[2025-12-10 21:55:24] \n",
      "[44/180] Processing ESMA-only pool: 969500TJWRCC8CMQWT96N202401\n",
      "[2025-12-10 21:55:24]   Saved 10,027 rows in 0.4s\n",
      "[2025-12-10 21:55:24] \n",
      "[44/180] Processing ESMA-only pool: 969500TJWRCC8CMQWT96N202401\n",
      "[2025-12-10 21:55:24]   ESMA rows: 61674\n",
      "[2025-12-10 21:55:24]   ESMA rows: 61674\n",
      "[2025-12-10 21:55:26]   Saved 61,674 rows in 2.2s\n",
      "[2025-12-10 21:55:26] \n",
      "[45/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202501\n",
      "[2025-12-10 21:55:26]   ESMA rows: 12419\n",
      "[2025-12-10 21:55:26]   Saved 61,674 rows in 2.2s\n",
      "[2025-12-10 21:55:26] \n",
      "[45/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202501\n",
      "[2025-12-10 21:55:26]   ESMA rows: 12419\n",
      "[2025-12-10 21:55:26]   Saved 12,419 rows in 0.5s\n",
      "[2025-12-10 21:55:26] \n",
      "[46/180] Processing ESMA-only pool: 724500O4GUVTGSZEU248N202301\n",
      "[2025-12-10 21:55:26]   Saved 12,419 rows in 0.5s\n",
      "[2025-12-10 21:55:26] \n",
      "[46/180] Processing ESMA-only pool: 724500O4GUVTGSZEU248N202301\n",
      "[2025-12-10 21:55:27]   ESMA rows: 65150\n",
      "[2025-12-10 21:55:27]   ESMA rows: 65150\n",
      "[2025-12-10 21:55:29]   Saved 65,150 rows in 2.4s\n",
      "[2025-12-10 21:55:29] \n",
      "[47/180] Processing ESMA-only pool: 54930056IRBXK0Q1FP96N201601\n",
      "[2025-12-10 21:55:29]   ESMA rows: 12398\n",
      "[2025-12-10 21:55:29]   Saved 65,150 rows in 2.4s\n",
      "[2025-12-10 21:55:29] \n",
      "[47/180] Processing ESMA-only pool: 54930056IRBXK0Q1FP96N201601\n",
      "[2025-12-10 21:55:29]   ESMA rows: 12398\n",
      "[2025-12-10 21:55:29]   Saved 12,398 rows in 0.5s\n",
      "[2025-12-10 21:55:29] \n",
      "[48/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200703\n",
      "[2025-12-10 21:55:29]   Saved 12,398 rows in 0.5s\n",
      "[2025-12-10 21:55:29] \n",
      "[48/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200703\n",
      "[2025-12-10 21:55:30]   ESMA rows: 53647\n",
      "[2025-12-10 21:55:30]   ESMA rows: 53647\n",
      "[2025-12-10 21:55:31]   Saved 53,647 rows in 1.8s\n",
      "[2025-12-10 21:55:31] \n",
      "[49/180] Processing ESMA-only pool: 959800LQ598A5RQASA61N200713\n",
      "[2025-12-10 21:55:31]   ESMA rows: 9597\n",
      "[2025-12-10 21:55:31]   Saved 53,647 rows in 1.8s\n",
      "[2025-12-10 21:55:31] \n",
      "[49/180] Processing ESMA-only pool: 959800LQ598A5RQASA61N200713\n",
      "[2025-12-10 21:55:31]   ESMA rows: 9597\n",
      "[2025-12-10 21:55:32]   Saved 9,597 rows in 0.4s\n",
      "[2025-12-10 21:55:32] \n",
      "[50/180] Processing ESMA-only pool: 724500FJ7SUXFJB7NN36N200601\n",
      "[2025-12-10 21:55:32]   Saved 9,597 rows in 0.4s\n",
      "[2025-12-10 21:55:32] \n",
      "[50/180] Processing ESMA-only pool: 724500FJ7SUXFJB7NN36N200601\n",
      "[2025-12-10 21:55:32]   ESMA rows: 23260\n",
      "[2025-12-10 21:55:32]   ESMA rows: 23260\n",
      "[2025-12-10 21:55:33]   Saved 23,260 rows in 1.0s\n",
      "[2025-12-10 21:55:33] \n",
      "[51/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200701\n",
      "[2025-12-10 21:55:33]   Saved 23,260 rows in 1.0s\n",
      "[2025-12-10 21:55:33] \n",
      "[51/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200701\n",
      "[2025-12-10 21:55:33]   ESMA rows: 70064\n",
      "[2025-12-10 21:55:33]   ESMA rows: 70064\n",
      "[2025-12-10 21:55:35]   Saved 70,064 rows in 2.4s\n",
      "[2025-12-10 21:55:35] \n",
      "[52/180] Processing ESMA-only pool: 549300CV91UZJXB8RL09N202301\n",
      "[2025-12-10 21:55:35]   Saved 70,064 rows in 2.4s\n",
      "[2025-12-10 21:55:35] \n",
      "[52/180] Processing ESMA-only pool: 549300CV91UZJXB8RL09N202301\n",
      "[2025-12-10 21:55:36]   ESMA rows: 95861\n",
      "[2025-12-10 21:55:36]   ESMA rows: 95861\n",
      "[2025-12-10 21:55:38]   Saved 95,861 rows in 3.5s\n",
      "[2025-12-10 21:55:38] \n",
      "[53/180] Processing ESMA-only pool: 9695008RHUDZ68X7SV34N202501\n",
      "[2025-12-10 21:55:38]   Saved 95,861 rows in 3.5s\n",
      "[2025-12-10 21:55:38] \n",
      "[53/180] Processing ESMA-only pool: 9695008RHUDZ68X7SV34N202501\n",
      "[2025-12-10 21:55:39]   ESMA rows: 26684\n",
      "[2025-12-10 21:55:39]   ESMA rows: 26684\n",
      "[2025-12-10 21:55:39]   Saved 26,684 rows in 0.9s\n",
      "[2025-12-10 21:55:39] \n",
      "[54/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N201802\n",
      "[2025-12-10 21:55:39]   Saved 26,684 rows in 0.9s\n",
      "[2025-12-10 21:55:39] \n",
      "[54/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N201802\n",
      "[2025-12-10 21:55:40]   ESMA rows: 38179\n",
      "[2025-12-10 21:55:40]   ESMA rows: 38179\n",
      "[2025-12-10 21:55:41]   Saved 38,179 rows in 1.5s\n",
      "[2025-12-10 21:55:41] \n",
      "[55/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N201901\n",
      "[2025-12-10 21:55:41]   ESMA rows: 5456\n",
      "[2025-12-10 21:55:41]   Saved 38,179 rows in 1.5s\n",
      "[2025-12-10 21:55:41] \n",
      "[55/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N201901\n",
      "[2025-12-10 21:55:41]   ESMA rows: 5456\n",
      "[2025-12-10 21:55:41]   Saved 5,456 rows in 0.3s\n",
      "[2025-12-10 21:55:41] \n",
      "[56/180] Processing ESMA-only pool: 95980020140005216158N200701\n",
      "[2025-12-10 21:55:41]   Saved 5,456 rows in 0.3s\n",
      "[2025-12-10 21:55:41] \n",
      "[56/180] Processing ESMA-only pool: 95980020140005216158N200701\n",
      "[2025-12-10 21:55:41]   ESMA rows: 16323\n",
      "[2025-12-10 21:55:41]   ESMA rows: 16323\n",
      "[2025-12-10 21:55:42]   Saved 16,323 rows in 0.6s\n",
      "[2025-12-10 21:55:42] \n",
      "[57/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202001\n",
      "[2025-12-10 21:55:42]   Saved 16,323 rows in 0.6s\n",
      "[2025-12-10 21:55:42] \n",
      "[57/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202001\n",
      "[2025-12-10 21:55:42]   ESMA rows: 55066\n",
      "[2025-12-10 21:55:42]   ESMA rows: 55066\n",
      "[2025-12-10 21:55:44]   Saved 55,066 rows in 1.9s\n",
      "[2025-12-10 21:55:44] \n",
      "[58/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N201603\n",
      "[2025-12-10 21:55:44]   Saved 55,066 rows in 1.9s\n",
      "[2025-12-10 21:55:44] \n",
      "[58/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N201603\n",
      "[2025-12-10 21:55:44]   ESMA rows: 23992\n",
      "[2025-12-10 21:55:44]   ESMA rows: 23992\n",
      "[2025-12-10 21:55:45]   Saved 23,992 rows in 1.0s\n",
      "[2025-12-10 21:55:45] \n",
      "[59/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200602\n",
      "[2025-12-10 21:55:45]   Saved 23,992 rows in 1.0s\n",
      "[2025-12-10 21:55:45] \n",
      "[59/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200602\n",
      "[2025-12-10 21:55:45]   ESMA rows: 37020\n",
      "[2025-12-10 21:55:45]   ESMA rows: 37020\n",
      "[2025-12-10 21:55:46]   Saved 37,020 rows in 1.2s\n",
      "[2025-12-10 21:55:46] \n",
      "[60/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200601\n",
      "[2025-12-10 21:55:46]   Saved 37,020 rows in 1.2s\n",
      "[2025-12-10 21:55:46] \n",
      "[60/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200601\n",
      "[2025-12-10 21:55:46]   ESMA rows: 44064\n",
      "[2025-12-10 21:55:46]   ESMA rows: 44064\n",
      "[2025-12-10 21:55:47]   Saved 44,064 rows in 1.6s\n",
      "[2025-12-10 21:55:47] \n",
      "[61/180] Processing ESMA-only pool: 95980020140005310927N202201\n",
      "[2025-12-10 21:55:47]   Saved 44,064 rows in 1.6s\n",
      "[2025-12-10 21:55:47] \n",
      "[61/180] Processing ESMA-only pool: 95980020140005310927N202201\n",
      "[2025-12-10 21:55:48]   ESMA rows: 26587\n",
      "[2025-12-10 21:55:48]   ESMA rows: 26587\n",
      "[2025-12-10 21:55:48]   Saved 26,587 rows in 1.1s\n",
      "[2025-12-10 21:55:48] \n",
      "[62/180] Processing ESMA-only pool: 549300B0FLNFTFYQDJ30N201701\n",
      "[2025-12-10 21:55:48]   Saved 26,587 rows in 1.1s\n",
      "[2025-12-10 21:55:48] \n",
      "[62/180] Processing ESMA-only pool: 549300B0FLNFTFYQDJ30N201701\n",
      "[2025-12-10 21:55:49]   ESMA rows: 31518\n",
      "[2025-12-10 21:55:49]   ESMA rows: 31518\n",
      "[2025-12-10 21:55:50]   Saved 31,518 rows in 1.3s\n",
      "[2025-12-10 21:55:50] \n",
      "[63/180] Processing ESMA-only pool: 549300WBHY25B75DWR71N201401\n",
      "[2025-12-10 21:55:50]   ESMA rows: 7332\n",
      "[2025-12-10 21:55:50]   Saved 31,518 rows in 1.3s\n",
      "[2025-12-10 21:55:50] \n",
      "[63/180] Processing ESMA-only pool: 549300WBHY25B75DWR71N201401\n",
      "[2025-12-10 21:55:50]   ESMA rows: 7332\n",
      "[2025-12-10 21:55:50]   Saved 7,332 rows in 0.3s\n",
      "[2025-12-10 21:55:50] \n",
      "[64/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N202301\n",
      "[2025-12-10 21:55:50]   Saved 7,332 rows in 0.3s\n",
      "[2025-12-10 21:55:50] \n",
      "[64/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N202301\n",
      "[2025-12-10 21:55:51]   ESMA rows: 60362\n",
      "[2025-12-10 21:55:51]   ESMA rows: 60362\n",
      "[2025-12-10 21:55:52]   Saved 60,362 rows in 2.4s\n",
      "[2025-12-10 21:55:52] \n",
      "[65/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202101\n",
      "[2025-12-10 21:55:52]   Saved 60,362 rows in 2.4s\n",
      "[2025-12-10 21:55:52] \n",
      "[65/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202101\n",
      "[2025-12-10 21:55:53]   ESMA rows: 14999\n",
      "[2025-12-10 21:55:53]   ESMA rows: 14999\n",
      "[2025-12-10 21:55:53]   Saved 14,999 rows in 0.6s\n",
      "[2025-12-10 21:55:53] \n",
      "[66/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202401\n",
      "[2025-12-10 21:55:53]   Saved 14,999 rows in 0.6s\n",
      "[2025-12-10 21:55:53] \n",
      "[66/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202401\n",
      "[2025-12-10 21:55:53]   ESMA rows: 14091\n",
      "[2025-12-10 21:55:53]   ESMA rows: 14091\n",
      "[2025-12-10 21:55:54]   Saved 14,091 rows in 0.6s\n",
      "[2025-12-10 21:55:54] \n",
      "[67/180] Processing ESMA-only pool: 984500415841D00D2F22N202501\n",
      "[2025-12-10 21:55:54]   ESMA rows: 11611\n",
      "[2025-12-10 21:55:54]   Saved 14,091 rows in 0.6s\n",
      "[2025-12-10 21:55:54] \n",
      "[67/180] Processing ESMA-only pool: 984500415841D00D2F22N202501\n",
      "[2025-12-10 21:55:54]   ESMA rows: 11611\n",
      "[2025-12-10 21:55:54]   Saved 11,611 rows in 0.4s\n",
      "[2025-12-10 21:55:54] \n",
      "[68/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N200801\n",
      "[2025-12-10 21:55:54]   ESMA rows: 11015\n",
      "[2025-12-10 21:55:54]   Saved 11,611 rows in 0.4s\n",
      "[2025-12-10 21:55:54] \n",
      "[68/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N200801\n",
      "[2025-12-10 21:55:54]   ESMA rows: 11015\n",
      "[2025-12-10 21:55:55]   Saved 11,015 rows in 0.5s\n",
      "[2025-12-10 21:55:55] \n",
      "[69/180] Processing ESMA-only pool: 8IBZUGJ7JPLH368JE346N201801\n",
      "[2025-12-10 21:55:55]   ESMA rows: 6489\n",
      "[2025-12-10 21:55:55]   Saved 11,015 rows in 0.5s\n",
      "[2025-12-10 21:55:55] \n",
      "[69/180] Processing ESMA-only pool: 8IBZUGJ7JPLH368JE346N201801\n",
      "[2025-12-10 21:55:55]   ESMA rows: 6489\n",
      "[2025-12-10 21:55:55]   Saved 6,489 rows in 0.3s\n",
      "[2025-12-10 21:55:55] \n",
      "[70/180] Processing ESMA-only pool: 95980020140005213054N200601\n",
      "[2025-12-10 21:55:55]   ESMA rows: 12695\n",
      "[2025-12-10 21:55:55]   Saved 6,489 rows in 0.3s\n",
      "[2025-12-10 21:55:55] \n",
      "[70/180] Processing ESMA-only pool: 95980020140005213054N200601\n",
      "[2025-12-10 21:55:55]   ESMA rows: 12695\n",
      "[2025-12-10 21:55:55]   Saved 12,695 rows in 0.4s\n",
      "[2025-12-10 21:55:55] \n",
      "[71/180] Processing ESMA-only pool: F1T87K3OQ2OV1UORLH26N202002\n",
      "[2025-12-10 21:55:55]   Saved 12,695 rows in 0.4s\n",
      "[2025-12-10 21:55:55] \n",
      "[71/180] Processing ESMA-only pool: F1T87K3OQ2OV1UORLH26N202002\n",
      "[2025-12-10 21:55:56]   ESMA rows: 33873\n",
      "[2025-12-10 21:55:56]   ESMA rows: 33873\n",
      "[2025-12-10 21:55:57]   Saved 33,873 rows in 1.5s\n",
      "[2025-12-10 21:55:57] \n",
      "[72/180] Processing ESMA-only pool: 95980020140005209756N201101\n",
      "[2025-12-10 21:55:57]   ESMA rows: 17593\n",
      "[2025-12-10 21:55:57]   Saved 33,873 rows in 1.5s\n",
      "[2025-12-10 21:55:57] \n",
      "[72/180] Processing ESMA-only pool: 95980020140005209756N201101\n",
      "[2025-12-10 21:55:57]   ESMA rows: 17593\n",
      "[2025-12-10 21:55:57]   Saved 17,593 rows in 0.6s\n",
      "[2025-12-10 21:55:57] \n",
      "[73/180] Processing ESMA-only pool: 969500FEKK527RVCPM68N202401\n",
      "[2025-12-10 21:55:57]   Saved 17,593 rows in 0.6s\n",
      "[2025-12-10 21:55:57] \n",
      "[73/180] Processing ESMA-only pool: 969500FEKK527RVCPM68N202401\n",
      "[2025-12-10 21:55:58]   ESMA rows: 22298\n",
      "[2025-12-10 21:55:58]   ESMA rows: 22298\n",
      "[2025-12-10 21:55:58]   Saved 22,298 rows in 0.9s\n",
      "[2025-12-10 21:55:58] \n",
      "[74/180] Processing ESMA-only pool: VWMYAEQSTOPNV0SUGU82N200501\n",
      "[2025-12-10 21:55:58]   ESMA rows: 6149\n",
      "[2025-12-10 21:55:58]   Saved 22,298 rows in 0.9s\n",
      "[2025-12-10 21:55:58] \n",
      "[74/180] Processing ESMA-only pool: VWMYAEQSTOPNV0SUGU82N200501\n",
      "[2025-12-10 21:55:58]   ESMA rows: 6149\n",
      "[2025-12-10 21:55:59]   Saved 6,149 rows in 0.3s\n",
      "[2025-12-10 21:55:59] \n",
      "[75/180] Processing ESMA-only pool: 969500BEY2ZTHR3JG165N202401\n",
      "[2025-12-10 21:55:59]   Saved 6,149 rows in 0.3s\n",
      "[2025-12-10 21:55:59] \n",
      "[75/180] Processing ESMA-only pool: 969500BEY2ZTHR3JG165N202401\n",
      "[2025-12-10 21:56:00]   ESMA rows: 78397\n",
      "[2025-12-10 21:56:00]   ESMA rows: 78397\n",
      "[2025-12-10 21:56:01]   Saved 78,397 rows in 2.7s\n",
      "[2025-12-10 21:56:01] \n",
      "[76/180] Processing ESMA-only pool: 95980020140005501435N200701\n",
      "[2025-12-10 21:56:01]   Saved 78,397 rows in 2.7s\n",
      "[2025-12-10 21:56:01] \n",
      "[76/180] Processing ESMA-only pool: 95980020140005501435N200701\n",
      "[2025-12-10 21:56:02]   ESMA rows: 29750\n",
      "[2025-12-10 21:56:02]   ESMA rows: 29750\n",
      "[2025-12-10 21:56:02]   Saved 29,750 rows in 1.1s\n",
      "[2025-12-10 21:56:02] \n",
      "[77/180] Processing ESMA-only pool: 635400IU9RIO17FMKE17N202401\n",
      "[2025-12-10 21:56:03]   ESMA rows: 9625\n",
      "[2025-12-10 21:56:02]   Saved 29,750 rows in 1.1s\n",
      "[2025-12-10 21:56:02] \n",
      "[77/180] Processing ESMA-only pool: 635400IU9RIO17FMKE17N202401\n",
      "[2025-12-10 21:56:03]   ESMA rows: 9625\n",
      "[2025-12-10 21:56:03]   Saved 9,625 rows in 0.5s\n",
      "[2025-12-10 21:56:03] \n",
      "[78/180] Processing ESMA-only pool: 95980020140005501629N200601\n",
      "[2025-12-10 21:56:03]   ESMA rows: 9573\n",
      "[2025-12-10 21:56:03]   Saved 9,625 rows in 0.5s\n",
      "[2025-12-10 21:56:03] \n",
      "[78/180] Processing ESMA-only pool: 95980020140005501629N200601\n",
      "[2025-12-10 21:56:03]   ESMA rows: 9573\n",
      "[2025-12-10 21:56:03]   Saved 9,573 rows in 0.4s\n",
      "[2025-12-10 21:56:03] \n",
      "[79/180] Processing ESMA-only pool: 7245002ZUXNOEJ0QPZ44N202101\n",
      "[2025-12-10 21:56:03]   Saved 9,573 rows in 0.4s\n",
      "[2025-12-10 21:56:03] \n",
      "[79/180] Processing ESMA-only pool: 7245002ZUXNOEJ0QPZ44N202101\n",
      "[2025-12-10 21:56:04]   ESMA rows: 63510\n",
      "[2025-12-10 21:56:04]   ESMA rows: 63510\n",
      "[2025-12-10 21:56:06]   Saved 63,510 rows in 2.5s\n",
      "[2025-12-10 21:56:06] \n",
      "[80/180] Processing ESMA-only pool: 549300F2EUS6QS7H4D81N201901\n",
      "[2025-12-10 21:56:06]   Saved 63,510 rows in 2.5s\n",
      "[2025-12-10 21:56:06] \n",
      "[80/180] Processing ESMA-only pool: 549300F2EUS6QS7H4D81N201901\n",
      "[2025-12-10 21:56:06]   ESMA rows: 13236\n",
      "[2025-12-10 21:56:06]   ESMA rows: 13236\n",
      "[2025-12-10 21:56:06]   Saved 13,236 rows in 0.7s\n",
      "[2025-12-10 21:56:06] \n",
      "[81/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200701\n",
      "[2025-12-10 21:56:06]   Saved 13,236 rows in 0.7s\n",
      "[2025-12-10 21:56:06] \n",
      "[81/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200701\n",
      "[2025-12-10 21:56:07]   ESMA rows: 83792\n",
      "[2025-12-10 21:56:07]   ESMA rows: 83792\n",
      "[2025-12-10 21:56:09]   Saved 83,792 rows in 2.6s\n",
      "[2025-12-10 21:56:09] \n",
      "[82/180] Processing ESMA-only pool: 213800WQJJDCAN4BCO57N201901\n",
      "[2025-12-10 21:56:09]   Saved 83,792 rows in 2.6s\n",
      "[2025-12-10 21:56:09] \n",
      "[82/180] Processing ESMA-only pool: 213800WQJJDCAN4BCO57N201901\n",
      "[2025-12-10 21:56:10]   ESMA rows: 128842\n",
      "[2025-12-10 21:56:10]   ESMA rows: 128842\n",
      "[2025-12-10 21:56:13]   Saved 128,842 rows in 4.1s\n",
      "[2025-12-10 21:56:13] \n",
      "[83/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N202501\n",
      "[2025-12-10 21:56:13]   Saved 128,842 rows in 4.1s\n",
      "[2025-12-10 21:56:13] \n",
      "[83/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N202501\n",
      "[2025-12-10 21:56:14]   ESMA rows: 140471\n",
      "[2025-12-10 21:56:14]   ESMA rows: 140471\n",
      "[2025-12-10 21:56:17]   Saved 140,471 rows in 4.0s\n",
      "[2025-12-10 21:56:17] \n",
      "[84/180] Processing ESMA-only pool: VWMYAEQSTOPNV0SUGU82N200601\n",
      "[2025-12-10 21:56:17]   Saved 140,471 rows in 4.0s\n",
      "[2025-12-10 21:56:17] \n",
      "[84/180] Processing ESMA-only pool: VWMYAEQSTOPNV0SUGU82N200601\n",
      "[2025-12-10 21:56:17]   ESMA rows: 22857\n",
      "[2025-12-10 21:56:17]   ESMA rows: 22857\n",
      "[2025-12-10 21:56:18]   Saved 22,857 rows in 0.8s\n",
      "[2025-12-10 21:56:18] \n",
      "[85/180] Processing ESMA-only pool: 54930002GQ40DD57AK55N201801\n",
      "[2025-12-10 21:56:18]   Saved 22,857 rows in 0.8s\n",
      "[2025-12-10 21:56:18] \n",
      "[85/180] Processing ESMA-only pool: 54930002GQ40DD57AK55N201801\n",
      "[2025-12-10 21:56:19]   ESMA rows: 48180\n",
      "[2025-12-10 21:56:19]   ESMA rows: 48180\n",
      "[2025-12-10 21:56:20]   Saved 48,180 rows in 1.7s\n",
      "[2025-12-10 21:56:20] \n",
      "[86/180] Processing ESMA-only pool: 95980020140005212860N200701\n",
      "[2025-12-10 21:56:20]   ESMA rows: 12503\n",
      "[2025-12-10 21:56:20]   Saved 48,180 rows in 1.7s\n",
      "[2025-12-10 21:56:20] \n",
      "[86/180] Processing ESMA-only pool: 95980020140005212860N200701\n",
      "[2025-12-10 21:56:20]   ESMA rows: 12503\n",
      "[2025-12-10 21:56:20]   Saved 12,503 rows in 0.4s\n",
      "[2025-12-10 21:56:20] \n",
      "[87/180] Processing ESMA-only pool: 815600AA335E51233F89N201901\n",
      "[2025-12-10 21:56:20]   Saved 12,503 rows in 0.4s\n",
      "[2025-12-10 21:56:20] \n",
      "[87/180] Processing ESMA-only pool: 815600AA335E51233F89N201901\n",
      "[2025-12-10 21:56:21]   ESMA rows: 83682\n",
      "[2025-12-10 21:56:21]   ESMA rows: 83682\n",
      "[2025-12-10 21:56:23]   Saved 83,682 rows in 3.0s\n",
      "[2025-12-10 21:56:23] \n",
      "[88/180] Processing ESMA-only pool: 549300GP8VGBVVL86O86N201801\n",
      "[2025-12-10 21:56:23]   Saved 83,682 rows in 3.0s\n",
      "[2025-12-10 21:56:23] \n",
      "[88/180] Processing ESMA-only pool: 549300GP8VGBVVL86O86N201801\n",
      "[2025-12-10 21:56:24]   ESMA rows: 58768\n",
      "[2025-12-10 21:56:24]   ESMA rows: 58768\n",
      "[2025-12-10 21:56:25]   Saved 58,768 rows in 2.2s\n",
      "[2025-12-10 21:56:25] \n",
      "[89/180] Processing ESMA-only pool: 959800LQ598A5RQASA61N200612\n",
      "[2025-12-10 21:56:25]   ESMA rows: 8861\n",
      "[2025-12-10 21:56:25]   Saved 58,768 rows in 2.2s\n",
      "[2025-12-10 21:56:25] \n",
      "[89/180] Processing ESMA-only pool: 959800LQ598A5RQASA61N200612\n",
      "[2025-12-10 21:56:25]   ESMA rows: 8861\n",
      "[2025-12-10 21:56:26]   Saved 8,861 rows in 0.4s\n",
      "[2025-12-10 21:56:26] \n",
      "[90/180] Processing ESMA-only pool: 95980020140005209368N202001\n",
      "[2025-12-10 21:56:26]   Saved 8,861 rows in 0.4s\n",
      "[2025-12-10 21:56:26] \n",
      "[90/180] Processing ESMA-only pool: 95980020140005209368N202001\n",
      "[2025-12-10 21:56:27]   ESMA rows: 88860\n",
      "[2025-12-10 21:56:27]   ESMA rows: 88860\n",
      "[2025-12-10 21:56:29]   Saved 88,860 rows in 3.5s\n",
      "[2025-12-10 21:56:29] \n",
      "[91/180] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N201801\n",
      "[2025-12-10 21:56:29]   Saved 88,860 rows in 3.5s\n",
      "[2025-12-10 21:56:29] \n",
      "[91/180] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N201801\n",
      "[2025-12-10 21:56:30]   ESMA rows: 26862\n",
      "[2025-12-10 21:56:30]   ESMA rows: 26862\n",
      "[2025-12-10 21:56:30]   Saved 26,862 rows in 1.1s\n",
      "[2025-12-10 21:56:30] \n",
      "[92/180] Processing ESMA-only pool: 635400X8LJOEVGKVCU62N202001\n",
      "[2025-12-10 21:56:30]   Saved 26,862 rows in 1.1s\n",
      "[2025-12-10 21:56:30] \n",
      "[92/180] Processing ESMA-only pool: 635400X8LJOEVGKVCU62N202001\n",
      "[2025-12-10 21:56:31]   ESMA rows: 62118\n",
      "[2025-12-10 21:56:31]   ESMA rows: 62118\n",
      "[2025-12-10 21:56:33]   Saved 62,118 rows in 2.5s\n",
      "[2025-12-10 21:56:33] \n",
      "[93/180] Processing ESMA-only pool: J4CP7MHCXR8DAQMKIL78N200702\n",
      "[2025-12-10 21:56:33]   Saved 62,118 rows in 2.5s\n",
      "[2025-12-10 21:56:33] \n",
      "[93/180] Processing ESMA-only pool: J4CP7MHCXR8DAQMKIL78N200702\n",
      "[2025-12-10 21:56:34]   ESMA rows: 113569\n",
      "[2025-12-10 21:56:34]   ESMA rows: 113569\n",
      "[2025-12-10 21:56:37]   Saved 113,569 rows in 3.6s\n",
      "[2025-12-10 21:56:37] \n",
      "[94/180] Processing ESMA-only pool: 95980020140005209368N202102\n",
      "[2025-12-10 21:56:37]   Saved 113,569 rows in 3.6s\n",
      "[2025-12-10 21:56:37] \n",
      "[94/180] Processing ESMA-only pool: 95980020140005209368N202102\n",
      "[2025-12-10 21:56:38]   ESMA rows: 79390\n",
      "[2025-12-10 21:56:38]   ESMA rows: 79390\n",
      "[2025-12-10 21:56:40]   Saved 79,390 rows in 3.2s\n",
      "[2025-12-10 21:56:40] \n",
      "[95/180] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N200602\n",
      "[2025-12-10 21:56:40]   ESMA rows: 8550\n",
      "[2025-12-10 21:56:40]   Saved 79,390 rows in 3.2s\n",
      "[2025-12-10 21:56:40] \n",
      "[95/180] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N200602\n",
      "[2025-12-10 21:56:40]   ESMA rows: 8550\n",
      "[2025-12-10 21:56:40]   Saved 8,550 rows in 0.4s\n",
      "[2025-12-10 21:56:40] \n",
      "[96/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202401\n",
      "[2025-12-10 21:56:40]   ESMA rows: 11442\n",
      "[2025-12-10 21:56:40]   Saved 8,550 rows in 0.4s\n",
      "[2025-12-10 21:56:40] \n",
      "[96/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202401\n",
      "[2025-12-10 21:56:40]   ESMA rows: 11442\n",
      "[2025-12-10 21:56:41]   Saved 11,442 rows in 0.5s\n",
      "[2025-12-10 21:56:41] \n",
      "[97/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N202101\n",
      "[2025-12-10 21:56:41]   Saved 11,442 rows in 0.5s\n",
      "[2025-12-10 21:56:41] \n",
      "[97/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N202101\n",
      "[2025-12-10 21:56:41]   ESMA rows: 14064\n",
      "[2025-12-10 21:56:41]   ESMA rows: 14064\n",
      "[2025-12-10 21:56:41]   Saved 14,064 rows in 0.6s\n",
      "[2025-12-10 21:56:41] \n",
      "[98/180] Processing ESMA-only pool: 7245002ZUXNOEJ0QPZ44N202501\n",
      "[2025-12-10 21:56:41]   ESMA rows: 12374\n",
      "[2025-12-10 21:56:41]   Saved 14,064 rows in 0.6s\n",
      "[2025-12-10 21:56:41] \n",
      "[98/180] Processing ESMA-only pool: 7245002ZUXNOEJ0QPZ44N202501\n",
      "[2025-12-10 21:56:41]   ESMA rows: 12374\n",
      "[2025-12-10 21:56:42]   Saved 12,374 rows in 0.5s\n",
      "[2025-12-10 21:56:42] \n",
      "[99/180] Processing ESMA-only pool: 549300HXUI1QLHV9NT89N201701\n",
      "[2025-12-10 21:56:42]   Saved 12,374 rows in 0.5s\n",
      "[2025-12-10 21:56:42] \n",
      "[99/180] Processing ESMA-only pool: 549300HXUI1QLHV9NT89N201701\n",
      "[2025-12-10 21:56:43]   ESMA rows: 101180\n",
      "[2025-12-10 21:56:43]   ESMA rows: 101180\n",
      "[2025-12-10 21:56:45]   Saved 101,180 rows in 3.2s\n",
      "[2025-12-10 21:56:45] \n",
      "[100/180] Processing ESMA-only pool: 95980020140005212957N200601\n",
      "[2025-12-10 21:56:45]   ESMA rows: 11820\n",
      "[2025-12-10 21:56:45]   Saved 101,180 rows in 3.2s\n",
      "[2025-12-10 21:56:45] \n",
      "[100/180] Processing ESMA-only pool: 95980020140005212957N200601\n",
      "[2025-12-10 21:56:45]   ESMA rows: 11820\n",
      "[2025-12-10 21:56:46]   Saved 11,820 rows in 0.5s\n",
      "[2025-12-10 21:56:46] \n",
      "[101/180] Processing ESMA-only pool: 213800LIUKUVD7YK6W32N200701\n",
      "[2025-12-10 21:56:46]   ESMA rows: 12014\n",
      "[2025-12-10 21:56:46]   Saved 11,820 rows in 0.5s\n",
      "[2025-12-10 21:56:46] \n",
      "[101/180] Processing ESMA-only pool: 213800LIUKUVD7YK6W32N200701\n",
      "[2025-12-10 21:56:46]   ESMA rows: 12014\n",
      "[2025-12-10 21:56:46]   Saved 12,014 rows in 0.4s\n",
      "[2025-12-10 21:56:46] \n",
      "[102/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202201\n",
      "[2025-12-10 21:56:46]   Saved 12,014 rows in 0.4s\n",
      "[2025-12-10 21:56:46] \n",
      "[102/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202201\n",
      "[2025-12-10 21:56:46]   ESMA rows: 30280\n",
      "[2025-12-10 21:56:46]   ESMA rows: 30280\n",
      "[2025-12-10 21:56:47]   Saved 30,280 rows in 1.1s\n",
      "[2025-12-10 21:56:47] \n",
      "[103/180] Processing ESMA-only pool: 7245006LFTO2U2HODE80N202201\n",
      "[2025-12-10 21:56:47]   Saved 30,280 rows in 1.1s\n",
      "[2025-12-10 21:56:47] \n",
      "[103/180] Processing ESMA-only pool: 7245006LFTO2U2HODE80N202201\n",
      "[2025-12-10 21:56:48]   ESMA rows: 46041\n",
      "[2025-12-10 21:56:48]   ESMA rows: 46041\n",
      "[2025-12-10 21:56:49]   Saved 46,041 rows in 2.0s\n",
      "[2025-12-10 21:56:49] \n",
      "[104/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202101\n",
      "[2025-12-10 21:56:49]   Saved 46,041 rows in 2.0s\n",
      "[2025-12-10 21:56:49] \n",
      "[104/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202101\n",
      "[2025-12-10 21:56:50]   ESMA rows: 69351\n",
      "[2025-12-10 21:56:50]   ESMA rows: 69351\n",
      "[2025-12-10 21:56:52]   Saved 69,351 rows in 2.7s\n",
      "[2025-12-10 21:56:52] \n",
      "[105/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200801\n",
      "[2025-12-10 21:56:52]   Saved 69,351 rows in 2.7s\n",
      "[2025-12-10 21:56:52] \n",
      "[105/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200801\n",
      "[2025-12-10 21:56:53]   ESMA rows: 80741\n",
      "[2025-12-10 21:56:53]   ESMA rows: 80741\n",
      "[2025-12-10 21:56:54]   Saved 80,741 rows in 2.6s\n",
      "[2025-12-10 21:56:54] \n",
      "[106/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200702\n",
      "[2025-12-10 21:56:54]   Saved 80,741 rows in 2.6s\n",
      "[2025-12-10 21:56:54] \n",
      "[106/180] Processing ESMA-only pool: 7CUNS533WID6K7DGFI87N200702\n",
      "[2025-12-10 21:56:55]   ESMA rows: 21154\n",
      "[2025-12-10 21:56:55]   ESMA rows: 21154\n",
      "[2025-12-10 21:56:55]   Saved 21,154 rows in 0.8s\n",
      "[2025-12-10 21:56:55] \n",
      "[107/180] Processing ESMA-only pool: 635400DYIVV4EPKDBN26N202201\n",
      "[2025-12-10 21:56:55]   Saved 21,154 rows in 0.8s\n",
      "[2025-12-10 21:56:55] \n",
      "[107/180] Processing ESMA-only pool: 635400DYIVV4EPKDBN26N202201\n",
      "[2025-12-10 21:56:56]   ESMA rows: 73098\n",
      "[2025-12-10 21:56:56]   ESMA rows: 73098\n",
      "[2025-12-10 21:56:58]   Saved 73,098 rows in 3.0s\n",
      "[2025-12-10 21:56:58] \n",
      "[108/180] Processing ESMA-only pool: 549300F2EUS6QS7H4D81N202001\n",
      "[2025-12-10 21:56:58]   Saved 73,098 rows in 3.0s\n",
      "[2025-12-10 21:56:58] \n",
      "[108/180] Processing ESMA-only pool: 549300F2EUS6QS7H4D81N202001\n",
      "[2025-12-10 21:56:59]   ESMA rows: 36648\n",
      "[2025-12-10 21:56:59]   ESMA rows: 36648\n",
      "[2025-12-10 21:57:00]   Saved 36,648 rows in 1.7s\n",
      "[2025-12-10 21:57:00] \n",
      "[109/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N202501\n",
      "[2025-12-10 21:57:00]   ESMA rows: 16987\n",
      "[2025-12-10 21:57:00]   Saved 36,648 rows in 1.7s\n",
      "[2025-12-10 21:57:00] \n",
      "[109/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N202501\n",
      "[2025-12-10 21:57:00]   ESMA rows: 16987\n",
      "[2025-12-10 21:57:01]   Saved 16,987 rows in 0.6s\n",
      "[2025-12-10 21:57:01] \n",
      "[110/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200601\n",
      "[2025-12-10 21:57:01]   Saved 16,987 rows in 0.6s\n",
      "[2025-12-10 21:57:01] \n",
      "[110/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200601\n",
      "[2025-12-10 21:57:01]   ESMA rows: 14658\n",
      "[2025-12-10 21:57:01]   ESMA rows: 14658\n",
      "[2025-12-10 21:57:01]   Saved 14,658 rows in 0.6s\n",
      "[2025-12-10 21:57:01] \n",
      "[111/180] Processing ESMA-only pool: 549300RUEYYMW163W968N201702\n",
      "[2025-12-10 21:57:01]   Saved 14,658 rows in 0.6s\n",
      "[2025-12-10 21:57:01] \n",
      "[111/180] Processing ESMA-only pool: 549300RUEYYMW163W968N201702\n",
      "[2025-12-10 21:57:02]   ESMA rows: 44880\n",
      "[2025-12-10 21:57:02]   ESMA rows: 44880\n",
      "[2025-12-10 21:57:03]   Saved 44,880 rows in 1.5s\n",
      "[2025-12-10 21:57:03] \n",
      "[112/180] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N200401\n",
      "[2025-12-10 21:57:03]   ESMA rows: 4944\n",
      "[2025-12-10 21:57:03]   Saved 44,880 rows in 1.5s\n",
      "[2025-12-10 21:57:03] \n",
      "[112/180] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N200401\n",
      "[2025-12-10 21:57:03]   ESMA rows: 4944\n",
      "[2025-12-10 21:57:03]   Saved 4,944 rows in 0.3s\n",
      "[2025-12-10 21:57:03] \n",
      "[113/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N201901\n",
      "[2025-12-10 21:57:03]   Saved 4,944 rows in 0.3s\n",
      "[2025-12-10 21:57:03] \n",
      "[113/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N201901\n",
      "[2025-12-10 21:57:03]   ESMA rows: 32858\n",
      "[2025-12-10 21:57:03]   ESMA rows: 32858\n",
      "[2025-12-10 21:57:04]   Saved 32,858 rows in 1.2s\n",
      "[2025-12-10 21:57:04] \n",
      "[114/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202301\n",
      "[2025-12-10 21:57:04]   Saved 32,858 rows in 1.2s\n",
      "[2025-12-10 21:57:04] \n",
      "[114/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202301\n",
      "[2025-12-10 21:57:04]   ESMA rows: 22269\n",
      "[2025-12-10 21:57:04]   ESMA rows: 22269\n",
      "[2025-12-10 21:57:05]   Saved 22,269 rows in 0.8s\n",
      "[2025-12-10 21:57:05] \n",
      "[115/180] Processing ESMA-only pool: 549300TRUWO2CD2G5692N200701\n",
      "[2025-12-10 21:57:05]   Saved 22,269 rows in 0.8s\n",
      "[2025-12-10 21:57:05] \n",
      "[115/180] Processing ESMA-only pool: 549300TRUWO2CD2G5692N200701\n",
      "[2025-12-10 21:57:05]   ESMA rows: 29952\n",
      "[2025-12-10 21:57:05]   ESMA rows: 29952\n",
      "[2025-12-10 21:57:06]   Saved 29,952 rows in 1.0s\n",
      "[2025-12-10 21:57:06] \n",
      "[116/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202102\n",
      "[2025-12-10 21:57:06]   Saved 29,952 rows in 1.0s\n",
      "[2025-12-10 21:57:06] \n",
      "[116/180] Processing ESMA-only pool: 6354008Z6PDP7AXBAD38N202102\n",
      "[2025-12-10 21:57:06]   ESMA rows: 21705\n",
      "[2025-12-10 21:57:06]   ESMA rows: 21705\n",
      "[2025-12-10 21:57:07]   Saved 21,705 rows in 0.9s\n",
      "[2025-12-10 21:57:07] \n",
      "[117/180] Processing ESMA-only pool: 724500T5HC7NI06PEW24N202001\n",
      "[2025-12-10 21:57:07]   Saved 21,705 rows in 0.9s\n",
      "[2025-12-10 21:57:07] \n",
      "[117/180] Processing ESMA-only pool: 724500T5HC7NI06PEW24N202001\n",
      "[2025-12-10 21:57:07]   ESMA rows: 18186\n",
      "[2025-12-10 21:57:07]   ESMA rows: 18186\n",
      "[2025-12-10 21:57:08]   Saved 18,186 rows in 0.9s\n",
      "[2025-12-10 21:57:08] \n",
      "[118/180] Processing ESMA-only pool: 724500FUE2OO0PBKK238N202501\n",
      "[2025-12-10 21:57:08]   ESMA rows: 5775\n",
      "[2025-12-10 21:57:08]   Saved 18,186 rows in 0.9s\n",
      "[2025-12-10 21:57:08] \n",
      "[118/180] Processing ESMA-only pool: 724500FUE2OO0PBKK238N202501\n",
      "[2025-12-10 21:57:08]   ESMA rows: 5775\n",
      "[2025-12-10 21:57:08]   Saved 5,775 rows in 0.3s\n",
      "[2025-12-10 21:57:08] \n",
      "[119/180] Processing ESMA-only pool: 54930066NQ53X7Z8DO76N200701\n",
      "[2025-12-10 21:57:08]   Saved 5,775 rows in 0.3s\n",
      "[2025-12-10 21:57:08] \n",
      "[119/180] Processing ESMA-only pool: 54930066NQ53X7Z8DO76N200701\n",
      "[2025-12-10 21:57:09]   ESMA rows: 83785\n",
      "[2025-12-10 21:57:09]   ESMA rows: 83785\n",
      "[2025-12-10 21:57:11]   Saved 83,785 rows in 2.8s\n",
      "[2025-12-10 21:57:11] \n",
      "[120/180] Processing ESMA-only pool: 8156003A4FB445454553N201801\n",
      "[2025-12-10 21:57:11]   ESMA rows: 12741\n",
      "[2025-12-10 21:57:11]   Saved 83,785 rows in 2.8s\n",
      "[2025-12-10 21:57:11] \n",
      "[120/180] Processing ESMA-only pool: 8156003A4FB445454553N201801\n",
      "[2025-12-10 21:57:11]   ESMA rows: 12741\n",
      "[2025-12-10 21:57:11]   Saved 12,741 rows in 0.5s\n",
      "[2025-12-10 21:57:11] \n",
      "[121/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202101\n",
      "[2025-12-10 21:57:11]   Saved 12,741 rows in 0.5s\n",
      "[2025-12-10 21:57:11] \n",
      "[121/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202101\n",
      "[2025-12-10 21:57:12]   ESMA rows: 43481\n",
      "[2025-12-10 21:57:12]   ESMA rows: 43481\n",
      "[2025-12-10 21:57:13]   Saved 43,481 rows in 1.4s\n",
      "[2025-12-10 21:57:13] \n",
      "[122/180] Processing ESMA-only pool: 81560027D07F9BDB8436N202401\n",
      "[2025-12-10 21:57:13]   Saved 43,481 rows in 1.4s\n",
      "[2025-12-10 21:57:13] \n",
      "[122/180] Processing ESMA-only pool: 81560027D07F9BDB8436N202401\n",
      "[2025-12-10 21:57:13]   ESMA rows: 38134\n",
      "[2025-12-10 21:57:13]   ESMA rows: 38134\n",
      "[2025-12-10 21:57:14]   Saved 38,134 rows in 1.2s\n",
      "[2025-12-10 21:57:14] \n",
      "[123/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200607\n",
      "[2025-12-10 21:57:14]   Saved 38,134 rows in 1.2s\n",
      "[2025-12-10 21:57:14] \n",
      "[123/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200607\n",
      "[2025-12-10 21:57:14]   ESMA rows: 24197\n",
      "[2025-12-10 21:57:14]   ESMA rows: 24197\n",
      "[2025-12-10 21:57:15]   Saved 24,197 rows in 0.8s\n",
      "[2025-12-10 21:57:15] \n",
      "[124/180] Processing ESMA-only pool: 8IBZUGJ7JPLH368JE346N202101\n",
      "[2025-12-10 21:57:15]   Saved 24,197 rows in 0.8s\n",
      "[2025-12-10 21:57:15] \n",
      "[124/180] Processing ESMA-only pool: 8IBZUGJ7JPLH368JE346N202101\n",
      "[2025-12-10 21:57:16]   ESMA rows: 79677\n",
      "[2025-12-10 21:57:16]   ESMA rows: 79677\n",
      "[2025-12-10 21:57:17]   Saved 79,677 rows in 2.7s\n",
      "[2025-12-10 21:57:17] \n",
      "[125/180] Processing ESMA-only pool: 984500B8E9RW7C014018N202501\n",
      "[2025-12-10 21:57:17]   ESMA rows: 9719\n",
      "[2025-12-10 21:57:17]   Saved 79,677 rows in 2.7s\n",
      "[2025-12-10 21:57:17] \n",
      "[125/180] Processing ESMA-only pool: 984500B8E9RW7C014018N202501\n",
      "[2025-12-10 21:57:17]   ESMA rows: 9719\n",
      "[2025-12-10 21:57:18]   Saved 9,719 rows in 0.3s\n",
      "[2025-12-10 21:57:18] \n",
      "[126/180] Processing ESMA-only pool: JU1U6S0DG9YLT7N8ZV32N200501\n",
      "[2025-12-10 21:57:18]   Saved 9,719 rows in 0.3s\n",
      "[2025-12-10 21:57:18] \n",
      "[126/180] Processing ESMA-only pool: JU1U6S0DG9YLT7N8ZV32N200501\n",
      "[2025-12-10 21:57:18]   ESMA rows: 48840\n",
      "[2025-12-10 21:57:18]   ESMA rows: 48840\n",
      "[2025-12-10 21:57:19]   Saved 48,840 rows in 1.4s\n",
      "[2025-12-10 21:57:19] \n",
      "[127/180] Processing ESMA-only pool: 549300UE2ZF21B02QG64N201901\n",
      "[2025-12-10 21:57:19]   Saved 48,840 rows in 1.4s\n",
      "[2025-12-10 21:57:19] \n",
      "[127/180] Processing ESMA-only pool: 549300UE2ZF21B02QG64N201901\n",
      "[2025-12-10 21:57:19]   ESMA rows: 53087\n",
      "[2025-12-10 21:57:19]   ESMA rows: 53087\n",
      "[2025-12-10 21:57:21]   Saved 53,087 rows in 1.6s\n",
      "[2025-12-10 21:57:21] \n",
      "[128/180] Processing ESMA-only pool: 2138001WO21Z3B8Y8K20N202101\n",
      "[2025-12-10 21:57:21]   Saved 53,087 rows in 1.6s\n",
      "[2025-12-10 21:57:21] \n",
      "[128/180] Processing ESMA-only pool: 2138001WO21Z3B8Y8K20N202101\n",
      "[2025-12-10 21:57:22]   ESMA rows: 73277\n",
      "[2025-12-10 21:57:22]   ESMA rows: 73277\n",
      "[2025-12-10 21:57:23]   Saved 73,277 rows in 2.7s\n",
      "[2025-12-10 21:57:23] \n",
      "[129/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N202101\n",
      "[2025-12-10 21:57:23]   Saved 73,277 rows in 2.7s\n",
      "[2025-12-10 21:57:23] \n",
      "[129/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N202101\n",
      "[2025-12-10 21:57:24]   ESMA rows: 90669\n",
      "[2025-12-10 21:57:24]   ESMA rows: 90669\n",
      "[2025-12-10 21:57:26]   Saved 90,669 rows in 2.9s\n",
      "[2025-12-10 21:57:26] \n",
      "[130/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N201804\n",
      "[2025-12-10 21:57:26]   Saved 90,669 rows in 2.9s\n",
      "[2025-12-10 21:57:26] \n",
      "[130/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N201804\n",
      "[2025-12-10 21:57:27]   ESMA rows: 92133\n",
      "[2025-12-10 21:57:27]   ESMA rows: 92133\n",
      "[2025-12-10 21:57:29]   Saved 92,133 rows in 2.8s\n",
      "[2025-12-10 21:57:29] \n",
      "[131/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N201801\n",
      "[2025-12-10 21:57:29]   Saved 92,133 rows in 2.8s\n",
      "[2025-12-10 21:57:29] \n",
      "[131/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N201801\n",
      "[2025-12-10 21:57:30]   ESMA rows: 73501\n",
      "[2025-12-10 21:57:30]   ESMA rows: 73501\n",
      "[2025-12-10 21:57:31]   Saved 73,501 rows in 2.0s\n",
      "[2025-12-10 21:57:31] \n",
      "[132/180] Processing ESMA-only pool: 95980020140005216158N200802\n",
      "[2025-12-10 21:57:31]   ESMA rows: 6168\n",
      "[2025-12-10 21:57:31]   Saved 73,501 rows in 2.0s\n",
      "[2025-12-10 21:57:31] \n",
      "[132/180] Processing ESMA-only pool: 95980020140005216158N200802\n",
      "[2025-12-10 21:57:31]   ESMA rows: 6168\n",
      "[2025-12-10 21:57:31]   Saved 6,168 rows in 0.3s\n",
      "[2025-12-10 21:57:31] \n",
      "[133/180] Processing ESMA-only pool: 549300UE2ZF21B02QG64N201701\n",
      "[2025-12-10 21:57:31]   Saved 6,168 rows in 0.3s\n",
      "[2025-12-10 21:57:31] \n",
      "[133/180] Processing ESMA-only pool: 549300UE2ZF21B02QG64N201701\n",
      "[2025-12-10 21:57:32]   ESMA rows: 33868\n",
      "[2025-12-10 21:57:32]   ESMA rows: 33868\n",
      "[2025-12-10 21:57:32]   Saved 33,868 rows in 1.0s\n",
      "[2025-12-10 21:57:32] \n",
      "[134/180] Processing ESMA-only pool: 815600BEE9123CD92E12N202201\n",
      "[2025-12-10 21:57:32]   Saved 33,868 rows in 1.0s\n",
      "[2025-12-10 21:57:32] \n",
      "[134/180] Processing ESMA-only pool: 815600BEE9123CD92E12N202201\n",
      "[2025-12-10 21:57:33]   ESMA rows: 111503\n",
      "[2025-12-10 21:57:33]   ESMA rows: 111503\n",
      "[2025-12-10 21:57:36]   Saved 111,503 rows in 3.4s\n",
      "[2025-12-10 21:57:36] \n",
      "[135/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202501\n",
      "[2025-12-10 21:57:36]   ESMA rows: 3873\n",
      "[2025-12-10 21:57:36]   Saved 3,873 rows in 0.2s\n",
      "[2025-12-10 21:57:36] \n",
      "[136/180] Processing ESMA-only pool: 959800SK3N802RGT5340N201301\n",
      "[2025-12-10 21:57:36]   Saved 111,503 rows in 3.4s\n",
      "[2025-12-10 21:57:36] \n",
      "[135/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202501\n",
      "[2025-12-10 21:57:36]   ESMA rows: 3873\n",
      "[2025-12-10 21:57:36]   Saved 3,873 rows in 0.2s\n",
      "[2025-12-10 21:57:36] \n",
      "[136/180] Processing ESMA-only pool: 959800SK3N802RGT5340N201301\n",
      "[2025-12-10 21:57:36]   ESMA rows: 4316\n",
      "[2025-12-10 21:57:36]   Saved 4,316 rows in 0.2s\n",
      "[2025-12-10 21:57:36] \n",
      "[137/180] Processing ESMA-only pool: 9598000HT3WLVN4DB470N201401\n",
      "[2025-12-10 21:57:36]   ESMA rows: 4316\n",
      "[2025-12-10 21:57:36]   Saved 4,316 rows in 0.2s\n",
      "[2025-12-10 21:57:36] \n",
      "[137/180] Processing ESMA-only pool: 9598000HT3WLVN4DB470N201401\n",
      "[2025-12-10 21:57:36]   ESMA rows: 2509\n",
      "[2025-12-10 21:57:36]   Saved 2,509 rows in 0.2s\n",
      "[2025-12-10 21:57:36] \n",
      "[138/180] Processing ESMA-only pool: 5493007SJLLCTM6J6M37N201001\n",
      "[2025-12-10 21:57:36]   ESMA rows: 2509\n",
      "[2025-12-10 21:57:36]   Saved 2,509 rows in 0.2s\n",
      "[2025-12-10 21:57:36] \n",
      "[138/180] Processing ESMA-only pool: 5493007SJLLCTM6J6M37N201001\n",
      "[2025-12-10 21:57:37]   ESMA rows: 38911\n",
      "[2025-12-10 21:57:37]   ESMA rows: 38911\n",
      "[2025-12-10 21:57:38]   Saved 38,911 rows in 1.2s\n",
      "[2025-12-10 21:57:38] \n",
      "[139/180] Processing ESMA-only pool: 959800AJTB9KP9HKB323N202401\n",
      "[2025-12-10 21:57:38]   Saved 38,911 rows in 1.2s\n",
      "[2025-12-10 21:57:38] \n",
      "[139/180] Processing ESMA-only pool: 959800AJTB9KP9HKB323N202401\n",
      "[2025-12-10 21:57:38]   ESMA rows: 82288\n",
      "[2025-12-10 21:57:38]   ESMA rows: 82288\n",
      "[2025-12-10 21:57:40]   Saved 82,288 rows in 2.4s\n",
      "[2025-12-10 21:57:40] \n",
      "[140/180] Processing ESMA-only pool: 959800CVG0VNETWAF746N202501\n",
      "[2025-12-10 21:57:40]   Saved 82,288 rows in 2.4s\n",
      "[2025-12-10 21:57:40] \n",
      "[140/180] Processing ESMA-only pool: 959800CVG0VNETWAF746N202501\n",
      "[2025-12-10 21:57:41]   ESMA rows: 58441\n",
      "[2025-12-10 21:57:41]   ESMA rows: 58441\n",
      "[2025-12-10 21:57:42]   Saved 58,441 rows in 1.7s\n",
      "[2025-12-10 21:57:42] \n",
      "[141/180] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202401\n",
      "[2025-12-10 21:57:42]   Saved 58,441 rows in 1.7s\n",
      "[2025-12-10 21:57:42] \n",
      "[141/180] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202401\n",
      "[2025-12-10 21:57:43]   ESMA rows: 93615\n",
      "[2025-12-10 21:57:43]   ESMA rows: 93615\n",
      "[2025-12-10 21:57:45]   Saved 93,615 rows in 2.9s\n",
      "[2025-12-10 21:57:45] \n",
      "[142/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N200802\n",
      "[2025-12-10 21:57:45]   ESMA rows: 17169\n",
      "[2025-12-10 21:57:45]   Saved 93,615 rows in 2.9s\n",
      "[2025-12-10 21:57:45] \n",
      "[142/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N200802\n",
      "[2025-12-10 21:57:45]   ESMA rows: 17169\n",
      "[2025-12-10 21:57:45]   Saved 17,169 rows in 0.6s\n",
      "[2025-12-10 21:57:45] \n",
      "[143/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N200701\n",
      "[2025-12-10 21:57:45]   ESMA rows: 15056\n",
      "[2025-12-10 21:57:45]   Saved 17,169 rows in 0.6s\n",
      "[2025-12-10 21:57:45] \n",
      "[143/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N200701\n",
      "[2025-12-10 21:57:45]   ESMA rows: 15056\n",
      "[2025-12-10 21:57:46]   Saved 15,056 rows in 0.5s\n",
      "[2025-12-10 21:57:46] \n",
      "[144/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N202401\n",
      "[2025-12-10 21:57:46]   Saved 15,056 rows in 0.5s\n",
      "[2025-12-10 21:57:46] \n",
      "[144/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N202401\n",
      "[2025-12-10 21:57:46]   ESMA rows: 44986\n",
      "[2025-12-10 21:57:46]   ESMA rows: 44986\n",
      "[2025-12-10 21:57:47]   Saved 44,986 rows in 1.6s\n",
      "[2025-12-10 21:57:47] \n",
      "[145/180] Processing ESMA-only pool: 54930056IRBXK0Q1FP96N201701\n",
      "[2025-12-10 21:57:48]   ESMA rows: 16602\n",
      "[2025-12-10 21:57:47]   Saved 44,986 rows in 1.6s\n",
      "[2025-12-10 21:57:47] \n",
      "[145/180] Processing ESMA-only pool: 54930056IRBXK0Q1FP96N201701\n",
      "[2025-12-10 21:57:48]   ESMA rows: 16602\n",
      "[2025-12-10 21:57:48]   Saved 16,602 rows in 0.5s\n",
      "[2025-12-10 21:57:48] \n",
      "[146/180] Processing ESMA-only pool: 724500T5HC7NI06PEW24N201902\n",
      "[2025-12-10 21:57:48]   Saved 16,602 rows in 0.5s\n",
      "[2025-12-10 21:57:48] \n",
      "[146/180] Processing ESMA-only pool: 724500T5HC7NI06PEW24N201902\n",
      "[2025-12-10 21:57:48]   ESMA rows: 23635\n",
      "[2025-12-10 21:57:48]   ESMA rows: 23635\n",
      "[2025-12-10 21:57:49]   Saved 23,635 rows in 0.9s\n",
      "[2025-12-10 21:57:49] \n",
      "[147/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200601\n",
      "[2025-12-10 21:57:49]   Saved 23,635 rows in 0.9s\n",
      "[2025-12-10 21:57:49] \n",
      "[147/180] Processing ESMA-only pool: 549300U4LIZV0REEQQ46N200601\n",
      "[2025-12-10 21:57:49]   ESMA rows: 26420\n",
      "[2025-12-10 21:57:49]   ESMA rows: 26420\n",
      "[2025-12-10 21:57:50]   Saved 26,420 rows in 0.8s\n",
      "[2025-12-10 21:57:50] \n",
      "[148/180] Processing ESMA-only pool: 724500O4GUVTGSZEU248N202501\n",
      "[2025-12-10 21:57:50]   ESMA rows: 17788\n",
      "[2025-12-10 21:57:50]   Saved 26,420 rows in 0.8s\n",
      "[2025-12-10 21:57:50] \n",
      "[148/180] Processing ESMA-only pool: 724500O4GUVTGSZEU248N202501\n",
      "[2025-12-10 21:57:50]   ESMA rows: 17788\n",
      "[2025-12-10 21:57:50]   Saved 17,788 rows in 0.6s\n",
      "[2025-12-10 21:57:50] \n",
      "[149/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202202\n",
      "[2025-12-10 21:57:50]   ESMA rows: 12232\n",
      "[2025-12-10 21:57:50]   Saved 17,788 rows in 0.6s\n",
      "[2025-12-10 21:57:50] \n",
      "[149/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202202\n",
      "[2025-12-10 21:57:50]   ESMA rows: 12232\n",
      "[2025-12-10 21:57:51]   Saved 12,232 rows in 0.4s\n",
      "[2025-12-10 21:57:51] \n",
      "[150/180] Processing ESMA-only pool: 635400X8LJOEVGKVCU62N202201\n",
      "[2025-12-10 21:57:51]   Saved 12,232 rows in 0.4s\n",
      "[2025-12-10 21:57:51] \n",
      "[150/180] Processing ESMA-only pool: 635400X8LJOEVGKVCU62N202201\n",
      "[2025-12-10 21:57:51]   ESMA rows: 34963\n",
      "[2025-12-10 21:57:51]   ESMA rows: 34963\n",
      "[2025-12-10 21:57:52]   Saved 34,963 rows in 1.3s\n",
      "[2025-12-10 21:57:52] \n",
      "[151/180] Processing ESMA-only pool: 549300YAVKUX02HU6210N200501\n",
      "[2025-12-10 21:57:52]   Saved 34,963 rows in 1.3s\n",
      "[2025-12-10 21:57:52] \n",
      "[151/180] Processing ESMA-only pool: 549300YAVKUX02HU6210N200501\n",
      "[2025-12-10 21:57:53]   ESMA rows: 98100\n",
      "[2025-12-10 21:57:53]   ESMA rows: 98100\n",
      "[2025-12-10 21:57:54]   Saved 98,100 rows in 2.6s\n",
      "[2025-12-10 21:57:54] \n",
      "[152/180] Processing ESMA-only pool: 95980020140005012943N201301\n",
      "[2025-12-10 21:57:55]   ESMA rows: 7133\n",
      "[2025-12-10 21:57:54]   Saved 98,100 rows in 2.6s\n",
      "[2025-12-10 21:57:54] \n",
      "[152/180] Processing ESMA-only pool: 95980020140005012943N201301\n",
      "[2025-12-10 21:57:55]   ESMA rows: 7133\n",
      "[2025-12-10 21:57:55]   Saved 7,133 rows in 0.3s\n",
      "[2025-12-10 21:57:55] \n",
      "[153/180] Processing ESMA-only pool: 724500YBGF8VIHSMLV45N201801\n",
      "[2025-12-10 21:57:55]   Saved 7,133 rows in 0.3s\n",
      "[2025-12-10 21:57:55] \n",
      "[153/180] Processing ESMA-only pool: 724500YBGF8VIHSMLV45N201801\n",
      "[2025-12-10 21:57:56]   ESMA rows: 76538\n",
      "[2025-12-10 21:57:56]   ESMA rows: 76538\n",
      "[2025-12-10 21:57:58]   Saved 76,538 rows in 2.8s\n",
      "[2025-12-10 21:57:58] \n",
      "[154/180] Processing ESMA-only pool: 969500LXTMN8327PHD94N202501\n",
      "[2025-12-10 21:57:58]   Saved 76,538 rows in 2.8s\n",
      "[2025-12-10 21:57:58] \n",
      "[154/180] Processing ESMA-only pool: 969500LXTMN8327PHD94N202501\n",
      "[2025-12-10 21:57:58]   ESMA rows: 49565\n",
      "[2025-12-10 21:57:58]   ESMA rows: 49565\n",
      "[2025-12-10 21:57:59]   Saved 49,565 rows in 1.5s\n",
      "[2025-12-10 21:57:59] \n",
      "[155/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N200901\n",
      "[2025-12-10 21:57:59]   Saved 49,565 rows in 1.5s\n",
      "[2025-12-10 21:57:59] \n",
      "[155/180] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N200901\n",
      "[2025-12-10 21:57:59]   ESMA rows: 19765\n",
      "[2025-12-10 21:57:59]   ESMA rows: 19765\n",
      "[2025-12-10 21:58:00]   Saved 19,765 rows in 0.6s\n",
      "[2025-12-10 21:58:00] \n",
      "[156/180] Processing ESMA-only pool: 95980020140005209368N202201\n",
      "[2025-12-10 21:58:00]   Saved 19,765 rows in 0.6s\n",
      "[2025-12-10 21:58:00] \n",
      "[156/180] Processing ESMA-only pool: 95980020140005209368N202201\n",
      "[2025-12-10 21:58:01]   ESMA rows: 89665\n",
      "[2025-12-10 21:58:01]   ESMA rows: 89665\n",
      "[2025-12-10 21:58:03]   Saved 89,665 rows in 3.1s\n",
      "[2025-12-10 21:58:03] \n",
      "[157/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N201403\n",
      "[2025-12-10 21:58:03]   Saved 89,665 rows in 3.1s\n",
      "[2025-12-10 21:58:03] \n",
      "[157/180] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N201403\n",
      "[2025-12-10 21:58:03]   ESMA rows: 44444\n",
      "[2025-12-10 21:58:03]   ESMA rows: 44444\n",
      "[2025-12-10 21:58:04]   Saved 44,444 rows in 1.3s\n",
      "[2025-12-10 21:58:04] \n",
      "[158/180] Processing ESMA-only pool: 5299008I0TO44SUINZ71N202501\n",
      "[2025-12-10 21:58:04]   ESMA rows: 9013\n",
      "[2025-12-10 21:58:04]   Saved 44,444 rows in 1.3s\n",
      "[2025-12-10 21:58:04] \n",
      "[158/180] Processing ESMA-only pool: 5299008I0TO44SUINZ71N202501\n",
      "[2025-12-10 21:58:04]   ESMA rows: 9013\n",
      "[2025-12-10 21:58:05]   Saved 9,013 rows in 0.4s\n",
      "[2025-12-10 21:58:05] \n",
      "[159/180] Processing ESMA-only pool: 549300UE2ZF21B02QG64N201801\n",
      "[2025-12-10 21:58:05]   Saved 9,013 rows in 0.4s\n",
      "[2025-12-10 21:58:05] \n",
      "[159/180] Processing ESMA-only pool: 549300UE2ZF21B02QG64N201801\n",
      "[2025-12-10 21:58:05]   ESMA rows: 44814\n",
      "[2025-12-10 21:58:05]   ESMA rows: 44814\n",
      "[2025-12-10 21:58:06]   Saved 44,814 rows in 1.3s\n",
      "[2025-12-10 21:58:06] \n",
      "[160/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202201\n",
      "[2025-12-10 21:58:06]   Saved 44,814 rows in 1.3s\n",
      "[2025-12-10 21:58:06] \n",
      "[160/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202201\n",
      "[2025-12-10 21:58:06]   ESMA rows: 22508\n",
      "[2025-12-10 21:58:06]   ESMA rows: 22508\n",
      "[2025-12-10 21:58:07]   Saved 22,508 rows in 0.7s\n",
      "[2025-12-10 21:58:07] \n",
      "[161/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N201710\n",
      "[2025-12-10 21:58:07]   Saved 22,508 rows in 0.7s\n",
      "[2025-12-10 21:58:07] \n",
      "[161/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N201710\n",
      "[2025-12-10 21:58:07]   ESMA rows: 72672\n",
      "[2025-12-10 21:58:07]   ESMA rows: 72672\n",
      "[2025-12-10 21:58:09]   Saved 72,672 rows in 1.9s\n",
      "[2025-12-10 21:58:09] \n",
      "[162/180] Processing ESMA-only pool: 635400VOFDHJRN8DSL34N202201\n",
      "[2025-12-10 21:58:09]   Saved 72,672 rows in 1.9s\n",
      "[2025-12-10 21:58:09] \n",
      "[162/180] Processing ESMA-only pool: 635400VOFDHJRN8DSL34N202201\n",
      "[2025-12-10 21:58:09]   ESMA rows: 22438\n",
      "[2025-12-10 21:58:09]   ESMA rows: 22438\n",
      "[2025-12-10 21:58:09]   Saved 22,438 rows in 0.8s\n",
      "[2025-12-10 21:58:09] \n",
      "[163/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N201601\n",
      "[2025-12-10 21:58:09]   Saved 22,438 rows in 0.8s\n",
      "[2025-12-10 21:58:09] \n",
      "[163/180] Processing ESMA-only pool: 635400CE9HHFB55PEY43N201601\n",
      "[2025-12-10 21:58:10]   ESMA rows: 67463\n",
      "[2025-12-10 21:58:10]   ESMA rows: 67463\n",
      "[2025-12-10 21:58:11]   Saved 67,463 rows in 2.1s\n",
      "[2025-12-10 21:58:11] \n",
      "[164/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N202201\n",
      "[2025-12-10 21:58:11]   Saved 67,463 rows in 2.1s\n",
      "[2025-12-10 21:58:11] \n",
      "[164/180] Processing ESMA-only pool: 724500VZ11H30K1D6902N202201\n",
      "[2025-12-10 21:58:12]   ESMA rows: 79109\n",
      "[2025-12-10 21:58:12]   ESMA rows: 79109\n",
      "[2025-12-10 21:58:14]   Saved 79,109 rows in 2.6s\n",
      "[2025-12-10 21:58:14] \n",
      "[165/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N201901\n",
      "[2025-12-10 21:58:14]   Saved 79,109 rows in 2.6s\n",
      "[2025-12-10 21:58:14] \n",
      "[165/180] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N201901\n",
      "[2025-12-10 21:58:15]   ESMA rows: 80959\n",
      "[2025-12-10 21:58:15]   ESMA rows: 80959\n",
      "[2025-12-10 21:58:17]   Saved 80,959 rows in 2.7s\n",
      "[2025-12-10 21:58:17] \n",
      "[166/180] Processing ESMA-only pool: 8156005A8E679C960363N202301\n",
      "[2025-12-10 21:58:17]   ESMA rows: 11528\n",
      "[2025-12-10 21:58:17]   Saved 80,959 rows in 2.7s\n",
      "[2025-12-10 21:58:17] \n",
      "[166/180] Processing ESMA-only pool: 8156005A8E679C960363N202301\n",
      "[2025-12-10 21:58:17]   ESMA rows: 11528\n",
      "[2025-12-10 21:58:17]   Saved 11,528 rows in 0.5s\n",
      "[2025-12-10 21:58:17] \n",
      "[167/180] Processing ESMA-only pool: 635400ZYEGNCE4TCV776N202301\n",
      "[2025-12-10 21:58:17]   ESMA rows: 8936\n",
      "[2025-12-10 21:58:17]   Saved 11,528 rows in 0.5s\n",
      "[2025-12-10 21:58:17] \n",
      "[167/180] Processing ESMA-only pool: 635400ZYEGNCE4TCV776N202301\n",
      "[2025-12-10 21:58:17]   ESMA rows: 8936\n",
      "[2025-12-10 21:58:18]   Saved 8,936 rows in 0.4s\n",
      "[2025-12-10 21:58:18] \n",
      "[168/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200505\n",
      "[2025-12-10 21:58:18]   Saved 8,936 rows in 0.4s\n",
      "[2025-12-10 21:58:18] \n",
      "[168/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200505\n",
      "[2025-12-10 21:58:18]   ESMA rows: 30248\n",
      "[2025-12-10 21:58:18]   ESMA rows: 30248\n",
      "[2025-12-10 21:58:19]   Saved 30,248 rows in 0.9s\n",
      "[2025-12-10 21:58:19] \n",
      "[169/180] Processing ESMA-only pool: 95980020140005209368N202301\n",
      "[2025-12-10 21:58:19]   Saved 30,248 rows in 0.9s\n",
      "[2025-12-10 21:58:19] \n",
      "[169/180] Processing ESMA-only pool: 95980020140005209368N202301\n",
      "[2025-12-10 21:58:19]   ESMA rows: 51212\n",
      "[2025-12-10 21:58:19]   ESMA rows: 51212\n",
      "[2025-12-10 21:58:20]   Saved 51,212 rows in 1.8s\n",
      "[2025-12-10 21:58:20] \n",
      "[170/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202001\n",
      "[2025-12-10 21:58:20]   Saved 51,212 rows in 1.8s\n",
      "[2025-12-10 21:58:20] \n",
      "[170/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202001\n",
      "[2025-12-10 21:58:21]   ESMA rows: 57967\n",
      "[2025-12-10 21:58:21]   ESMA rows: 57967\n",
      "[2025-12-10 21:58:22]   Saved 57,967 rows in 2.0s\n",
      "[2025-12-10 21:58:22] \n",
      "[171/180] Processing ESMA-only pool: 724500BJL5Q2QAAIO690N202401\n",
      "[2025-12-10 21:58:23]   ESMA rows: 12436\n",
      "[2025-12-10 21:58:22]   Saved 57,967 rows in 2.0s\n",
      "[2025-12-10 21:58:22] \n",
      "[171/180] Processing ESMA-only pool: 724500BJL5Q2QAAIO690N202401\n",
      "[2025-12-10 21:58:23]   ESMA rows: 12436\n",
      "[2025-12-10 21:58:23]   Saved 12,436 rows in 0.5s\n",
      "[2025-12-10 21:58:23] \n",
      "[172/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202001\n",
      "[2025-12-10 21:58:23]   Saved 12,436 rows in 0.5s\n",
      "[2025-12-10 21:58:23] \n",
      "[172/180] Processing ESMA-only pool: 724500YGUK3ZCLGPE353N202001\n",
      "[2025-12-10 21:58:23]   ESMA rows: 39162\n",
      "[2025-12-10 21:58:23]   ESMA rows: 39162\n",
      "[2025-12-10 21:58:24]   Saved 39,162 rows in 1.2s\n",
      "[2025-12-10 21:58:24] \n",
      "[173/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N202201\n",
      "[2025-12-10 21:58:24]   Saved 39,162 rows in 1.2s\n",
      "[2025-12-10 21:58:24] \n",
      "[173/180] Processing ESMA-only pool: 549300YIM8RX3KHGIU35N202201\n",
      "[2025-12-10 21:58:24]   ESMA rows: 18774\n",
      "[2025-12-10 21:58:24]   ESMA rows: 18774\n",
      "[2025-12-10 21:58:25]   Saved 18,774 rows in 0.8s\n",
      "[2025-12-10 21:58:25] \n",
      "[174/180] Processing ESMA-only pool: 8IBZUGJ7JPLH368JE346N201901\n",
      "[2025-12-10 21:58:25]   Saved 18,774 rows in 0.8s\n",
      "[2025-12-10 21:58:25] \n",
      "[174/180] Processing ESMA-only pool: 8IBZUGJ7JPLH368JE346N201901\n",
      "[2025-12-10 21:58:25]   ESMA rows: 53561\n",
      "[2025-12-10 21:58:25]   ESMA rows: 53561\n",
      "[2025-12-10 21:58:27]   Saved 53,561 rows in 1.8s\n",
      "[2025-12-10 21:58:27] \n",
      "[175/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202201\n",
      "[2025-12-10 21:58:27]   Saved 53,561 rows in 1.8s\n",
      "[2025-12-10 21:58:27] \n",
      "[175/180] Processing ESMA-only pool: 529900MATKY89NT0U738N202201\n",
      "[2025-12-10 21:58:27]   ESMA rows: 64314\n",
      "[2025-12-10 21:58:27]   ESMA rows: 64314\n",
      "[2025-12-10 21:58:29]   Saved 64,314 rows in 2.2s\n",
      "[2025-12-10 21:58:29] \n",
      "[176/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200901\n",
      "[2025-12-10 21:58:29]   Saved 64,314 rows in 2.2s\n",
      "[2025-12-10 21:58:29] \n",
      "[176/180] Processing ESMA-only pool: 549300OLBL49CW8CT155N200901\n",
      "[2025-12-10 21:58:29]   ESMA rows: 64430\n",
      "[2025-12-10 21:58:29]   ESMA rows: 64430\n",
      "[2025-12-10 21:58:31]   Saved 64,430 rows in 1.8s\n",
      "[2025-12-10 21:58:31] \n",
      "[177/180] Processing ESMA-only pool: 95980020140005012943N201801\n",
      "[2025-12-10 21:58:31]   Saved 64,430 rows in 1.8s\n",
      "[2025-12-10 21:58:31] \n",
      "[177/180] Processing ESMA-only pool: 95980020140005012943N201801\n",
      "[2025-12-10 21:58:31]   ESMA rows: 19358\n",
      "[2025-12-10 21:58:31]   ESMA rows: 19358\n",
      "[2025-12-10 21:58:31]   Saved 19,358 rows in 0.7s\n",
      "[2025-12-10 21:58:31] \n",
      "[178/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202301\n",
      "[2025-12-10 21:58:31]   ESMA rows: 13307\n",
      "[2025-12-10 21:58:31]   Saved 19,358 rows in 0.7s\n",
      "[2025-12-10 21:58:31] \n",
      "[178/180] Processing ESMA-only pool: 72450018LF4K4XBY1B12N202301\n",
      "[2025-12-10 21:58:31]   ESMA rows: 13307\n",
      "[2025-12-10 21:58:32]   Saved 13,307 rows in 0.5s\n",
      "[2025-12-10 21:58:32] \n",
      "[179/180] Processing ESMA-only pool: 2138001WO21Z3B8Y8K20N202401\n",
      "[2025-12-10 21:58:32]   Saved 13,307 rows in 0.5s\n",
      "[2025-12-10 21:58:32] \n",
      "[179/180] Processing ESMA-only pool: 2138001WO21Z3B8Y8K20N202401\n",
      "[2025-12-10 21:58:32]   ESMA rows: 17760\n",
      "[2025-12-10 21:58:32]   ESMA rows: 17760\n",
      "[2025-12-10 21:58:32]   Saved 17,760 rows in 0.7s\n",
      "[2025-12-10 21:58:32] \n",
      "[180/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200608\n",
      "[2025-12-10 21:58:33]   ESMA rows: 12291\n",
      "[2025-12-10 21:58:32]   Saved 17,760 rows in 0.7s\n",
      "[2025-12-10 21:58:32] \n",
      "[180/180] Processing ESMA-only pool: SI5RG2M0WQQLZCXKRM20N200608\n",
      "[2025-12-10 21:58:33]   ESMA rows: 12291\n",
      "[2025-12-10 21:58:33]   Saved 12,291 rows in 0.4s\n",
      "[2025-12-10 21:58:33] \n",
      "[LARGE 1/66] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202102 (417 MB)\n",
      "[2025-12-10 21:58:33]   Scanning 51 files for column schema...\n",
      "[2025-12-10 21:58:33]   Saved 12,291 rows in 0.4s\n",
      "[2025-12-10 21:58:33] \n",
      "[LARGE 1/66] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202102 (417 MB)\n",
      "[2025-12-10 21:58:33]   Scanning 51 files for column schema...\n",
      "[2025-12-10 21:58:40]   Found 121 columns\n",
      "[2025-12-10 21:58:40]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 21:58:40]   Found 121 columns\n",
      "[2025-12-10 21:58:40]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 21:58:46]     Processed 20/51 files, 181,214 rows so far\n",
      "[2025-12-10 21:58:46]     Processed 20/51 files, 181,214 rows so far\n",
      "[2025-12-10 21:58:52]     Processed 40/51 files, 363,637 rows so far\n",
      "[2025-12-10 21:58:52]     Processed 40/51 files, 363,637 rows so far\n",
      "[2025-12-10 21:58:55]   Saved 463,933 rows in 22.4s (chunked)\n",
      "[2025-12-10 21:58:55] \n",
      "[LARGE 2/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202102 (287 MB)\n",
      "[2025-12-10 21:58:55]   Scanning 17 files for column schema...\n",
      "[2025-12-10 21:58:55]   Saved 463,933 rows in 22.4s (chunked)\n",
      "[2025-12-10 21:58:55] \n",
      "[LARGE 2/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202102 (287 MB)\n",
      "[2025-12-10 21:58:55]   Scanning 17 files for column schema...\n",
      "[2025-12-10 21:59:00]   Found 122 columns\n",
      "[2025-12-10 21:59:00]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 21:59:00]   Found 122 columns\n",
      "[2025-12-10 21:59:00]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 21:59:10]   Saved 374,997 rows in 14.9s (chunked)\n",
      "[2025-12-10 21:59:10] \n",
      "[LARGE 3/66] Processing ESMA-only pool: 8156004B244AA70DE787N201400 (387 MB)\n",
      "[2025-12-10 21:59:10]   Scanning 18 files for column schema...\n",
      "[2025-12-10 21:59:10]   Saved 374,997 rows in 14.9s (chunked)\n",
      "[2025-12-10 21:59:10] \n",
      "[LARGE 3/66] Processing ESMA-only pool: 8156004B244AA70DE787N201400 (387 MB)\n",
      "[2025-12-10 21:59:10]   Scanning 18 files for column schema...\n",
      "[2025-12-10 21:59:17]   Found 119 columns\n",
      "[2025-12-10 21:59:17]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 21:59:17]   Found 119 columns\n",
      "[2025-12-10 21:59:17]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 21:59:31]   Saved 490,276 rows in 21.1s (chunked)\n",
      "[2025-12-10 21:59:31] \n",
      "[LARGE 4/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202201 (1733 MB)\n",
      "[2025-12-10 21:59:31]   Scanning 15 files for column schema...\n",
      "[2025-12-10 21:59:31]   Saved 490,276 rows in 21.1s (chunked)\n",
      "[2025-12-10 21:59:31] \n",
      "[LARGE 4/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202201 (1733 MB)\n",
      "[2025-12-10 21:59:31]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:00:05]   Found 121 columns\n",
      "[2025-12-10 22:00:05]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:00:05]   Found 121 columns\n",
      "[2025-12-10 22:00:05]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:01:11]   Saved 2,265,834 rows in 99.3s (chunked)\n",
      "[2025-12-10 22:01:11] \n",
      "[LARGE 5/66] Processing ESMA-only pool: 851WYGNLUQLFZBSYGB56N202201 (6487 MB)\n",
      "[2025-12-10 22:01:11]   Scanning 41 files for column schema...\n",
      "[2025-12-10 22:01:11]   Saved 2,265,834 rows in 99.3s (chunked)\n",
      "[2025-12-10 22:01:11] \n",
      "[LARGE 5/66] Processing ESMA-only pool: 851WYGNLUQLFZBSYGB56N202201 (6487 MB)\n",
      "[2025-12-10 22:01:11]   Scanning 41 files for column schema...\n",
      "[2025-12-10 22:03:24]   Found 120 columns\n",
      "[2025-12-10 22:03:24]   Processing 41 files in chunked mode...\n",
      "[2025-12-10 22:03:24]   Found 120 columns\n",
      "[2025-12-10 22:03:24]   Processing 41 files in chunked mode...\n",
      "[2025-12-10 22:05:21]     Processed 20/41 files, 3,639,670 rows so far\n",
      "[2025-12-10 22:05:21]     Processed 20/41 files, 3,639,670 rows so far\n",
      "[2025-12-10 22:07:21]     Processed 40/41 files, 7,396,279 rows so far\n",
      "[2025-12-10 22:07:21]     Processed 40/41 files, 7,396,279 rows so far\n",
      "[2025-12-10 22:07:28]   Saved 7,587,853 rows in 376.8s (chunked)\n",
      "[2025-12-10 22:07:28] \n",
      "[LARGE 6/66] Processing ESMA-only pool: 959800R8CQLEK6JWF651N201704 (208 MB)\n",
      "[2025-12-10 22:07:28]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:07:28]   Saved 7,587,853 rows in 376.8s (chunked)\n",
      "[2025-12-10 22:07:28] \n",
      "[LARGE 6/66] Processing ESMA-only pool: 959800R8CQLEK6JWF651N201704 (208 MB)\n",
      "[2025-12-10 22:07:28]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:07:32]   Found 125 columns\n",
      "[2025-12-10 22:07:32]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:07:32]   Found 125 columns\n",
      "[2025-12-10 22:07:32]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:07:40]   Saved 294,948 rows in 12.3s (chunked)\n",
      "[2025-12-10 22:07:40] \n",
      "[LARGE 7/66] Processing ESMA-only pool: 8156006C66E55C898594N201901 (172 MB)\n",
      "[2025-12-10 22:07:40]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:07:40]   Saved 294,948 rows in 12.3s (chunked)\n",
      "[2025-12-10 22:07:40] \n",
      "[LARGE 7/66] Processing ESMA-only pool: 8156006C66E55C898594N201901 (172 MB)\n",
      "[2025-12-10 22:07:40]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:07:43]   Found 121 columns\n",
      "[2025-12-10 22:07:43]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:07:43]   Found 121 columns\n",
      "[2025-12-10 22:07:43]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:07:49]   Saved 198,138 rows in 9.4s (chunked)\n",
      "[2025-12-10 22:07:49] \n",
      "[LARGE 8/66] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N201601 (135 MB)\n",
      "[2025-12-10 22:07:49]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:07:49]   Saved 198,138 rows in 9.4s (chunked)\n",
      "[2025-12-10 22:07:49] \n",
      "[LARGE 8/66] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N201601 (135 MB)\n",
      "[2025-12-10 22:07:49]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:07:52]   Found 126 columns\n",
      "[2025-12-10 22:07:52]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:07:52]   Found 126 columns\n",
      "[2025-12-10 22:07:52]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:07:57]   Saved 150,457 rows in 7.6s (chunked)\n",
      "[2025-12-10 22:07:57] \n",
      "[LARGE 9/66] Processing ESMA-only pool: 549300XY136B5UBOKZ47N201401 (457 MB)\n",
      "[2025-12-10 22:07:57]   Scanning 9 files for column schema...\n",
      "[2025-12-10 22:07:57]   Saved 150,457 rows in 7.6s (chunked)\n",
      "[2025-12-10 22:07:57] \n",
      "[LARGE 9/66] Processing ESMA-only pool: 549300XY136B5UBOKZ47N201401 (457 MB)\n",
      "[2025-12-10 22:07:57]   Scanning 9 files for column schema...\n",
      "[2025-12-10 22:08:06]   Found 123 columns\n",
      "[2025-12-10 22:08:06]   Processing 9 files in chunked mode...\n",
      "[2025-12-10 22:08:06]   Found 123 columns\n",
      "[2025-12-10 22:08:06]   Processing 9 files in chunked mode...\n",
      "[2025-12-10 22:08:24]   Saved 636,370 rows in 26.7s (chunked)\n",
      "[2025-12-10 22:08:24] \n",
      "[LARGE 10/66] Processing ESMA-only pool: 81560027D07F9BDB8436N202101 (171 MB)\n",
      "[2025-12-10 22:08:24]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:08:24]   Saved 636,370 rows in 26.7s (chunked)\n",
      "[2025-12-10 22:08:24] \n",
      "[LARGE 10/66] Processing ESMA-only pool: 81560027D07F9BDB8436N202101 (171 MB)\n",
      "[2025-12-10 22:08:24]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:08:26]   Found 121 columns\n",
      "[2025-12-10 22:08:26]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:08:26]   Found 121 columns\n",
      "[2025-12-10 22:08:26]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:08:33]   Saved 194,959 rows in 9.1s (chunked)\n",
      "[2025-12-10 22:08:33] \n",
      "[LARGE 11/66] Processing ESMA-only pool: 2ZCNRR8UK83OBTEK2170N200801 (1796 MB)\n",
      "[2025-12-10 22:08:33]   Scanning 51 files for column schema...\n",
      "[2025-12-10 22:08:33]   Saved 194,959 rows in 9.1s (chunked)\n",
      "[2025-12-10 22:08:33] \n",
      "[LARGE 11/66] Processing ESMA-only pool: 2ZCNRR8UK83OBTEK2170N200801 (1796 MB)\n",
      "[2025-12-10 22:08:33]   Scanning 51 files for column schema...\n",
      "[2025-12-10 22:09:03]   Found 118 columns\n",
      "[2025-12-10 22:09:03]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 22:09:03]   Found 118 columns\n",
      "[2025-12-10 22:09:03]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 22:09:29]     Processed 20/51 files, 973,954 rows so far\n",
      "[2025-12-10 22:09:29]     Processed 20/51 files, 973,954 rows so far\n",
      "[2025-12-10 22:09:53]     Processed 40/51 files, 1,880,303 rows so far\n",
      "[2025-12-10 22:09:53]     Processed 40/51 files, 1,880,303 rows so far\n",
      "[2025-12-10 22:10:06]   Saved 2,358,532 rows in 93.5s (chunked)\n",
      "[2025-12-10 22:10:06] \n",
      "[LARGE 12/66] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N201801 (107 MB)\n",
      "[2025-12-10 22:10:06]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:10:06]   Saved 2,358,532 rows in 93.5s (chunked)\n",
      "[2025-12-10 22:10:06] \n",
      "[LARGE 12/66] Processing ESMA-only pool: 549300Y8LUK3U2T7BG03N201801 (107 MB)\n",
      "[2025-12-10 22:10:06]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:10:08]   Found 122 columns\n",
      "[2025-12-10 22:10:08]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:10:08]   Found 122 columns\n",
      "[2025-12-10 22:10:08]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:10:12]   Saved 141,053 rows in 5.7s (chunked)\n",
      "[2025-12-10 22:10:12] \n",
      "[LARGE 13/66] Processing ESMA-only pool: 969500KPK1WZO8QA4W81N202101 (174 MB)\n",
      "[2025-12-10 22:10:12]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:10:12]   Saved 141,053 rows in 5.7s (chunked)\n",
      "[2025-12-10 22:10:12] \n",
      "[LARGE 13/66] Processing ESMA-only pool: 969500KPK1WZO8QA4W81N202101 (174 MB)\n",
      "[2025-12-10 22:10:12]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:10:15]   Found 118 columns\n",
      "[2025-12-10 22:10:15]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:10:15]   Found 118 columns\n",
      "[2025-12-10 22:10:15]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:10:22]   Saved 260,168 rows in 9.7s (chunked)\n",
      "[2025-12-10 22:10:22] \n",
      "[LARGE 14/66] Processing ESMA-only pool: 969500EF6ED9WU4JFN81N202201 (143 MB)\n",
      "[2025-12-10 22:10:22]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:10:22]   Saved 260,168 rows in 9.7s (chunked)\n",
      "[2025-12-10 22:10:22] \n",
      "[LARGE 14/66] Processing ESMA-only pool: 969500EF6ED9WU4JFN81N202201 (143 MB)\n",
      "[2025-12-10 22:10:22]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:10:25]   Found 119 columns\n",
      "[2025-12-10 22:10:25]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:10:25]   Found 119 columns\n",
      "[2025-12-10 22:10:25]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:10:31]   Saved 192,763 rows in 9.2s (chunked)\n",
      "[2025-12-10 22:10:31] \n",
      "[LARGE 15/66] Processing ESMA-only pool: 3TK20IVIUJ8J3ZU0QE75N202001 (2456 MB)\n",
      "[2025-12-10 22:10:31]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:10:31]   Saved 192,763 rows in 9.2s (chunked)\n",
      "[2025-12-10 22:10:31] \n",
      "[LARGE 15/66] Processing ESMA-only pool: 3TK20IVIUJ8J3ZU0QE75N202001 (2456 MB)\n",
      "[2025-12-10 22:10:31]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:11:25]   Found 122 columns\n",
      "[2025-12-10 22:11:25]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:11:25]   Found 122 columns\n",
      "[2025-12-10 22:11:25]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:13:04]   Saved 2,827,339 rows in 153.2s (chunked)\n",
      "[2025-12-10 22:13:04] \n",
      "[LARGE 16/66] Processing ESMA-only pool: 9695003SJ7Z9C9HIGV14N202201 (142 MB)\n",
      "[2025-12-10 22:13:04]   Scanning 35 files for column schema...\n",
      "[2025-12-10 22:13:04]   Saved 2,827,339 rows in 153.2s (chunked)\n",
      "[2025-12-10 22:13:04] \n",
      "[LARGE 16/66] Processing ESMA-only pool: 9695003SJ7Z9C9HIGV14N202201 (142 MB)\n",
      "[2025-12-10 22:13:04]   Scanning 35 files for column schema...\n",
      "[2025-12-10 22:13:07]   Found 122 columns\n",
      "[2025-12-10 22:13:07]   Processing 35 files in chunked mode...\n",
      "[2025-12-10 22:13:07]   Found 122 columns\n",
      "[2025-12-10 22:13:07]   Processing 35 files in chunked mode...\n",
      "[2025-12-10 22:13:11]     Processed 20/35 files, 103,549 rows so far\n",
      "[2025-12-10 22:13:11]     Processed 20/35 files, 103,549 rows so far\n",
      "[2025-12-10 22:13:14]   Saved 207,657 rows in 10.1s (chunked)\n",
      "[2025-12-10 22:13:14] \n",
      "[LARGE 17/66] Processing ESMA-only pool: 969500MFD98SLZ425M62N202001 (150 MB)\n",
      "[2025-12-10 22:13:14]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:13:14]   Saved 207,657 rows in 10.1s (chunked)\n",
      "[2025-12-10 22:13:14] \n",
      "[LARGE 17/66] Processing ESMA-only pool: 969500MFD98SLZ425M62N202001 (150 MB)\n",
      "[2025-12-10 22:13:14]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:13:17]   Found 119 columns\n",
      "[2025-12-10 22:13:17]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:13:17]   Found 119 columns\n",
      "[2025-12-10 22:13:17]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:13:23]   Saved 202,561 rows in 9.2s (chunked)\n",
      "[2025-12-10 22:13:23] \n",
      "[LARGE 18/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202404 (295 MB)\n",
      "[2025-12-10 22:13:23]   Scanning 6 files for column schema...\n",
      "[2025-12-10 22:13:23]   Saved 202,561 rows in 9.2s (chunked)\n",
      "[2025-12-10 22:13:23] \n",
      "[LARGE 18/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202404 (295 MB)\n",
      "[2025-12-10 22:13:23]   Scanning 6 files for column schema...\n",
      "[2025-12-10 22:13:29]   Found 121 columns\n",
      "[2025-12-10 22:13:29]   Processing 6 files in chunked mode...\n",
      "[2025-12-10 22:13:29]   Found 121 columns\n",
      "[2025-12-10 22:13:29]   Processing 6 files in chunked mode...\n",
      "[2025-12-10 22:13:39]   Saved 385,606 rows in 15.9s (chunked)\n",
      "[2025-12-10 22:13:39] \n",
      "[LARGE 19/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N201902 (157 MB)\n",
      "[2025-12-10 22:13:39]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:13:39]   Saved 385,606 rows in 15.9s (chunked)\n",
      "[2025-12-10 22:13:39] \n",
      "[LARGE 19/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N201902 (157 MB)\n",
      "[2025-12-10 22:13:39]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:13:42]   Found 122 columns\n",
      "[2025-12-10 22:13:42]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:13:42]   Found 122 columns\n",
      "[2025-12-10 22:13:42]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:13:48]   Saved 209,648 rows in 8.7s (chunked)\n",
      "[2025-12-10 22:13:48] \n",
      "[LARGE 20/66] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N201701 (139 MB)\n",
      "[2025-12-10 22:13:48]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:13:48]   Saved 209,648 rows in 8.7s (chunked)\n",
      "[2025-12-10 22:13:48] \n",
      "[LARGE 20/66] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N201701 (139 MB)\n",
      "[2025-12-10 22:13:48]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:13:50]   Found 126 columns\n",
      "[2025-12-10 22:13:50]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:13:50]   Found 126 columns\n",
      "[2025-12-10 22:13:50]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:13:56]   Saved 155,447 rows in 7.8s (chunked)\n",
      "[2025-12-10 22:13:56] \n",
      "[LARGE 21/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200703 (112 MB)\n",
      "[2025-12-10 22:13:56]   Scanning 6 files for column schema...\n",
      "[2025-12-10 22:13:56]   Saved 155,447 rows in 7.8s (chunked)\n",
      "[2025-12-10 22:13:56] \n",
      "[LARGE 21/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N200703 (112 MB)\n",
      "[2025-12-10 22:13:56]   Scanning 6 files for column schema...\n",
      "[2025-12-10 22:13:58]   Found 121 columns\n",
      "[2025-12-10 22:13:58]   Processing 6 files in chunked mode...\n",
      "[2025-12-10 22:13:58]   Found 121 columns\n",
      "[2025-12-10 22:13:58]   Processing 6 files in chunked mode...\n",
      "[2025-12-10 22:14:02]   Saved 144,355 rows in 6.3s (chunked)\n",
      "[2025-12-10 22:14:02] \n",
      "[LARGE 22/66] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202101 (1191 MB)\n",
      "[2025-12-10 22:14:02]   Scanning 51 files for column schema...\n",
      "[2025-12-10 22:14:02]   Saved 144,355 rows in 6.3s (chunked)\n",
      "[2025-12-10 22:14:02] \n",
      "[LARGE 22/66] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202101 (1191 MB)\n",
      "[2025-12-10 22:14:02]   Scanning 51 files for column schema...\n",
      "[2025-12-10 22:14:23]   Found 121 columns\n",
      "[2025-12-10 22:14:23]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 22:14:23]   Found 121 columns\n",
      "[2025-12-10 22:14:23]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 22:14:38]     Processed 20/51 files, 515,149 rows so far\n",
      "[2025-12-10 22:14:38]     Processed 20/51 files, 515,149 rows so far\n",
      "[2025-12-10 22:14:53]     Processed 40/51 files, 1,030,609 rows so far\n",
      "[2025-12-10 22:14:53]     Processed 40/51 files, 1,030,609 rows so far\n",
      "[2025-12-10 22:15:02]   Saved 1,314,168 rows in 60.0s (chunked)\n",
      "[2025-12-10 22:15:02] \n",
      "[LARGE 23/66] Processing ESMA-only pool: 7LTWFZYICNSX8D621K86N202507 (852 MB)\n",
      "[2025-12-10 22:15:02]   Scanning 4 files for column schema...\n",
      "[2025-12-10 22:15:02]   Saved 1,314,168 rows in 60.0s (chunked)\n",
      "[2025-12-10 22:15:02] \n",
      "[LARGE 23/66] Processing ESMA-only pool: 7LTWFZYICNSX8D621K86N202507 (852 MB)\n",
      "[2025-12-10 22:15:02]   Scanning 4 files for column schema...\n",
      "[2025-12-10 22:15:20]   Found 120 columns\n",
      "[2025-12-10 22:15:20]   Processing 4 files in chunked mode...\n",
      "[2025-12-10 22:15:20]   Found 120 columns\n",
      "[2025-12-10 22:15:20]   Processing 4 files in chunked mode...\n",
      "[2025-12-10 22:15:52]   Saved 963,789 rows in 50.1s (chunked)\n",
      "[2025-12-10 22:15:52] \n",
      "[LARGE 24/66] Processing ESMA-only pool: 7LTWFZYICNSX8D621K86N201701 (8527 MB)\n",
      "[2025-12-10 22:15:52]   Scanning 35 files for column schema...\n",
      "[2025-12-10 22:15:52]   Saved 963,789 rows in 50.1s (chunked)\n",
      "[2025-12-10 22:15:52] \n",
      "[LARGE 24/66] Processing ESMA-only pool: 7LTWFZYICNSX8D621K86N201701 (8527 MB)\n",
      "[2025-12-10 22:15:52]   Scanning 35 files for column schema...\n",
      "[2025-12-10 22:18:58]   Found 119 columns\n",
      "[2025-12-10 22:18:58]   Processing 35 files in chunked mode...\n",
      "[2025-12-10 22:18:58]   Found 119 columns\n",
      "[2025-12-10 22:18:58]   Processing 35 files in chunked mode...\n",
      "[2025-12-10 22:22:10]     Processed 20/35 files, 5,867,781 rows so far\n",
      "[2025-12-10 22:22:10]     Processed 20/35 files, 5,867,781 rows so far\n",
      "[2025-12-10 22:24:22]   Saved 9,892,728 rows in 509.7s (chunked)\n",
      "[2025-12-10 22:24:22] \n",
      "[LARGE 25/66] Processing ESMA-only pool: 9695000NVUKTJ495CJ05N202401 (536 MB)\n",
      "[2025-12-10 22:24:22]   Scanning 12 files for column schema...\n",
      "[2025-12-10 22:24:22]   Saved 9,892,728 rows in 509.7s (chunked)\n",
      "[2025-12-10 22:24:22] \n",
      "[LARGE 25/66] Processing ESMA-only pool: 9695000NVUKTJ495CJ05N202401 (536 MB)\n",
      "[2025-12-10 22:24:22]   Scanning 12 files for column schema...\n",
      "[2025-12-10 22:24:32]   Found 121 columns\n",
      "[2025-12-10 22:24:32]   Processing 12 files in chunked mode...\n",
      "[2025-12-10 22:24:32]   Found 121 columns\n",
      "[2025-12-10 22:24:32]   Processing 12 files in chunked mode...\n",
      "[2025-12-10 22:24:51]   Saved 683,652 rows in 28.8s (chunked)\n",
      "[2025-12-10 22:24:51] \n",
      "[LARGE 26/66] Processing ESMA-only pool: 724500VZ11H30K1D6902N202001 (167 MB)\n",
      "[2025-12-10 22:24:51]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:24:51]   Saved 683,652 rows in 28.8s (chunked)\n",
      "[2025-12-10 22:24:51] \n",
      "[LARGE 26/66] Processing ESMA-only pool: 724500VZ11H30K1D6902N202001 (167 MB)\n",
      "[2025-12-10 22:24:51]   Scanning 15 files for column schema...\n",
      "[2025-12-10 22:24:53]   Found 125 columns\n",
      "[2025-12-10 22:24:53]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:24:53]   Found 125 columns\n",
      "[2025-12-10 22:24:53]   Processing 15 files in chunked mode...\n",
      "[2025-12-10 22:24:59]   Saved 189,301 rows in 8.7s (chunked)\n",
      "[2025-12-10 22:24:59] \n",
      "[LARGE 27/66] Processing ESMA-only pool: 969500JLLGA7AQZQ1L92N202001 (180 MB)\n",
      "[2025-12-10 22:24:59]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:24:59]   Saved 189,301 rows in 8.7s (chunked)\n",
      "[2025-12-10 22:24:59] \n",
      "[LARGE 27/66] Processing ESMA-only pool: 969500JLLGA7AQZQ1L92N202001 (180 MB)\n",
      "[2025-12-10 22:24:59]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:25:03]   Found 119 columns\n",
      "[2025-12-10 22:25:03]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:25:03]   Found 119 columns\n",
      "[2025-12-10 22:25:03]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:25:10]   Saved 263,736 rows in 10.6s (chunked)\n",
      "[2025-12-10 22:25:10] \n",
      "[LARGE 28/66] Processing ESMA-only pool: 635400DJAT97IZDCJN35N202002 (903 MB)\n",
      "[2025-12-10 22:25:10]   Scanning 51 files for column schema...\n",
      "[2025-12-10 22:25:10]   Saved 263,736 rows in 10.6s (chunked)\n",
      "[2025-12-10 22:25:10] \n",
      "[LARGE 28/66] Processing ESMA-only pool: 635400DJAT97IZDCJN35N202002 (903 MB)\n",
      "[2025-12-10 22:25:10]   Scanning 51 files for column schema...\n",
      "[2025-12-10 22:25:28]   Found 118 columns\n",
      "[2025-12-10 22:25:28]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 22:25:28]   Found 118 columns\n",
      "[2025-12-10 22:25:28]   Processing 51 files in chunked mode...\n",
      "[2025-12-10 22:25:43]     Processed 20/51 files, 506,260 rows so far\n",
      "[2025-12-10 22:25:43]     Processed 20/51 files, 506,260 rows so far\n",
      "[2025-12-10 22:25:57]     Processed 40/51 files, 1,012,520 rows so far\n",
      "[2025-12-10 22:25:57]     Processed 40/51 files, 1,012,520 rows so far\n",
      "[2025-12-10 22:26:06]   Saved 1,290,963 rows in 55.9s (chunked)\n",
      "[2025-12-10 22:26:06] \n",
      "[LARGE 29/66] Processing ESMA-only pool: 724500O4GUVTGSZEU248N202302 (141 MB)\n",
      "[2025-12-10 22:26:06]   Scanning 9 files for column schema...\n",
      "[2025-12-10 22:26:06]   Saved 1,290,963 rows in 55.9s (chunked)\n",
      "[2025-12-10 22:26:06] \n",
      "[LARGE 29/66] Processing ESMA-only pool: 724500O4GUVTGSZEU248N202302 (141 MB)\n",
      "[2025-12-10 22:26:06]   Scanning 9 files for column schema...\n",
      "[2025-12-10 22:26:08]   Found 125 columns\n",
      "[2025-12-10 22:26:08]   Processing 9 files in chunked mode...\n",
      "[2025-12-10 22:26:08]   Found 125 columns\n",
      "[2025-12-10 22:26:08]   Processing 9 files in chunked mode...\n",
      "[2025-12-10 22:26:13]   Saved 185,633 rows in 7.6s (chunked)\n",
      "[2025-12-10 22:26:13] \n",
      "[LARGE 30/66] Processing ESMA-only pool: 3KXUNHVVQFIJN6RHLO76N202101 (1168 MB)\n",
      "[2025-12-10 22:26:13]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:26:13]   Saved 185,633 rows in 7.6s (chunked)\n",
      "[2025-12-10 22:26:13] \n",
      "[LARGE 30/66] Processing ESMA-only pool: 3KXUNHVVQFIJN6RHLO76N202101 (1168 MB)\n",
      "[2025-12-10 22:26:13]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:26:37]   Found 123 columns\n",
      "[2025-12-10 22:26:37]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:26:37]   Found 123 columns\n",
      "[2025-12-10 22:26:37]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:27:24]   Saved 1,719,448 rows in 70.5s (chunked)\n",
      "[2025-12-10 22:27:24] \n",
      "[LARGE 31/66] Processing ESMA-only pool: 969500651HMG0IO6S894N201901 (121 MB)\n",
      "[2025-12-10 22:27:24]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:27:24]   Saved 1,719,448 rows in 70.5s (chunked)\n",
      "[2025-12-10 22:27:24] \n",
      "[LARGE 31/66] Processing ESMA-only pool: 969500651HMG0IO6S894N201901 (121 MB)\n",
      "[2025-12-10 22:27:24]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:27:26]   Found 119 columns\n",
      "[2025-12-10 22:27:26]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:27:26]   Found 119 columns\n",
      "[2025-12-10 22:27:26]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:27:31]   Saved 177,968 rows in 7.0s (chunked)\n",
      "[2025-12-10 22:27:31] \n",
      "[LARGE 32/66] Processing ESMA-only pool: 2W8N8UU78PMDQKZENC08N201901 (542 MB)\n",
      "[2025-12-10 22:27:31]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:27:31]   Saved 177,968 rows in 7.0s (chunked)\n",
      "[2025-12-10 22:27:31] \n",
      "[LARGE 32/66] Processing ESMA-only pool: 2W8N8UU78PMDQKZENC08N201901 (542 MB)\n",
      "[2025-12-10 22:27:31]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:27:41]   Found 121 columns\n",
      "[2025-12-10 22:27:41]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:27:41]   Found 121 columns\n",
      "[2025-12-10 22:27:41]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:28:01]   Saved 798,036 rows in 30.6s (chunked)\n",
      "[2025-12-10 22:28:01] \n",
      "[LARGE 33/66] Processing ESMA-only pool: 959800R8CQLEK6JWF651N201701 (108 MB)\n",
      "[2025-12-10 22:28:01]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:28:01]   Saved 798,036 rows in 30.6s (chunked)\n",
      "[2025-12-10 22:28:01] \n",
      "[LARGE 33/66] Processing ESMA-only pool: 959800R8CQLEK6JWF651N201701 (108 MB)\n",
      "[2025-12-10 22:28:01]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:28:04]   Found 125 columns\n",
      "[2025-12-10 22:28:04]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:28:04]   Found 125 columns\n",
      "[2025-12-10 22:28:04]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:28:08]   Saved 152,342 rows in 6.7s (chunked)\n",
      "[2025-12-10 22:28:08] \n",
      "[LARGE 34/66] Processing ESMA-only pool: DG3RU1DBUFHT4ZF9WN62N201001 (2366 MB)\n",
      "[2025-12-10 22:28:08]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:28:08]   Saved 152,342 rows in 6.7s (chunked)\n",
      "[2025-12-10 22:28:08] \n",
      "[LARGE 34/66] Processing ESMA-only pool: DG3RU1DBUFHT4ZF9WN62N201001 (2366 MB)\n",
      "[2025-12-10 22:28:08]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:28:56]   Found 122 columns\n",
      "[2025-12-10 22:28:56]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:28:56]   Found 122 columns\n",
      "[2025-12-10 22:28:56]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:30:21]   Saved 2,460,253 rows in 133.0s (chunked)\n",
      "[2025-12-10 22:30:21] \n",
      "[LARGE 35/66] Processing ESMA-only pool: A6NZLYKYN1UV7VVGFX65N201901 (101 MB)\n",
      "[2025-12-10 22:30:21]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:30:21]   Saved 2,460,253 rows in 133.0s (chunked)\n",
      "[2025-12-10 22:30:21] \n",
      "[LARGE 35/66] Processing ESMA-only pool: A6NZLYKYN1UV7VVGFX65N201901 (101 MB)\n",
      "[2025-12-10 22:30:21]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:30:23]   Found 126 columns\n",
      "[2025-12-10 22:30:23]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:30:23]   Found 126 columns\n",
      "[2025-12-10 22:30:23]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:30:27]   Saved 104,456 rows in 5.9s (chunked)\n",
      "[2025-12-10 22:30:27] \n",
      "[LARGE 36/66] Processing ESMA-only pool: 724500VZ11H30K1D6902N201805 (180 MB)\n",
      "[2025-12-10 22:30:27]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:30:27]   Saved 104,456 rows in 5.9s (chunked)\n",
      "[2025-12-10 22:30:27] \n",
      "[LARGE 36/66] Processing ESMA-only pool: 724500VZ11H30K1D6902N201805 (180 MB)\n",
      "[2025-12-10 22:30:27]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:30:30]   Found 125 columns\n",
      "[2025-12-10 22:30:30]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:30:30]   Found 125 columns\n",
      "[2025-12-10 22:30:30]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:30:37]   Saved 199,582 rows in 9.6s (chunked)\n",
      "[2025-12-10 22:30:37] \n",
      "[LARGE 37/66] Processing ESMA-only pool: 5493005TM46YMTICAU84N202201 (123 MB)\n",
      "[2025-12-10 22:30:37]   Scanning 12 files for column schema...\n",
      "[2025-12-10 22:30:37]   Saved 199,582 rows in 9.6s (chunked)\n",
      "[2025-12-10 22:30:37] \n",
      "[LARGE 37/66] Processing ESMA-only pool: 5493005TM46YMTICAU84N202201 (123 MB)\n",
      "[2025-12-10 22:30:37]   Scanning 12 files for column schema...\n",
      "[2025-12-10 22:30:39]   Found 124 columns\n",
      "[2025-12-10 22:30:39]   Processing 12 files in chunked mode...\n",
      "[2025-12-10 22:30:39]   Found 124 columns\n",
      "[2025-12-10 22:30:39]   Processing 12 files in chunked mode...\n",
      "[2025-12-10 22:30:44]   Saved 173,628 rows in 7.3s (chunked)\n",
      "[2025-12-10 22:30:44] \n",
      "[LARGE 38/66] Processing ESMA-only pool: J4CP7MHCXR8DAQMKIL78N200701 (131 MB)\n",
      "[2025-12-10 22:30:44]   Scanning 13 files for column schema...\n",
      "[2025-12-10 22:30:44]   Saved 173,628 rows in 7.3s (chunked)\n",
      "[2025-12-10 22:30:44] \n",
      "[LARGE 38/66] Processing ESMA-only pool: J4CP7MHCXR8DAQMKIL78N200701 (131 MB)\n",
      "[2025-12-10 22:30:44]   Scanning 13 files for column schema...\n",
      "[2025-12-10 22:30:46]   Found 118 columns\n",
      "[2025-12-10 22:30:46]   Processing 13 files in chunked mode...\n",
      "[2025-12-10 22:30:46]   Found 118 columns\n",
      "[2025-12-10 22:30:46]   Processing 13 files in chunked mode...\n",
      "[2025-12-10 22:30:51]   Saved 173,048 rows in 7.2s (chunked)\n",
      "[2025-12-10 22:30:51] \n",
      "[LARGE 39/66] Processing ESMA-only pool: 549300772D1G8JPIUR96N202001 (237 MB)\n",
      "[2025-12-10 22:30:51]   Scanning 16 files for column schema...\n",
      "[2025-12-10 22:30:51]   Saved 173,048 rows in 7.2s (chunked)\n",
      "[2025-12-10 22:30:51] \n",
      "[LARGE 39/66] Processing ESMA-only pool: 549300772D1G8JPIUR96N202001 (237 MB)\n",
      "[2025-12-10 22:30:51]   Scanning 16 files for column schema...\n",
      "[2025-12-10 22:30:55]   Found 125 columns\n",
      "[2025-12-10 22:30:55]   Processing 16 files in chunked mode...\n",
      "[2025-12-10 22:30:55]   Found 125 columns\n",
      "[2025-12-10 22:30:55]   Processing 16 files in chunked mode...\n",
      "[2025-12-10 22:31:03]   Saved 245,154 rows in 11.7s (chunked)\n",
      "[2025-12-10 22:31:03] \n",
      "[LARGE 40/66] Processing ESMA-only pool: 969500XG6BPNADMZBB17N201901 (1974 MB)\n",
      "[2025-12-10 22:31:03]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:31:03]   Saved 245,154 rows in 11.7s (chunked)\n",
      "[2025-12-10 22:31:03] \n",
      "[LARGE 40/66] Processing ESMA-only pool: 969500XG6BPNADMZBB17N201901 (1974 MB)\n",
      "[2025-12-10 22:31:03]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:31:52]   Found 119 columns\n",
      "[2025-12-10 22:31:52]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:31:52]   Found 119 columns\n",
      "[2025-12-10 22:31:52]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:33:13]   Saved 2,708,929 rows in 130.2s (chunked)\n",
      "[2025-12-10 22:33:13] \n",
      "[LARGE 41/66] Processing ESMA-only pool: 724500VZ11H30K1D6902N201901 (242 MB)\n",
      "[2025-12-10 22:33:13]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:33:13]   Saved 2,708,929 rows in 130.2s (chunked)\n",
      "[2025-12-10 22:33:13] \n",
      "[LARGE 41/66] Processing ESMA-only pool: 724500VZ11H30K1D6902N201901 (242 MB)\n",
      "[2025-12-10 22:33:13]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:33:17]   Found 125 columns\n",
      "[2025-12-10 22:33:17]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:33:17]   Found 125 columns\n",
      "[2025-12-10 22:33:17]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:33:25]   Saved 274,509 rows in 12.2s (chunked)\n",
      "[2025-12-10 22:33:25] \n",
      "[LARGE 42/66] Processing ESMA-only pool: 213800WQJJDCAN4BCO57N202001 (133 MB)\n",
      "[2025-12-10 22:33:25]   Scanning 26 files for column schema...\n",
      "[2025-12-10 22:33:25]   Saved 274,509 rows in 12.2s (chunked)\n",
      "[2025-12-10 22:33:25] \n",
      "[LARGE 42/66] Processing ESMA-only pool: 213800WQJJDCAN4BCO57N202001 (133 MB)\n",
      "[2025-12-10 22:33:25]   Scanning 26 files for column schema...\n",
      "[2025-12-10 22:33:28]   Found 192 columns\n",
      "[2025-12-10 22:33:28]   Processing 26 files in chunked mode...\n",
      "[2025-12-10 22:33:28]   Found 192 columns\n",
      "[2025-12-10 22:33:28]   Processing 26 files in chunked mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n",
      "C:\\Users\\jonat\\AppData\\Local\\Temp\\ipykernel_19288\\3948534792.py:225: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  prepared[col] = np.nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 22:33:34]     Processed 20/26 files, 117,431 rows so far\n",
      "[2025-12-10 22:33:37]   Saved 205,991 rows in 11.7s (chunked)\n",
      "[2025-12-10 22:33:37] \n",
      "[LARGE 43/66] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202201 (654 MB)\n",
      "[2025-12-10 22:33:37]   Scanning 40 files for column schema...\n",
      "[2025-12-10 22:33:37]   Saved 205,991 rows in 11.7s (chunked)\n",
      "[2025-12-10 22:33:37] \n",
      "[LARGE 43/66] Processing ESMA-only pool: 549300WL1M55G8FIDZ68N202201 (654 MB)\n",
      "[2025-12-10 22:33:37]   Scanning 40 files for column schema...\n",
      "[2025-12-10 22:33:48]   Found 121 columns\n",
      "[2025-12-10 22:33:48]   Processing 40 files in chunked mode...\n",
      "[2025-12-10 22:33:48]   Found 121 columns\n",
      "[2025-12-10 22:33:48]   Processing 40 files in chunked mode...\n",
      "[2025-12-10 22:33:59]     Processed 20/40 files, 357,975 rows so far\n",
      "[2025-12-10 22:33:59]     Processed 20/40 files, 357,975 rows so far\n",
      "[2025-12-10 22:34:11]     Processed 40/40 files, 716,226 rows so far\n",
      "[2025-12-10 22:34:11]   Saved 716,226 rows in 33.7s (chunked)\n",
      "[2025-12-10 22:34:11] \n",
      "[LARGE 44/66] Processing ESMA-only pool: 549300MIF85K8BXC6M91N202101 (217 MB)\n",
      "[2025-12-10 22:34:11]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:34:11]     Processed 40/40 files, 716,226 rows so far\n",
      "[2025-12-10 22:34:11]   Saved 716,226 rows in 33.7s (chunked)\n",
      "[2025-12-10 22:34:11] \n",
      "[LARGE 44/66] Processing ESMA-only pool: 549300MIF85K8BXC6M91N202101 (217 MB)\n",
      "[2025-12-10 22:34:11]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:34:14]   Found 125 columns\n",
      "[2025-12-10 22:34:14]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:34:14]   Found 125 columns\n",
      "[2025-12-10 22:34:14]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:34:23]   Saved 307,972 rows in 12.6s (chunked)\n",
      "[2025-12-10 22:34:23] \n",
      "[LARGE 45/66] Processing ESMA-only pool: 5493001Y7OCB84GOJ429N202001 (103 MB)\n",
      "[2025-12-10 22:34:23]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:34:23]   Saved 307,972 rows in 12.6s (chunked)\n",
      "[2025-12-10 22:34:23] \n",
      "[LARGE 45/66] Processing ESMA-only pool: 5493001Y7OCB84GOJ429N202001 (103 MB)\n",
      "[2025-12-10 22:34:23]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:34:25]   Found 126 columns\n",
      "[2025-12-10 22:34:25]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:34:25]   Found 126 columns\n",
      "[2025-12-10 22:34:25]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:34:30]   Saved 127,409 rows in 6.8s (chunked)\n",
      "[2025-12-10 22:34:30] \n",
      "[LARGE 46/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202203 (107 MB)\n",
      "[2025-12-10 22:34:30]   Scanning 12 files for column schema...\n",
      "[2025-12-10 22:34:30]   Saved 127,409 rows in 6.8s (chunked)\n",
      "[2025-12-10 22:34:30] \n",
      "[LARGE 46/66] Processing ESMA-only pool: K8MS7FD7N5Z2WQ51AZ71N202203 (107 MB)\n",
      "[2025-12-10 22:34:30]   Scanning 12 files for column schema...\n",
      "[2025-12-10 22:34:32]   Found 121 columns\n",
      "[2025-12-10 22:34:32]   Processing 12 files in chunked mode...\n",
      "[2025-12-10 22:34:32]   Found 121 columns\n",
      "[2025-12-10 22:34:32]   Processing 12 files in chunked mode...\n",
      "[2025-12-10 22:34:37]   Saved 140,531 rows in 6.6s (chunked)\n",
      "[2025-12-10 22:34:37] \n",
      "[LARGE 47/66] Processing ESMA-only pool: 724500A1FNICHSDF2I11N202301 (2334 MB)\n",
      "[2025-12-10 22:34:37]   Scanning 30 files for column schema...\n",
      "[2025-12-10 22:34:37]   Saved 140,531 rows in 6.6s (chunked)\n",
      "[2025-12-10 22:34:37] \n",
      "[LARGE 47/66] Processing ESMA-only pool: 724500A1FNICHSDF2I11N202301 (2334 MB)\n",
      "[2025-12-10 22:34:37]   Scanning 30 files for column schema...\n",
      "[2025-12-10 22:35:12]   Found 125 columns\n",
      "[2025-12-10 22:35:12]   Processing 30 files in chunked mode...\n",
      "[2025-12-10 22:35:12]   Found 125 columns\n",
      "[2025-12-10 22:35:12]   Processing 30 files in chunked mode...\n",
      "[2025-12-10 22:36:01]     Processed 20/30 files, 1,643,601 rows so far\n",
      "[2025-12-10 22:36:01]     Processed 20/30 files, 1,643,601 rows so far\n",
      "[2025-12-10 22:36:27]   Saved 2,505,111 rows in 110.7s (chunked)\n",
      "[2025-12-10 22:36:27] \n",
      "[LARGE 48/66] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N202002 (925 MB)\n",
      "[2025-12-10 22:36:27]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:36:27]   Saved 2,505,111 rows in 110.7s (chunked)\n",
      "[2025-12-10 22:36:27] \n",
      "[LARGE 48/66] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N202002 (925 MB)\n",
      "[2025-12-10 22:36:27]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:36:43]   Found 126 columns\n",
      "[2025-12-10 22:36:43]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:36:43]   Found 126 columns\n",
      "[2025-12-10 22:36:43]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:37:17]   Saved 1,140,060 rows in 49.9s (chunked)\n",
      "[2025-12-10 22:37:17] \n",
      "[LARGE 49/66] Processing ESMA-only pool: 724500AH42V5X8BCPE49N202101 (164 MB)\n",
      "[2025-12-10 22:37:17]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:37:17]   Saved 1,140,060 rows in 49.9s (chunked)\n",
      "[2025-12-10 22:37:17] \n",
      "[LARGE 49/66] Processing ESMA-only pool: 724500AH42V5X8BCPE49N202101 (164 MB)\n",
      "[2025-12-10 22:37:17]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:37:20]   Found 126 columns\n",
      "[2025-12-10 22:37:20]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:37:20]   Found 126 columns\n",
      "[2025-12-10 22:37:20]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:37:27]   Saved 216,730 rows in 9.3s (chunked)\n",
      "[2025-12-10 22:37:27] \n",
      "[LARGE 50/66] Processing ESMA-only pool: 959800R8CQLEK6JWF651N201601 (409 MB)\n",
      "[2025-12-10 22:37:27]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:37:27]   Saved 216,730 rows in 9.3s (chunked)\n",
      "[2025-12-10 22:37:27] \n",
      "[LARGE 50/66] Processing ESMA-only pool: 959800R8CQLEK6JWF651N201601 (409 MB)\n",
      "[2025-12-10 22:37:27]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:37:36]   Found 125 columns\n",
      "[2025-12-10 22:37:36]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:37:36]   Found 125 columns\n",
      "[2025-12-10 22:37:36]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:37:53]   Saved 572,142 rows in 26.8s (chunked)\n",
      "[2025-12-10 22:37:53] \n",
      "[LARGE 51/66] Processing ESMA-only pool: JLS56RAMYQZECFUF2G44N202501 (217 MB)\n",
      "[2025-12-10 22:37:53]   Scanning 4 files for column schema...\n",
      "[2025-12-10 22:37:53]   Saved 572,142 rows in 26.8s (chunked)\n",
      "[2025-12-10 22:37:53] \n",
      "[LARGE 51/66] Processing ESMA-only pool: JLS56RAMYQZECFUF2G44N202501 (217 MB)\n",
      "[2025-12-10 22:37:53]   Scanning 4 files for column schema...\n",
      "[2025-12-10 22:37:57]   Found 122 columns\n",
      "[2025-12-10 22:37:57]   Processing 4 files in chunked mode...\n",
      "[2025-12-10 22:37:57]   Found 122 columns\n",
      "[2025-12-10 22:37:57]   Processing 4 files in chunked mode...\n",
      "[2025-12-10 22:38:04]   Saved 254,508 rows in 10.7s (chunked)\n",
      "[2025-12-10 22:38:04] \n",
      "[LARGE 52/66] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N202102 (1338 MB)\n",
      "[2025-12-10 22:38:04]   Scanning 18 files for column schema...\n",
      "[2025-12-10 22:38:04]   Saved 254,508 rows in 10.7s (chunked)\n",
      "[2025-12-10 22:38:04] \n",
      "[LARGE 52/66] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N202102 (1338 MB)\n",
      "[2025-12-10 22:38:04]   Scanning 18 files for column schema...\n",
      "[2025-12-10 22:38:28]   Found 126 columns\n",
      "[2025-12-10 22:38:28]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 22:38:28]   Found 126 columns\n",
      "[2025-12-10 22:38:28]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 22:39:19]   Saved 1,649,705 rows in 75.0s (chunked)\n",
      "[2025-12-10 22:39:19] \n",
      "[LARGE 53/66] Processing ESMA-only pool: 635400KOA4XWWG9CDC43N202301 (279 MB)\n",
      "[2025-12-10 22:39:19]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:39:19]   Saved 1,649,705 rows in 75.0s (chunked)\n",
      "[2025-12-10 22:39:19] \n",
      "[LARGE 53/66] Processing ESMA-only pool: 635400KOA4XWWG9CDC43N202301 (279 MB)\n",
      "[2025-12-10 22:39:19]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:39:24]   Found 118 columns\n",
      "[2025-12-10 22:39:24]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:39:24]   Found 118 columns\n",
      "[2025-12-10 22:39:24]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:39:35]   Saved 396,231 rows in 15.7s (chunked)\n",
      "[2025-12-10 22:39:35] \n",
      "[LARGE 54/66] Processing ESMA-only pool: 5493003SPEWN841SWG39N201901 (591 MB)\n",
      "[2025-12-10 22:39:35]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:39:35]   Saved 396,231 rows in 15.7s (chunked)\n",
      "[2025-12-10 22:39:35] \n",
      "[LARGE 54/66] Processing ESMA-only pool: 5493003SPEWN841SWG39N201901 (591 MB)\n",
      "[2025-12-10 22:39:35]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:39:44]   Found 125 columns\n",
      "[2025-12-10 22:39:44]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:39:44]   Found 125 columns\n",
      "[2025-12-10 22:39:44]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:40:03]   Saved 608,462 rows in 28.3s (chunked)\n",
      "[2025-12-10 22:40:03] \n",
      "[LARGE 55/66] Processing ESMA-only pool: 969500QV655PJVGV0544N202001 (139 MB)\n",
      "[2025-12-10 22:40:03]   Scanning 18 files for column schema...\n",
      "[2025-12-10 22:40:03]   Saved 608,462 rows in 28.3s (chunked)\n",
      "[2025-12-10 22:40:03] \n",
      "[LARGE 55/66] Processing ESMA-only pool: 969500QV655PJVGV0544N202001 (139 MB)\n",
      "[2025-12-10 22:40:03]   Scanning 18 files for column schema...\n",
      "[2025-12-10 22:40:06]   Found 119 columns\n",
      "[2025-12-10 22:40:06]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 22:40:06]   Found 119 columns\n",
      "[2025-12-10 22:40:06]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 22:40:11]   Saved 204,372 rows in 8.3s (chunked)\n",
      "[2025-12-10 22:40:11] \n",
      "[LARGE 56/66] Processing ESMA-only pool: 72450044QJ5PCRWOWA46N202007 (398 MB)\n",
      "[2025-12-10 22:40:11]   Scanning 28 files for column schema...\n",
      "[2025-12-10 22:40:11]   Saved 204,372 rows in 8.3s (chunked)\n",
      "[2025-12-10 22:40:11] \n",
      "[LARGE 56/66] Processing ESMA-only pool: 72450044QJ5PCRWOWA46N202007 (398 MB)\n",
      "[2025-12-10 22:40:11]   Scanning 28 files for column schema...\n",
      "[2025-12-10 22:40:17]   Found 121 columns\n",
      "[2025-12-10 22:40:17]   Processing 28 files in chunked mode...\n",
      "[2025-12-10 22:40:17]   Found 121 columns\n",
      "[2025-12-10 22:40:17]   Processing 28 files in chunked mode...\n",
      "[2025-12-10 22:40:29]     Processed 20/28 files, 382,662 rows so far\n",
      "[2025-12-10 22:40:29]     Processed 20/28 files, 382,662 rows so far\n",
      "[2025-12-10 22:40:33]   Saved 512,698 rows in 21.2s (chunked)\n",
      "[2025-12-10 22:40:33] \n",
      "[LARGE 57/66] Processing ESMA-only pool: 549300V6B3GYHJKZU741N201701 (823 MB)\n",
      "[2025-12-10 22:40:33]   Scanning 20 files for column schema...\n",
      "[2025-12-10 22:40:33]   Saved 512,698 rows in 21.2s (chunked)\n",
      "[2025-12-10 22:40:33] \n",
      "[LARGE 57/66] Processing ESMA-only pool: 549300V6B3GYHJKZU741N201701 (823 MB)\n",
      "[2025-12-10 22:40:33]   Scanning 20 files for column schema...\n",
      "[2025-12-10 22:40:47]   Found 126 columns\n",
      "[2025-12-10 22:40:47]   Processing 20 files in chunked mode...\n",
      "[2025-12-10 22:40:47]   Found 126 columns\n",
      "[2025-12-10 22:40:47]   Processing 20 files in chunked mode...\n",
      "[2025-12-10 22:41:17]     Processed 20/20 files, 1,029,837 rows so far\n",
      "[2025-12-10 22:41:17]   Saved 1,029,837 rows in 44.8s (chunked)\n",
      "[2025-12-10 22:41:17] \n",
      "[LARGE 58/66] Processing ESMA-only pool: O2RNE8IBXP4R0TD8PU41N202201 (1284 MB)\n",
      "[2025-12-10 22:41:17]   Scanning 20 files for column schema...\n",
      "[2025-12-10 22:41:17]     Processed 20/20 files, 1,029,837 rows so far\n",
      "[2025-12-10 22:41:17]   Saved 1,029,837 rows in 44.8s (chunked)\n",
      "[2025-12-10 22:41:17] \n",
      "[LARGE 58/66] Processing ESMA-only pool: O2RNE8IBXP4R0TD8PU41N202201 (1284 MB)\n",
      "[2025-12-10 22:41:17]   Scanning 20 files for column schema...\n",
      "[2025-12-10 22:41:43]   Found 121 columns\n",
      "[2025-12-10 22:41:43]   Processing 20 files in chunked mode...\n",
      "[2025-12-10 22:41:43]   Found 121 columns\n",
      "[2025-12-10 22:41:43]   Processing 20 files in chunked mode...\n",
      "[2025-12-10 22:42:30]     Processed 20/20 files, 1,644,440 rows so far\n",
      "[2025-12-10 22:42:30]   Saved 1,644,440 rows in 72.6s (chunked)\n",
      "[2025-12-10 22:42:30] \n",
      "[LARGE 59/66] Processing ESMA-only pool: 3TK20IVIUJ8J3ZU0QE75N202302 (665 MB)\n",
      "[2025-12-10 22:42:30]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:42:30]     Processed 20/20 files, 1,644,440 rows so far\n",
      "[2025-12-10 22:42:30]   Saved 1,644,440 rows in 72.6s (chunked)\n",
      "[2025-12-10 22:42:30] \n",
      "[LARGE 59/66] Processing ESMA-only pool: 3TK20IVIUJ8J3ZU0QE75N202302 (665 MB)\n",
      "[2025-12-10 22:42:30]   Scanning 11 files for column schema...\n",
      "[2025-12-10 22:42:42]   Found 120 columns\n",
      "[2025-12-10 22:42:42]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:42:42]   Found 120 columns\n",
      "[2025-12-10 22:42:42]   Processing 11 files in chunked mode...\n",
      "[2025-12-10 22:43:06]   Saved 778,910 rows in 35.8s (chunked)\n",
      "[2025-12-10 22:43:06] \n",
      "[LARGE 60/66] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N202002 (339 MB)\n",
      "[2025-12-10 22:43:06]   Scanning 50 files for column schema...\n",
      "[2025-12-10 22:43:06]   Saved 778,910 rows in 35.8s (chunked)\n",
      "[2025-12-10 22:43:06] \n",
      "[LARGE 60/66] Processing ESMA-only pool: B64D6Y3LBJS4ANNPCU93N202002 (339 MB)\n",
      "[2025-12-10 22:43:06]   Scanning 50 files for column schema...\n",
      "[2025-12-10 22:43:11]   Found 126 columns\n",
      "[2025-12-10 22:43:11]   Processing 50 files in chunked mode...\n",
      "[2025-12-10 22:43:11]   Found 126 columns\n",
      "[2025-12-10 22:43:11]   Processing 50 files in chunked mode...\n",
      "[2025-12-10 22:43:16]     Processed 20/50 files, 128,061 rows so far\n",
      "[2025-12-10 22:43:16]     Processed 20/50 files, 128,061 rows so far\n",
      "[2025-12-10 22:43:21]     Processed 40/50 files, 290,325 rows so far\n",
      "[2025-12-10 22:43:21]     Processed 40/50 files, 290,325 rows so far\n",
      "[2025-12-10 22:43:25]   Saved 381,246 rows in 18.7s (chunked)\n",
      "[2025-12-10 22:43:25] \n",
      "[LARGE 61/66] Processing ESMA-only pool: 635400X6AORFVBGWOY71N202401 (160 MB)\n",
      "[2025-12-10 22:43:25]   Scanning 2 files for column schema...\n",
      "[2025-12-10 22:43:25]   Saved 381,246 rows in 18.7s (chunked)\n",
      "[2025-12-10 22:43:25] \n",
      "[LARGE 61/66] Processing ESMA-only pool: 635400X6AORFVBGWOY71N202401 (160 MB)\n",
      "[2025-12-10 22:43:25]   Scanning 2 files for column schema...\n",
      "[2025-12-10 22:43:28]   Found 117 columns\n",
      "[2025-12-10 22:43:28]   Processing 2 files in chunked mode...\n",
      "[2025-12-10 22:43:28]   Found 117 columns\n",
      "[2025-12-10 22:43:28]   Processing 2 files in chunked mode...\n",
      "[2025-12-10 22:43:34]   Saved 215,050 rows in 9.7s (chunked)\n",
      "[2025-12-10 22:43:34] \n",
      "[LARGE 62/66] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N202101 (117 MB)\n",
      "[2025-12-10 22:43:34]   Scanning 18 files for column schema...\n",
      "[2025-12-10 22:43:34]   Saved 215,050 rows in 9.7s (chunked)\n",
      "[2025-12-10 22:43:34] \n",
      "[LARGE 62/66] Processing ESMA-only pool: 549300S7DH0HXAJSVI23N202101 (117 MB)\n",
      "[2025-12-10 22:43:34]   Scanning 18 files for column schema...\n",
      "[2025-12-10 22:43:36]   Found 125 columns\n",
      "[2025-12-10 22:43:36]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 22:43:36]   Found 125 columns\n",
      "[2025-12-10 22:43:36]   Processing 18 files in chunked mode...\n",
      "[2025-12-10 22:43:41]   Saved 127,490 rows in 6.5s (chunked)\n",
      "[2025-12-10 22:43:41] \n",
      "[LARGE 63/66] Processing ESMA-only pool: 815600B634BB07E05C27N201701 (422 MB)\n",
      "[2025-12-10 22:43:41]   Scanning 52 files for column schema...\n",
      "[2025-12-10 22:43:41]   Saved 127,490 rows in 6.5s (chunked)\n",
      "[2025-12-10 22:43:41] \n",
      "[LARGE 63/66] Processing ESMA-only pool: 815600B634BB07E05C27N201701 (422 MB)\n",
      "[2025-12-10 22:43:41]   Scanning 52 files for column schema...\n",
      "[2025-12-10 22:43:49]   Found 120 columns\n",
      "[2025-12-10 22:43:49]   Processing 52 files in chunked mode...\n",
      "[2025-12-10 22:43:49]   Found 120 columns\n",
      "[2025-12-10 22:43:49]   Processing 52 files in chunked mode...\n",
      "[2025-12-10 22:43:55]     Processed 20/52 files, 219,600 rows so far\n",
      "[2025-12-10 22:43:55]     Processed 20/52 files, 219,600 rows so far\n",
      "[2025-12-10 22:44:02]     Processed 40/52 files, 439,200 rows so far\n",
      "[2025-12-10 22:44:02]     Processed 40/52 files, 439,200 rows so far\n",
      "[2025-12-10 22:44:05]   Saved 532,702 rows in 24.1s (chunked)\n",
      "[2025-12-10 22:44:05] \n",
      "[LARGE 64/66] Processing ESMA-only pool: 724500KSGL8ZJTV22463N201901 (164 MB)\n",
      "[2025-12-10 22:44:05]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:44:05]   Saved 532,702 rows in 24.1s (chunked)\n",
      "[2025-12-10 22:44:05] \n",
      "[LARGE 64/66] Processing ESMA-only pool: 724500KSGL8ZJTV22463N201901 (164 MB)\n",
      "[2025-12-10 22:44:05]   Scanning 17 files for column schema...\n",
      "[2025-12-10 22:44:08]   Found 125 columns\n",
      "[2025-12-10 22:44:08]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:44:08]   Found 125 columns\n",
      "[2025-12-10 22:44:08]   Processing 17 files in chunked mode...\n",
      "[2025-12-10 22:44:14]   Saved 171,154 rows in 8.7s (chunked)\n",
      "[2025-12-10 22:44:14] \n",
      "[LARGE 65/66] Processing ESMA-only pool: 549300W5EE53HVM32O36N200801 (105 MB)\n",
      "[2025-12-10 22:44:14]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:44:14]   Saved 171,154 rows in 8.7s (chunked)\n",
      "[2025-12-10 22:44:14] \n",
      "[LARGE 65/66] Processing ESMA-only pool: 549300W5EE53HVM32O36N200801 (105 MB)\n",
      "[2025-12-10 22:44:14]   Scanning 5 files for column schema...\n",
      "[2025-12-10 22:44:16]   Found 123 columns\n",
      "[2025-12-10 22:44:16]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:44:16]   Found 123 columns\n",
      "[2025-12-10 22:44:16]   Processing 5 files in chunked mode...\n",
      "[2025-12-10 22:44:20]   Saved 155,810 rows in 6.5s (chunked)\n",
      "[2025-12-10 22:44:20] \n",
      "[LARGE 66/66] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N200601 (137 MB)\n",
      "[2025-12-10 22:44:20]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:44:20]   Saved 155,810 rows in 6.5s (chunked)\n",
      "[2025-12-10 22:44:20] \n",
      "[LARGE 66/66] Processing ESMA-only pool: 5493006QMFDDMYWIAM13N200601 (137 MB)\n",
      "[2025-12-10 22:44:20]   Scanning 14 files for column schema...\n",
      "[2025-12-10 22:44:23]   Found 126 columns\n",
      "[2025-12-10 22:44:23]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:44:23]   Found 126 columns\n",
      "[2025-12-10 22:44:23]   Processing 14 files in chunked mode...\n",
      "[2025-12-10 22:44:28]   Saved 169,582 rows in 8.2s (chunked)\n",
      "[2025-12-10 22:44:28] \\nESMA-only pools complete: 3008.9s\n",
      "[2025-12-10 22:44:28] Session rows processed: 65,712,569\n",
      "[2025-12-10 22:44:28]   Saved 169,582 rows in 8.2s (chunked)\n",
      "[2025-12-10 22:44:28] \\nESMA-only pools complete: 3008.9s\n",
      "[2025-12-10 22:44:28] Session rows processed: 65,712,569\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 10: Main Processing Loop - ESMA-ONLY POOLS (with large pool handling)\n",
    "# =============================================================================\n",
    "log_message(\"=\"*70)\n",
    "log_message(\"STARTING ESMA-ONLY POOLS PROCESSING\")\n",
    "log_message(\"=\"*70)\n",
    "\n",
    "# Scan output folders to find already-completed pools (file-based tracking)\n",
    "completed_pools = scan_completed_pools()\n",
    "completed_esma_only = completed_pools['esma_only']\n",
    "\n",
    "# Track total rows for this session\n",
    "session_rows_processed = 0\n",
    "\n",
    "# Get pools to process (skip already completed)\n",
    "esma_only_to_process = [p for p in esma_only_pools if p not in completed_esma_only]\n",
    "\n",
    "log_message(f\"ESMA-only pools: {len(esma_only_pools)} total, {len(esma_only_to_process)} remaining\")\n",
    "\n",
    "# Identify large pools\n",
    "large_pools = [p for p in esma_only_to_process if is_large_pool(p, 'esma')]\n",
    "normal_pools = [p for p in esma_only_to_process if not is_large_pool(p, 'esma')]\n",
    "log_message(f\"  Normal pools: {len(normal_pools)}, Large pools (chunked): {len(large_pools)}\")\n",
    "\n",
    "esma_only_start = time.time()\n",
    "\n",
    "# Process normal pools first\n",
    "for i, esma_pool_id in enumerate(normal_pools):\n",
    "    pool_start = time.time()\n",
    "    log_message(f\"\\n[{i+1}/{len(normal_pools)}] Processing ESMA-only pool: {esma_pool_id}\")\n",
    "    \n",
    "    try:\n",
    "        # Process pool\n",
    "        result_df = process_esma_only_pool(esma_pool_id)\n",
    "        \n",
    "        # Save result\n",
    "        rows_saved = save_pool_result(result_df, \"esma_only\", esma_pool_id)\n",
    "        \n",
    "        # Free memory\n",
    "        del result_df\n",
    "        gc.collect()\n",
    "        \n",
    "        # Track session progress (file-based tracking means completion is automatic)\n",
    "        session_rows_processed += rows_saved\n",
    "        \n",
    "        pool_elapsed = time.time() - pool_start\n",
    "        log_message(f\"  Saved {rows_saved:,} rows in {pool_elapsed:.1f}s\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR processing {esma_pool_id}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        gc.collect()\n",
    "\n",
    "# Process large pools with chunked mode\n",
    "for i, esma_pool_id in enumerate(large_pools):\n",
    "    pool_start = time.time()\n",
    "    pool_size_mb = get_pool_compressed_size(esma_pool_id, 'esma') / 1024 / 1024\n",
    "    log_message(f\"\\n[LARGE {i+1}/{len(large_pools)}] Processing ESMA-only pool: {esma_pool_id} ({pool_size_mb:.0f} MB)\")\n",
    "    \n",
    "    try:\n",
    "        # Setup output path with ATOMIC write pattern\n",
    "        subdir = os.path.join(OUTPUT_DIR, \"esma_only\")\n",
    "        os.makedirs(subdir, exist_ok=True)\n",
    "        safe_pool_id = esma_pool_id.replace('/', '_').replace('\\\\', '_')\n",
    "        output_path = os.path.join(subdir, f\"{safe_pool_id}.csv\")\n",
    "        temp_path = output_path + \".tmp\"\n",
    "        \n",
    "        # Remove existing files if any (fresh start)\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        \n",
    "        # Process with chunked mode - writes to TEMP file\n",
    "        rows_saved = process_esma_only_pool_chunked(esma_pool_id, temp_path)\n",
    "        \n",
    "        # ATOMIC: Rename temp to final ONLY after complete success\n",
    "        os.replace(temp_path, output_path)\n",
    "        \n",
    "        # Track session progress (file-based tracking means completion is automatic)\n",
    "        session_rows_processed += rows_saved\n",
    "        \n",
    "        pool_elapsed = time.time() - pool_start\n",
    "        log_message(f\"  Saved {rows_saved:,} rows in {pool_elapsed:.1f}s (chunked)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"ERROR processing {esma_pool_id}: {str(e)}\"\n",
    "        log_message(error_msg)\n",
    "        # Clean up temp file if it exists\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        gc.collect()\n",
    "\n",
    "esma_only_elapsed = time.time() - esma_only_start\n",
    "log_message(f\"\\\\nESMA-only pools complete: {esma_only_elapsed:.1f}s\")\n",
    "log_message(f\"Session rows processed: {session_rows_processed:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c7da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-10 18:14:56] ======================================================================\n",
      "[2025-12-10 18:14:56] PROCESSING COMPLETE\n",
      "[2025-12-10 18:14:56] ======================================================================\n",
      "[2025-12-10 18:14:56] \n",
      "Completed pools:\n",
      "[2025-12-10 18:14:56]   Matched: 22/22\n",
      "[2025-12-10 18:14:56]   ECB-only: 38/36\n",
      "[2025-12-10 18:14:56]   ESMA-only: 246/246\n",
      "[2025-12-10 18:14:56] \n",
      "Total rows processed: 527,355,838\n",
      "[2025-12-10 18:14:56] \n",
      "Errors encountered: 3\n",
      "[2025-12-10 18:14:56]   RMBMBE000095100120084: Unable to allocate 178. MiB for an array with shape (1, 23304387) and data type object\n",
      "[2025-12-10 18:14:56]   RMBMFR000083100220149: Unable to allocate 2.13 GiB for an array with shape (6, 47749917) and data type object\n",
      "[2025-12-10 18:14:56]   RMBMBE000095100120084: Unable to allocate 1.39 GiB for an array with shape (8, 23304387) and data type object\n",
      "[2025-12-10 18:14:56] \n",
      "Output files: 22\n",
      "[2025-12-10 18:14:56] Total output size: 167.38 GB\n",
      "[2025-12-10 18:14:56] Output directory: D:\\ECB_ESMA_MERGED\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CELL 11: Final Summary\n",
    "# =============================================================================\n",
    "log_message(\"=\"*70)\n",
    "log_message(\"PROCESSING COMPLETE\")\n",
    "log_message(\"=\"*70)\n",
    "\n",
    "# Scan output folders for final stats (file-based tracking)\n",
    "completed_pools = scan_completed_pools()\n",
    "\n",
    "log_message(f\"\\\\nCompleted pools:\")\n",
    "log_message(f\"  Matched: {len(completed_pools['matched'])}/{len(matched_pools)}\")\n",
    "log_message(f\"  ECB-only: {len(completed_pools['ecb_only'])}/{len(ecb_only_pools)}\")\n",
    "log_message(f\"  ESMA-only: {len(completed_pools['esma_only'])}/{len(esma_only_pools)}\")\n",
    "\n",
    "# Check output size\n",
    "total_size = 0\n",
    "file_count = 0\n",
    "for subdir in ['matched', 'ecb_only', 'esma_only']:\n",
    "    subdir_path = os.path.join(OUTPUT_DIR, subdir)\n",
    "    if os.path.exists(subdir_path):\n",
    "        for f in os.listdir(subdir_path):\n",
    "            if f.endswith('.csv'):\n",
    "                total_size += os.path.getsize(os.path.join(subdir_path, f))\n",
    "                file_count += 1\n",
    "\n",
    "log_message(f\"\\\\nOutput files: {file_count}\")\n",
    "log_message(f\"Total output size: {total_size / (1024**3):.2f} GB\")\n",
    "log_message(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tftcu128)",
   "language": "python",
   "name": "tftcu128"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
